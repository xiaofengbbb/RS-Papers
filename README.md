# RS-Papers
| Title                                                                                                                                                           | Author                                                                                                                                                                  | Publication Year | Item Type       | Publication Title                                                                                                                                                                                   | Url                                                                                                                                                                                                                                                                           | ISBN                                | ISSN                            | DOI                                   | Abstract Note                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | Date                                                                                                                                                                                    | Date Added                                                                                                                                                                                                                                                                                                                                                | Date Modified                                                                                                                                        | Access Date                                                                                                                                                                                                                                                                                                                                         | Pages                                                                                                                                                                                            | Manual Tags                                         | Automatic Tags                                                                                                                                                                                                                                                                                                                                                                                                                         |
|-----------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------|-----------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------|---------------------------------|---------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| FedAttack: Effective and Covert Poisoning Attack on Federated Recommendation via Hard Sampling                                                                  | Wu, Chuhan; Wu, Fangzhao; Qi, Tao; Huang, Yongfeng; Xie, Xing                                                                                                           | 2022             | conferencePaper | Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining                                                                                                                | https://dl.acm.org/doi/10.1145/3534678.3539119                                                                                                                                                                                                                                | 978-1-4503-9385-0                   |                                 | 10.1145/3534678.3539119               | Federated learning (FL) is a feasible technique to learn personalized recommendation models from decentralized user data. Unfortunately, federated recommender systems are vulnerable to poisoning attacks by malicious clients. Existing recommender system poisoning methods mainly focus on promoting the recommendation chances of target items due to financial incentives. In fact, in real-world scenarios, the attacker may also attempt to degrade the overall performance of recommender systems. However, existing general FL poisoning methods for degrading model performance are either ineffective or not concealed in poisoning federated recommender systems. In this paper, we propose a simple yet effective and covert poisoning attack method on federated recommendation, named FedAttack. Its core idea is using globally hardest samples to subvert model training. More specifically, the malicious clients first infer user embeddings based on local user profiles. Next, they choose the candidate items that are most relevant to the user embeddings as hardest negative samples, and find the candidates farthest from the user embeddings as hardest positive samples. The model gradients inferred from these poisoned samples are then uploaded for aggregation. Extensive experiments on two benchmark datasets show that FedAttack can effectively degrade the performance of various federated recommender systems, meanwhile cannot be effectively detected nor defended by many existing methods. 联合学习（FL）是从分散的用户数据中学习个性化推荐模型的一种可行技术。遗憾的是，联合推荐系统容易受到恶意客户的中毒攻击。现有的推荐系统投毒方法主要是通过经济激励来提高目标项目的推荐几率。事实上，在现实世界中，攻击者也可能试图降低推荐系统的整体性能。然而，现有的用于降低模型性能的一般 FL 中毒方法在联合推荐系统中毒中要么无效，要么不隐蔽。在本文中，我们提出了一种简单有效且隐蔽的联合推荐中毒攻击方法，命名为 FedAttack。其核心思想是利用全局最难样本来颠覆模型训练。更具体地说，恶意客户端首先根据本地用户配置文件推断用户嵌入。然后，它们选择与用户嵌入最相关的候选项目作为最难负样本，并找出离用户嵌入最远的候选项目作为最难正样本。然后，从这些中毒样本中推断出的模型梯度将被上传进行聚合。在两个基准数据集上进行的大量实验表明，FedAttack 能有效降低各种联合推荐系统的性能，而许多现有方法却无法有效检测或防御。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | 2022-08-14                                                                                                                                                                              | 2024-10-09 12:58:49                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:31                                                                                                                                  | 2024-10-09 12:58:49                                                                                                                                                                                                                                                                                                                                 | 4164-4172                                                                                                                                                                                        | ccfInfo: CCF-A SIGKDD; citationNumber: 26           | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Shilling Black-box Review-based Recommender Systems through Fake Review Generation                                                                              | Chiang, Hung-Yun; Chen, Yi-Syuan; Song, Yun-Zhu; Shuai, Hong-Han; Chang, Jason S.                                                                                       | 2023             | conferencePaper | Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining                                                                                                                | https://dl.acm.org/doi/10.1145/3580305.3599502                                                                                                                                                                                                                                | 979-8-4007-0103-0                   |                                 | 10.1145/3580305.3599502               | Review-Based Recommender Systems (RBRS) have attracted increasing research interest due to their ability to alleviate wellknown cold-start problems. RBRS utilizes reviews to construct the user and items representations. However, in this paper, we argue that such a reliance on reviews may instead expose systems to the risk of being shilled. To explore this possibility, in this paper, we propose the first generation-based model for shilling attacks against RBRSs. Specifically, we learn a fake review generator through reinforcement learning, which maliciously promotes items by forcing prediction shifts after adding generated reviews to the system. By introducing the auxiliary rewards to increase text fluency and diversity with the aid of pre-trained language models and aspect predictors, the generated reviews can be effective for shilling with high fidelity. Experimental results demonstrate that the proposed framework can successfully attack three different kinds of RBRSs on the Amazon corpus with three domains and Yelp corpus. Furthermore, human studies also show that the generated reviews are fluent and informative. Finally, equipped with Attack Review Generators (ARGs), RBRSs with adversarial training are much more robust to malicious reviews. 基于评论的推荐系统（RBRS）能够缓解众所周知的冷启动问题，因此吸引了越来越多的研究兴趣。RBRS 利用评论来构建用户和项目表征。然而，在本文中，我们认为这种对评论的依赖反而可能使系统面临被推销的风险。为了探讨这种可能性，我们在本文中提出了第一个基于生成的模型，用于应对针对 RBRS 的虚假评论攻击。具体来说，我们通过强化学习来学习一个虚假评论生成器，在系统中添加生成的评论后，该生成器会通过强制预测偏移来恶意推广商品。通过引入辅助奖励，借助预先训练好的语言模型和方面预测器来增加文本的流畅性和多样性，生成的评论就能有效地进行高保真伪造。实验结果表明，所提出的框架可以在亚马逊语料库的三个域和 Yelp 语料库上成功攻击三种不同的 RBRS。此外，人类研究也表明，生成的评论流畅且信息丰富。最后，在配备了攻击评论生成器（ARGs）后，经过对抗训练的 RBRS 对恶意评论的鲁棒性大大提高。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | 2023-08-06                                                                                                                                                                              | 2024-10-09 13:01:31                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:16                                                                                                                                  | 2024-10-09 13:01:31                                                                                                                                                                                                                                                                                                                                 | 286-297                                                                                                                                                                                          | ccfInfo: CCF-A SIGKDD; citationNumber: 3            | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Explaining predictions and attacks in federated learning via random forests                                                                                     | Haffar, Rami; Sánchez, David; Domingo-Ferrer, Josep                                                                                                                     | 2023             | journalArticle  | Applied Intelligence                                                                                                                                                                                | https://link.springer.com/10.1007/s10489-022-03435-1                                                                                                                                                                                                                          |                                     | 0924-669X, 1573-7497            | 10.1007/s10489-022-03435-1            | Artificial intelligence (AI) is used for various purposes that are critical to human life. However, most state-of-the-art AI algorithms are black-box models, which means that humans cannot understand how such models make decisions. To forestall an algorithm-based authoritarian society, decisions based on machine learning ought to inspire trust by being explainable. For AI explainability to be practical, it must be feasible to obtain explanations systematically and automatically. A usual methodology to explain predictions made by a (black-box) deep learning model is to build a surrogate model based on a less difficult, more understandable decision algorithm. In this work, we focus on explaining by means of model surrogates the (mis)behavior of black-box models trained via federated learning. Federated learning is a decentralized machine learning technique that aggregates partial models trained by a set of peers on their own private data to obtain a global model. Due to its decentralized nature, federated learning offers some privacy protection to the participating peers. Nonetheless, it remains vulnerable to a variety of security attacks and even to sophisticated privacy attacks. To mitigate the effects of such attacks, we turn to the causes underlying misclassification by the federated model, which may indicate manipulations of the model. Our approach is to use random forests containing decision trees of restricted depth as surrogates of the federated blackbox model. Then, we leverage decision trees in the forest to compute the importance of the features involved in the wrong predictions. We have applied our method to detect security and privacy attacks that malicious peers or the model manager may orchestrate in federated learning scenarios. Empirical results show that our method can detect attacks with high accuracy and, unlike other attack detection mechanisms, it can also explain the operation of such attacks at the peers’ side. 人工智能（AI）被用于对人类生活至关重要的各种用途。然而，大多数最先进的人工智能算法都是黑箱模型，这意味着人类无法理解这些模型是如何做出决策的。为了防止出现以算法为基础的专制社会，基于机器学习的决策应该可以解释，从而赢得信任。要使人工智能的可解释性切实可行，就必须能够系统地自动获得解释。解释（黑盒）深度学习模型所做预测的通常方法是基于难度较低、更易理解的决策算法建立一个代理模型。在这项工作中，我们的重点是通过模型代理来解释通过联合学习训练的黑盒模型的（错误）行为。联合学习是一种去中心化的机器学习技术，它将一组同伴在各自的私有数据上训练出来的部分模型聚合在一起，从而得到一个全局模型。由于其分散性，联合学习为参与的同行提供了一定的隐私保护。不过，它仍然容易受到各种安全攻击，甚至是复杂的隐私攻击。为了减轻此类攻击的影响，我们转而研究联合模型错误分类的根本原因，这可能表明模型受到了操纵。我们的方法是使用包含深度受限的决策树的随机森林作为联合黑盒模型的替代物。然后，我们利用森林中的决策树来计算错误预测所涉及的特征的重要性。我们将我们的方法用于检测联合学习场景中恶意同行或模型管理者可能策划的安全和隐私攻击。实证结果表明，我们的方法能高精度地检测到攻击，而且与其他攻击检测机制不同，它还能解释这种攻击在对等方的运作。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 2023-01                                                                                                                                                                                 | 2024-10-09 13:03:22                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:31                                                                                                                                  | 2024-10-09 13:03:22                                                                                                                                                                                                                                                                                                                                 | 169-185                                                                                                                                                                                          | ccfInfo: CCF-C; citationNumber: 5                   | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Single-User Injection for Invisible Shilling Attack against Recommender Systems                                                                                 | Huang, Chengzhi; Li, Hui                                                                                                                                                | 2023             | conferencePaper | Proceedings of the 32nd ACM International Conference on Information and Knowledge Management                                                                                                        | https://dl.acm.org/doi/10.1145/3583780.3615062                                                                                                                                                                                                                                | 979-8-4007-0124-5                   |                                 | 10.1145/3583780.3615062               | Recommendation systems (RS) are crucial for alleviating the information overload problem. Due to its pivotal role in guiding users to make decisions, unscrupulous parties are lured to launch attacks against RS to affect the decisions of normal users and gain illegal profits. Among various types of attacks, shilling attack is one of the most subsistent and profitable attacks. In shilling attack, an adversarial party injects a number of well-designed fake user profiles into the system to mislead RS so that the attack goal can be achieved. Although existing shilling attack methods have achieved promising results, they all adopt the attack paradigm of multi-user injection, where some fake user profiles are required. This paper provides the first study of shilling attack in an extremely limited scenario: only one fake user profile is injected into the victim RS to launch shilling attacks (i.e., single-user injection). We propose a novel single-user injection method SUI-Attack for invisible shilling attack. SUI-Attack is a graph based attack method that models shilling attack as a node generation task over the user-item bipartite graph of the victim RS, and it constructs the fake user profile by generating user features and edges that link the fake user to items. Extensive experiments demonstrate that SUI-Attack can achieve promising attack results in single-user injection. In addition to its attack power, SUI-Attack increases the stealthiness of shilling attack and reduces the risk of being detected. We provide our implementation at: https://github.com/KDEGroup/SUI-Attack. 推荐系统（RS）对于缓解信息过载问题至关重要。由于推荐系统在引导用户做出决策方面发挥着举足轻重的作用，不法分子被引诱对推荐系统发起攻击，以影响正常用户的决策并获取非法利益。在各种类型的攻击中，shilling”攻击是最常见、最有利可图的攻击之一。在 “贿赂 ”攻击中，敌对者向系统中注入大量精心设计的虚假用户配置文件，误导 RS，从而达到攻击目的。尽管现有的 “shilling ”攻击方法取得了可喜的成果，但它们都采用了多用户注入的攻击范式，即需要一些虚假用户配置文件。本文首次研究了一种极其有限的情况下的起价攻击：只向受害者 RS 注入一个虚假用户配置文件来发动托攻击（即单用户注入）。我们提出了一种新型的单用户注入方法 SUI-Attack，用于隐形托攻击。SUI-Attack 是一种基于图的攻击方法，它将起价攻击建模为在受害者 RS 的用户-物品双元图上生成节点的任务，并通过生成用户特征和虚假用户与物品的链接边来构建虚假用户配置文件。大量实验证明，SUI-Attack 可以在单用户注入中取得良好的攻击效果。除了攻击力之外，SUI-Attack 还增强了 “推销 ”攻击的隐蔽性，降低了被发现的风险。我们在以下网址提供了我们的实现：https://github.com/KDEGroup/SUI-Attack.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | 2023-10-21                                                                                                                                                                              | 2024-10-09 13:43:26                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:16                                                                                                                                  | 2024-10-09 13:43:26                                                                                                                                                                                                                                                                                                                                 | 864-873                                                                                                                                                                                          | ccfInfo: CCF-B CIKM; citationNumber: 0              | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Let graph be the go board: gradient-free node injection attack for graph neural networks via reinforcement learning                                             | Ju, Mingxuan; Fan, Yujie; Zhang, Chuxu; Ye, Yanfang                                                                                                                     | 2023             | conferencePaper | Proceedings of the AAAI Conference on Artificial Intelligence                                                                                                                                       | https://ojs.aaai.org/index.php/AAAI/article/view/25558                                                                                                                                                                                                                        |                                     |                                 |                                       | Graph Neural Networks (GNNs) have drawn significant attentions over the years and been broadly applied to essential applications requiring solid robustness or vigorous security standards, such as product recommendation and user behavior modeling. Under these scenarios, exploiting GNN’s vulnerabilities and further downgrading its performance become extremely incentive for adversaries. Previous attackers mainly focus on structural perturbations or node injections to the existing graphs, guided by gradients from the surrogate models. Although they deliver promising results, several limitations still exist. For the structural perturbation attack, to launch a proposed attack, adversaries need to manipulate the existing graph topology, which is impractical in most circumstances. Whereas for the node injection attack, though being more practical, current approaches require training surrogate models to simulate a white-box setting, which results in significant performance downgrade when the surrogate architecture diverges from the actual victim model. To bridge these gaps, in this paper, we study the problem of black-box node injection attack, without training a potentially misleading surrogate model. Specifically, we model the node injection attack as a Markov decision process and propose Gradient-free Graph Advantage Actor Critic, namely G2A2C, a reinforcement learning framework in the fashion of advantage actor critic. By directly querying the victim model, G2A2C learns to inject highly malicious nodes with extremely limited attacking budgets, while maintaining a similar node feature distribution. Through our comprehensive experiments over eight acknowledged benchmark datasets with different characteristics, we demonstrate the superior performance of our proposed G2A2C over the existing state-of-the-art attackers. Source code is publicly available at: https://github.com/jumxglhf/G2A2C. 多年来，图形神经网络（GNN）一直备受关注，并被广泛应用于要求稳健性或严格安全标准的重要应用领域，如产品推荐和用户行为建模。在这种情况下，利用 GNN 的漏洞并进一步降低其性能就成了攻击者的极大诱因。以前的攻击者主要集中在现有图的结构扰动或节点注入上，以代用模型的梯度为指导。虽然这些方法取得了可喜的成果，但仍存在一些局限性。就结构扰动攻击而言，要发起提议的攻击，对手需要操纵现有的图拓扑，而这在大多数情况下是不切实际的。而对于节点注入攻击，目前的方法虽然更实用，但需要训练代理模型来模拟白盒环境，当代理架构与实际受害者模型出现偏差时，就会导致性能大幅下降。为了弥补这些差距，我们在本文中研究了黑盒节点注入攻击问题，而无需训练可能会误导的代理模型。具体来说，我们将节点注入攻击建模为一个马尔可夫决策过程，并提出了无梯度图优势行为批判（Gradient-free Graph Advantage Actor Critic），即 G2A2C，这是一种以优势行为批判为基础的强化学习框架。通过直接查询受害者模型，G2A2C 学会了以极其有限的攻击预算注入高度恶意的节点，同时保持相似的节点特征分布。通过对八个公认的具有不同特征的基准数据集进行全面实验，我们证明了我们所提出的 G2A2C 比现有的最先进攻击器具有更优越的性能。源代码可在以下网址公开获取：https://github.com/jumxglhf/G2A2C。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | 2023                                                                                                                                                                                    | 2024-10-09 13:03:12                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:25                                                                                                                                  | 2024-10-09 13:03:12                                                                                                                                                                                                                                                                                                                                 | 4383–4390                                                                                                                                                                                        | citationNumber: 5; ccfInfo: CCF-A AAAI              | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Revisiting Item Promotion in GNN-Based Collaborative Filtering: A Masked Targeted Topological Attack Perspective                                                | Wang, Yongwei; Liu, Yong; Shen, Zhiqi                                                                                                                                   | 2023             | journalArticle  | Proceedings of the AAAI Conference on Artificial Intelligence                                                                                                                                       | https://ojs.aaai.org/index.php/AAAI/article/view/26774                                                                                                                                                                                                                        |                                     | 2374-3468                       | 10.1609/aaai.v37i12.26774             | Graph neural networks (GNN) based collaborative filtering (CF) has attracted increasing attention in e-commerce and financial marketing platforms. However, there still lack efforts to evaluate the robustness of such CF systems in deployment. Fundamentally different from existing attacks, this work revisits the item promotion task and reformulates it from a targeted topological attack perspective for the first time. Specifically, we first develop a targeted attack formulation to maximally increase a target item's popularity. We then leverage gradient-based optimizations to find a solution. However, we observe the gradient estimates often appear noisy due to the discrete nature of a graph, which leads to a degradation of attack ability. To resolve noisy gradient effects, we then propose a masked attack objective that can remarkably enhance the topological attack ability. Furthermore, we design a computationally efficient approach to the proposed attack, thus making it feasible to evaluate large-large CF systems. Experiments on two real-world datasets show the effectiveness of our attack in analyzing the robustness of GNN-based CF more practically. 基于图神经网络（GNN）的协同过滤（CF）在电子商务和金融营销平台中受到越来越多的关注。然而，目前仍缺乏对此类协同过滤系统部署鲁棒性的评估。与现有攻击不同的是，本研究重新审视了项目推广任务，并首次从有针对性的拓扑攻击角度对其进行了重新表述。具体来说，我们首先开发了一种有针对性的攻击方法，以最大限度地提高目标项目的受欢迎程度。然后，我们利用基于梯度的优化来找到解决方案。然而，由于图的离散性，我们发现梯度估计值经常会出现噪声，从而导致攻击能力下降。为了解决噪声梯度效应，我们提出了一种屏蔽攻击目标，它能显著增强拓扑攻击能力。此外，我们还为所提出的攻击设计了一种计算效率高的方法，从而使评估大型 CF 系统变得可行。在两个真实世界数据集上的实验表明，我们的攻击能更实际地分析基于 GNN 的 CF 的鲁棒性。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | 2023-06-26                                                                                                                                                                              | 2024-10-09 13:42:33                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:25                                                                                                                                  | 2024-10-09 13:42:33                                                                                                                                                                                                                                                                                                                                 | 15206-15214                                                                                                                                                                                      | ccfInfo: CCF-A AAAI; citationNumber: 1              | /unread; General                                                                                                                                                                                                                                                                                                                                                                                                                       |
| Unveiling Vulnerabilities of Contrastive Recommender Systems to Poisoning Attacks                                                                               | Wang, Zongwei; Yu, Junliang; Gao, Min; Yin, Hongzhi; Cui, Bin; Sadiq, Shazia                                                                                            | 2023             | webpage         | arXiv.org                                                                                                                                                                                           | https://arxiv.org/abs/2311.18244v2                                                                                                                                                                                                                                            |                                     |                                 |                                       | Contrastive learning (CL) has recently gained prominence in the domain of recommender systems due to its great ability to enhance recommendation accuracy and improve model robustness. Despite its advantages, this paper identifies a vulnerability of CL-based recommender systems that they are more susceptible to poisoning attacks aiming to promote individual items. Our analysis indicates that this vulnerability is attributed to the uniform spread of representations caused by the InfoNCE loss. Furthermore, theoretical and empirical evidence shows that optimizing this loss favors smooth spectral values of representations. This finding suggests that attackers could facilitate this optimization process of CL by encouraging a more uniform distribution of spectral values, thereby enhancing the degree of representation dispersion. With these insights, we attempt to reveal a potential poisoning attack against CL-based recommender systems, which encompasses a dual-objective framework: one that induces a smoother spectral value distribution to amplify the InfoNCE loss's inherent dispersion effect, named dispersion promotion; and the other that directly elevates the visibility of target items, named rank promotion. We validate the threats of our attack model through extensive experimentation on four datasets. By shedding light on these vulnerabilities, our goal is to advance the development of more robust CL-based recommender systems. The code is available at \url{https://github.com/CoderWZW/ARLib}. 对比学习（Contrastive Learning，CL）在提高推荐准确性和改善模型稳健性方面具有强大的能力，因此最近在推荐系统领域大放异彩。尽管具有这些优点，本文还是指出了基于对比学习的推荐系统的一个弱点，即它们更容易受到以推广单个项目为目的的中毒攻击。我们的分析表明，这一弱点归因于 InfoNCE 丢失导致的表征均匀分布。此外，理论和经验证据表明，优化这种损失有利于表征的平滑频谱值。这一发现表明，攻击者可以通过鼓励更均匀的频谱值分布来促进 CL 的优化过程，从而提高表征的分散程度。有了这些认识，我们试图揭示一种针对基于CL的推荐系统的潜在中毒攻击，它包含一个双目标框架：一个目标是诱导更平滑的谱值分布，以放大InfoNCE损失固有的分散效应，命名为分散促进；另一个目标是直接提升目标项目的可见度，命名为等级促进。我们在四个数据集上进行了大量实验，验证了我们的攻击模型所带来的威胁。通过揭示这些漏洞，我们的目标是推动开发更强大的基于CL的推荐系统。代码见 \url{https://github.com/Coder                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | 2023-11-30                                                                                                                                                                              | 2024-10-09 13:50:33                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:30                                                                                                                                  | 2024-10-09 13:50:33                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                  | ccfInfo: CCF-A SIGKDD; citationNumber: 0            | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| On the User Behavior Leakage from Recommender System Exposure                                                                                                   | Xin, Xin; Yang, Jiyuan; Wang, Hanbing; Ma, Jun; Ren, Pengjie; Luo, Hengliang; Shi, Xinlei; Chen, Zhumin; Ren, Zhaochun                                                  | 2023             | journalArticle  | ACM Transactions on Information Systems                                                                                                                                                             | https://dl.acm.org/doi/10.1145/3568954                                                                                                                                                                                                                                        |                                     | 1046-8188, 1558-2868            | 10.1145/3568954                       | Modern recommender systems are trained to predict users potential future interactions from users historical behavior data. During the interaction process, despite the data coming from the user side recommender systems also generate exposure data to provide users with personalized recommendation slates. Compared with the sparse user behavior data, the system exposure data is much larger in volume since only very few exposed items would be clicked by the user. Besides, the users historical behavior data is privacy sensitive and is commonly protected with careful access authorization. However, the large volume of recommender exposure data which is generated by the service provider itself usually receives less attention and could be accessed within a relatively larger scope of various information seekers or even potential adversaries. In this paper, we investigate the problem of user behavior leakage in the field of recommender systems. We show that the privacy sensitive user past behavior data can be inferred through the modeling of system exposure. In other words, one can infer which items the user have clicked just from the observation of current system exposure for this user. Given the fact that system exposure data could be widely accessed from a relatively larger scope, we believe that the user past behavior privacy has a high risk of leakage in recommender systems. More precisely, we conduct an attack model whose input is the current recommended item slate (i.e., system exposure) for the user while the output is the user’s historical behavior. Specifically, we exploit an encoder-decoder structure to construct the attack model and apply different encoding and decoding strategies to verify the attack performance. Experimental results on two real-world datasets indicate a great danger of user behavior leakage. To address the risk, we propose a two-stage privacy-protection mechanism which firstly selects a subset of items from the exposure slate and then replaces the selected items with uniform or popularity-based exposure.Experimental evaluation reveals a trade-off effect between the recommendation accuracy and the privacy disclosure risk, which is an interesting and important topic for privacy concerns in recommender systems. 现代推荐系统经过训练，可以从用户的历史行为数据中预测用户未来潜在的交互行为。在交互过程中，尽管数据来自用户端，但推荐系统也会生成曝光数据，为用户提供个性化推荐。与稀疏的用户行为数据相比，系统的曝光数据量要大得多，因为只有极少数曝光项目会被用户点击。此外，用户的历史行为数据属于隐私敏感数据，通常需要经过谨慎的访问授权才能得到保护。然而，由服务提供商自身生成的大量推荐器曝光数据通常较少受到关注，而且可能会在相对更大的范围内被各种信息搜索者甚至潜在对手访问。 本文研究了推荐系统领域的用户行为泄露问题。我们的研究表明，隐私敏感的用户过往行为数据可以通过系统曝光建模来推断。换句话说，只需观察该用户当前的系统曝光情况，就能推断出该用户点击了哪些项目。鉴于系统曝光数据可以在相对更大的范围内被广泛获取，我们认为用户过去行为隐私在推荐系统中存在很高的泄露风险。更确切地说，我们建立了一个攻击模型，其输入是用户当前的推荐项目板块（即系统曝光），而输出则是用户的历史行为。具体来说，我们利用编码器-解码器结构来构建攻击模型，并应用不同的编码和解码策略来验证攻击性能。在两个真实数据集上的实验结果表明，用户行为泄漏的危险性很大。为了解决这一风险，我们提出了一种两阶段隐私保护机制，首先从曝光板中选择一个子集，然后用统一曝光或基于流行度的曝光替换所选项目。实验评估揭示了推荐准确性和隐私泄露风险之间的权衡效应，这是推荐系统中隐私问题的一个有趣而重要的话题。                                                                                                                                                | 2023-07-31                                                                                                                                                                              | 2024-10-09 13:03:42                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:29                                                                                                                                  | 2024-10-09 13:03:42                                                                                                                                                                                                                                                                                                                                 | 1-25                                                                                                                                                                                             | ccfInfo: CCF-A  TOIS; citationNumber: 15            | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| UA-FedRec: Untargeted Attack on Federated News Recommendation                                                                                                   | Yi, Jingwei; Wu, Fangzhao; Zhu, Bin; Yao, Jing; Tao, Zhulin; Sun, Guangzhong; Xie, Xing                                                                                 | 2023             | conferencePaper | Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining                                                                                                                | https://dl.acm.org/doi/10.1145/3580305.3599923                                                                                                                                                                                                                                | 979-8-4007-0103-0                   |                                 | 10.1145/3580305.3599923               | News recommendation is essential for personalized news distribution. Federated news recommendation, which enables collaborative model learning from multiple clients without sharing their raw data, is a promising approach for preserving users’ privacy. However, the security of federated news recommendation is still unclear. In this paper, we study this problem by proposing an untargeted attack on federated news recommendation called UA-FedRec. By exploiting the prior knowledge of news recommendation and federated learning, UA-FedRec can effectively degrade the model performance with a small percentage of malicious clients. First, the effectiveness of news recommendation highly depends on user modeling and news modeling. We design a news similarity perturbation method to make representations of similar news farther and those of dissimilar news closer to interrupt news modeling, and propose a user model perturbation method to make malicious user updates in opposite directions of benign updates to interrupt user modeling. Second, updates from different clients are typically aggregated with a weighted average based on their sample sizes. We propose a quantity perturbation method to enlarge sample sizes of malicious clients in a reasonable range to amplify the impact of malicious updates. Extensive experiments on two real-world datasets show that UA-FedRec can effectively degrade the accuracy of existing federated news recommendation methods, even when defense is applied. Our study reveals a critical security issue in existing federated news recommendation systems and calls for research efforts to address the issue. Our code is available at https://github.com/yjw1029/UA-FedRec. 新闻推荐对于个性化新闻发布至关重要。联盟式新闻推荐可以在不共享原始数据的情况下从多个客户端进行协作模型学习，是一种保护用户隐私的有前途的方法。然而，联合新闻推荐的安全性仍不明确。本文研究了这一问题，提出了一种针对联合新闻推荐的非目标攻击，称为 UA-FedRec。通过利用新闻推荐和联合学习的先验知识，UA-FedRec 可以有效地降低模型性能，而恶意客户端的比例很小。首先，新闻推荐的有效性高度依赖于用户建模和新闻建模。我们设计了一种新闻相似性扰动方法，使相似新闻的表征更远，而不相似新闻的表征更近，以中断新闻建模；并提出了一种用户模型扰动方法，使恶意用户的更新与良性更新方向相反，以中断用户建模。其次，来自不同客户端的更新通常会根据其样本量进行加权平均。我们提出了一种数量扰动方法，在合理范围内扩大恶意客户端的样本量，以放大恶意更新的影响。在两个真实数据集上进行的广泛实验表明，UA-FedRec 可以有效降低现有联合新闻推荐方法的准确性，即使在应用了防御措施的情况下也是如此。我们的研究揭示了现有联合新闻推荐系统中的一个关键安全问题，并呼吁研究人员努力解决这一问题。我们的代码见 https://github.com/yjw1029/UA-FedRec。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | 2023-08-06                                                                                                                                                                              | 2024-10-09 13:21:49                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:28                                                                                                                                  | 2024-10-09 13:21:49                                                                                                                                                                                                                                                                                                                                 | 5428-5438                                                                                                                                                                                        | ccfInfo: CCF-A SIGKDD; citationNumber: 7            | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Untargeted attack against federated recommendation systems via poisonous item embeddings and the defense                                                        | Yu, Yang; Liu, Qi; Wu, Likang; Yu, Runlong; Yu, Sanshi Lei; Zhang, Zaixi                                                                                                | 2023             | conferencePaper | Proceedings of the AAAI Conference on Artificial Intelligence                                                                                                                                       | https://ojs.aaai.org/index.php/AAAI/article/view/25611                                                                                                                                                                                                                        |                                     |                                 |                                       | Federated recommendation (FedRec) can train personalized recommenders without collecting user data, but the decentralized nature makes it susceptible to poisoning attacks. Most previous studies focus on the targeted attack to promote certain items, while the untargeted attack that aims to degrade the overall performance of the FedRec system remains less explored. In fact, untargeted attacks can disrupt the user experience and bring severe financial loss to the service provider. However, existing untargeted attack methods are either inapplicable or ineffective against FedRec systems. In this paper, we delve into the untargeted attack and its defense for FedRec systems. (i) We propose ClusterAttack, a novel untargeted attack method. It uploads poisonous gradients that converge the item embeddings into several dense clusters, which make the recommender generate similar scores for these items in the same cluster and perturb the ranking order. (ii) We propose a uniformity-based defense mechanism (UNION) to protect FedRec systems from such attacks. We design a contrastive learning task that regularizes the item embeddings toward a uniform distribution. Then the server filters out these malicious gradients by estimating the uniformity of updated item embeddings. Experiments on two public datasets show that ClusterAttack can effectively degrade the performance of FedRec systems while circumventing many defense methods, and UNION can improve the resistance of the system against various untargeted attacks, including our ClusterAttack. 联合推荐（FedRec）可以在不收集用户数据的情况下训练个性化推荐器，但其分散性使其容易受到中毒攻击。以往的研究大多集中在为推广某些项目而进行的定向攻击上，而旨在降低 FedRec 系统整体性能的非定向攻击仍然较少被探讨。事实上，非目标攻击会破坏用户体验，并给服务提供商带来严重的经济损失。然而，现有的非目标攻击方法对联邦记录系统要么不适用，要么无效。在本文中，我们将深入探讨针对 FedRec 系统的非目标攻击及其防御。(i) 我们提出了一种新型非目标攻击方法 ClusterAttack。它可以上传有毒梯度，将项目嵌入收敛到多个密集簇中，从而使推荐器为同一簇中的这些项目生成相似的分数，并扰乱排名顺序。(ii) 我们提出了一种基于统一性的防御机制（UNION），以保护 FedRec 系统免受此类攻击。我们设计了一个对比学习任务，将项目嵌入正则化为均匀分布。然后，服务器通过估计更新项目嵌入的均匀性来过滤掉这些恶意梯度。在两个公共数据集上的实验表明，ClusterAttack 能有效降低 FedRec 系统的性能，同时规避许多防御方法，而 UNION 则能提高系统对各种攻击的抵御能力。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | 2023                                                                                                                                                                                    | 2024-10-09 13:03:02                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:31                                                                                                                                  | 2024-10-09 13:03:02                                                                                                                                                                                                                                                                                                                                 | 4854–4863                                                                                                                                                                                        | ccfInfo: CCF-A AAAI; citationNumber: 16             | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Manipulating Federated Recommender Systems: Poisoning with Synthetic Users and Its Countermeasures                                                              | Yuan, Wei; Nguyen, Quoc Viet Hung; He, Tieke; Chen, Liang; Yin, Hongzhi                                                                                                 | 2023             | conferencePaper | Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval                                                                                     | https://dl.acm.org/doi/10.1145/3539618.3591722                                                                                                                                                                                                                                | 978-1-4503-9408-6                   |                                 | 10.1145/3539618.3591722               | Federated Recommender Systems (FedRecs) are considered privacypreserving techniques to collaboratively learn a recommendation model without sharing user data. Since all participants can directly influence the systems by uploading gradients, FedRecs are vulnerable to poisoning attacks of malicious clients. However, most existing poisoning attacks on FedRecs are either based on some prior knowledge or with less effectiveness. To reveal the real vulnerability of FedRecs, in this paper, we present a new poisoning attack method to manipulate target items’ ranks and exposure rates effectively in the top-K recommendation without relying on any prior knowledge. Specifically, our attack manipulates target items’ exposure rate by a group of synthetic malicious users who upload poisoned gradients considering target items’ alternative products. We conduct extensive experiments with two widely used FedRecs (Fed-NCF and Fed-LightGCN) on two real-world recommendation datasets. The experimental results show that our attack can significantly improve the exposure rate of unpopular target items with extremely fewer malicious users and fewer global epochs than state-of-the-art attacks. In addition to disclosing the security hole, we design a novel countermeasure for poisoning attacks on FedRecs. Specifically, we propose a hierarchical gradient clipping with sparsified updating to defend against existing poisoning attacks. The empirical results demonstrate that the proposed defending mechanism improves the robustness of FedRecs. 联合推荐系统（FedRecs）被认为是在不共享用户数据的情况下协同学习推荐模型的隐私保护技术。由于所有参与者都可以通过上传梯度直接影响系统，因此 FedRecs 容易受到恶意客户端的中毒攻击。然而，大多数现有的针对 FedRecs 的中毒攻击要么基于一些先验知识，要么效果较差。为了揭示 FedRecs 的真正弱点，我们在本文中提出了一种新的投毒攻击方法，可以在不依赖任何先验知识的情况下，在 Top-K 推荐中有效操纵目标项目的排名和曝光率。具体来说，我们的攻击是通过一组合成的恶意用户上传考虑了目标商品替代产品的中毒梯度来操纵目标商品的曝光率。我们在两个真实世界的推荐数据集上使用两种广泛使用的 FedRecs（Fed-NCF 和 Fed-LightGCN）进行了大量实验。实验结果表明，与最先进的攻击相比，我们的攻击能以极少的恶意用户和更少的全局历时显著提高不受欢迎目标项的曝光率。除了揭示安全漏洞之外，我们还设计了一种新颖的针对联邦记录中毒攻击的对策。具体来说，我们提出了一种分层梯度剪切和稀疏更新的方法来抵御现有的中毒攻击。实证结果表明，所提出的防御机制能有效地防止中毒攻击。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | 2023-07-19                                                                                                                                                                              | 2024-10-09 13:14:31                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:25                                                                                                                                  | 2024-10-09 13:14:31                                                                                                                                                                                                                                                                                                                                 | 1690-1699                                                                                                                                                                                        | ccfInfo: CCF-A SIGIR; citationNumber: 12            | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Practical Cross-System Shilling Attacks with Limited Access to Data                                                                                             | Zeng, Meifang; Li, Ke; Jiang, Bingchuan; Cao, Liujuan; Li, Hui                                                                                                          | 2023             | journalArticle  | Proceedings of the AAAI Conference on Artificial Intelligence                                                                                                                                       | https://ojs.aaai.org/index.php/AAAI/article/view/25612                                                                                                                                                                                                                        |                                     | 2374-3468                       | 10.1609/aaai.v37i4.25612              | In shilling attacks, an adversarial party injects a few fake user profiles into a Recommender System (RS) so that the target item can be promoted or demoted. Although much effort has been devoted to developing shilling attack methods, we find that existing approaches are still far from practical. In this paper, we analyze the properties a practical shilling attack method should have and propose a new concept of Cross-system Attack. With the idea of Cross-system Attack, we design a Practical Cross-system Shilling Attack (PC-Attack) framework that requires little information about the victim RS model and the target RS data for conducting attacks. PC-Attack is trained to capture graph topology knowledge from public RS data in a self-supervised manner. Then, it is fine-tuned on a small portion of target data that is easy to access to construct fake profiles. Extensive experiments have demonstrated the superiority of PC-Attack over state-of-the-art baselines. Our implementation of PC-Attack is available at https://github.com/KDEGroup/PC-Attack. 在托攻击中，敌对方会向推荐系统（RS）注入一些虚假的用户配置文件，从而使目标项目得到提升或降级。尽管人们在开发托攻击方法上投入了大量精力，但我们发现现有的方法还远远不够实用。在本文中，我们分析了实用的托攻击方法应具备的特性，并提出了跨系统攻击的新概念。根据跨系统攻击的理念，我们设计了一种实用的跨系统起刺攻击（PC-Attack）框架，该框架在进行攻击时只需要很少的受害者 RS 模型和目标 RS 数据信息。PC-Attack 经过训练，能以自我监督的方式从公共 RS 数据中获取图拓扑知识。然后，在一小部分易于访问的目标数据上对其进行微调，以构建虚假配置文件。广泛的实验证明，PC-Attack 优于最先进的基线。我们的 PC-Attack 实现可在 https://github.com/KDEGroup/PC-Attack 上获取。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | 2023-06-26                                                                                                                                                                              | 2024-10-09 13:41:36                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:32                                                                                                                                  | 2024-10-09 13:41:36                                                                                                                                                                                                                                                                                                                                 | 4864-4874                                                                                                                                                                                        | ccfInfo: CCF-A AAAI; citationNumber: 4              | /unread; APP: Security                                                                                                                                                                                                                                                                                                                                                                                                                 |
| Multi-agent Attacks for Black-box Social Recommendations                                                                                                        | Wang, Shijie; Fan, Wenqi; Wei, Xiao-yong; Mei, Xiaowei; Lin, Shanru; Li, Qing                                                                                           | 2024             | preprint        |                                                                                                                                                                                                     | http://arxiv.org/abs/2311.07127                                                                                                                                                                                                                                               |                                     |                                 |                                       | The rise of online social networks has facilitated the evolution of social recommender systems, which incorporate social relations to enhance users’ decision-making process. With the great success of Graph Neural Networks (GNNs) in learning node representations, GNN-based social recommendations have been widely studied to model user-item interactions and user-user social relations simultaneously. Despite their great successes, recent studies have shown that these advanced recommender systems are highly vulnerable to adversarial attacks, in which attackers can inject well-designed fake user profiles to disrupt recommendation performances. While most existing studies mainly focus on targeted attacks to promote target items on vanilla recommender systems, untargeted attacks to degrade the overall prediction performance are less explored on social recommendations under a black-box scenario. To perform untargeted attacks on social recommender systems, attackers can construct malicious social relationships for fake users to enhance the attack performance. However, the coordination of social relations and item profiles is challenging for attacking black-box social recommendations. To address this limitation, we first conduct several preliminary studies to demonstrate the effectiveness of cross-community connections and cold-start items in degrading recommendations performance. Specifically, we propose a novel framework MultiAttack based on multi-agent reinforcement learning to coordinate the generation of cold-start item profiles and cross-community social relations for conducting untargeted attacks on black-box social recommendations. Comprehensive experiments on various real-world datasets demonstrate the effectiveness of our proposed attacking framework under the black-box setting. CCS Concepts: • Information systems → Collaborative and social computing systems and tools. 在线社交网络的兴起促进了社交推荐系统的发展，社交推荐系统结合了社交关系以增强用户的决策过程。随着图神经网络（GNN）在学习节点表征方面取得巨大成功，基于 GNN 的社交推荐系统也得到了广泛研究，以同时模拟用户-物品交互和用户-用户社交关系。尽管取得了巨大成功，但最近的研究表明，这些先进的推荐系统极易受到对抗性攻击，攻击者可以注入精心设计的虚假用户配置文件来破坏推荐性能。现有的大多数研究主要集中在有针对性的攻击，以促进虚构推荐系统中的目标项目，而在黑盒场景下对社交推荐进行非针对性攻击以降低整体预测性能的研究则较少。为了对社交推荐系统实施非定向攻击，攻击者可以为虚假用户构建恶意社交关系，以提高攻击性能。然而，协调社会关系和项目概况对于攻击黑盒社交推荐具有挑战性。为了解决这个问题，我们首先进行了几项初步研究，以证明跨社区连接和冷启动项目在降低推荐性能方面的有效性。具体地说，我们提出了一种基于多代理强化学习的新型框架 MultiAttack，以协调冷启动项目配置文件和跨社区社交关系的生成，从而对黑盒社交推荐进行无目标攻击。在各种真实数据集上进行的综合实验证明了我们提出的攻击框架在黑盒环境下的有效性。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | 2024-09-16                                                                                                                                                                              | 2024-10-09 13:44:46                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:27                                                                                                                                  | 2024-10-09 13:44:46                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                  | citationNumber: 0; ccfInfo: Not Found               | /unread; Computer Science - Artificial Intelligence; Computer Science - Social and Information Networks                                                                                                                                                                                                                                                                                                                                |
| Poisoning Federated Recommender Systems with Fake Users                                                                                                         | Yin, Ming; Xu, Yichang; Fang, Minghong; Gong, Neil Zhenqiang                                                                                                            | 2024             | preprint        |                                                                                                                                                                                                     | http://arxiv.org/abs/2402.11637                                                                                                                                                                                                                                               |                                     |                                 |                                       | Federated recommendation is a prominent use case within federated learning, yet it remains susceptible to various attacks, from user to server-side vulnerabilities. Poisoning attacks are particularly notable among user-side attacks, as participants upload malicious model updates to deceive the global model, often intending to promote or demote specific targeted items. This study investigates strategies for executing promotion attacks in federated recommender systems. Current poisoning attacks on federated recommender systems often rely on additional information, such as the local training data of genuine users or item popularity. However, such information is challenging for the potential attacker to obtain. Thus, there is a need to develop an attack that requires no extra information apart from item embeddings obtained from the server. In this paper, we introduce a novel fake user based poisoning attack named PoisonFRS to promote the attacker-chosen targeted item in federated recommender systems without requiring knowledge about user-item rating data, user attributes, or the aggregation rule used by the server. Extensive experiments on multiple real-world datasets demonstrate that PoisonFRS can effectively promote the attacker-chosen targeted item to a large portion of genuine users and outperform current benchmarks that rely on additional information about the system. We further observe that the model updates from both genuine and fake users are indistinguishable within the latent space. 联合推荐是联合学习中的一个重要用例，但它仍然容易受到从用户到服务器端漏洞的各种攻击。在用户端攻击中，“中毒 ”攻击尤为突出，因为参与者会上传恶意模型更新来欺骗全局模型，通常是为了提升或降低特定目标项目的等级。本研究探讨了在联合推荐系统中实施晋升攻击的策略。目前针对联合推荐系统的中毒攻击通常依赖于额外的信息，如真实用户的本地训练数据或项目流行度。然而，潜在的攻击者很难获得这些信息。因此，有必要开发一种除了从服务器上获取项目嵌入信息之外不需要额外信息的攻击。在本文中，我们介绍了一种名为 PoisonFRS 的基于虚假用户的新型中毒攻击，它可以在联合推荐系统中推广攻击者选择的目标项目，而无需了解用户-项目评级数据、用户属性或服务器使用的聚合规则。在多个真实数据集上进行的广泛实验表明，PoisonFRS 能有效地向大部分真实用户推广攻击者选择的目标项目，其性能优于目前依赖系统附加信息的基准。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | 2024-02-18                                                                                                                                                                              | 2024-10-17 12:21:02                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:30                                                                                                                                  | 2024-10-17 12:21:02                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                  | citationNumber: 0; ccfInfo: CCF-A WWW               | /unread; Computer Science - Cryptography and Security; Computer Science - Information Retrieval; Computer Science - Machine Learning                                                                                                                                                                                                                                                                                                   |
| A survey of attack detection approaches in collaborative filtering recommender systems                                                                          | Rezaimehr, Fatemeh; Dadkhah, Chitra                                                                                                                                     | 2021             | journalArticle  | Artificial Intelligence Review                                                                                                                                                                      | https://link.springer.com/10.1007/s10462-020-09898-3                                                                                                                                                                                                                          |                                     | 0269-2821, 1573-7462            | 10.1007/s10462-020-09898-3            |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | 2021-03                                                                                                                                                                                 | 2024-10-21 12:10:39                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:19:55                                                                                                                                  | 2024-10-21 12:10:39                                                                                                                                                                                                                                                                                                                                 | 2011-2066                                                                                                                                                                                        | citationNumber: 26; ccfInfo: CCF-None AIR           | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| A Survey on Adversarial Recommender Systems: From Attack/Defense Strategies to Generative Adversarial Networks                                                  | Deldjoo, Yashar; Noia, Tommaso Di; Merra, Felice Antonio                                                                                                                | 2022             | journalArticle  | ACM Computing Surveys                                                                                                                                                                               | https://dl.acm.org/doi/10.1145/3439729                                                                                                                                                                                                                                        |                                     | 0360-0300, 1557-7341            | 10.1145/3439729                       | Latent-factor models (LFM) based on collaborative filtering (CF), such as matrix factorization (MF) and deep CF methods, are widely used in modern recommender systems (RS) due to their excellent performance and recommendation accuracy. However, success has been accompanied with a major new arising challenge:               Many applications of machine learning (ML) are adversarial in nature               [146]. In recent years, it has been shown that these methods are vulnerable to adversarial examples, i.e., subtle but non-random perturbations designed to force recommendation models to produce erroneous outputs.                          The goal of this survey is two-fold: (i) to present recent advances on adversarial machine learning (AML) for the security of RS (i.e., attacking and defense recommendation models) and (ii) to show another successful application of AML in generative adversarial networks (GANs) for generative applications, thanks to their ability for learning (high-dimensional) data distributions. In this survey, we provide an exhaustive literature review of 76 articles published in major RS and ML journals and conferences. This review serves as a reference for the RS community working on the security of RS or on generative models using GANs to improve their quality.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 2022-03-31                                                                                                                                                                              | 2024-09-07 05:15:37                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:21                                                                                                                                  | 2024-09-07 05:15:37                                                                                                                                                                                                                                                                                                                                 | 1-38                                                                                                                                                                                             | ccfInfo: CCF-None CSUR; citationNumber: 60          | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Understanding Shilling Attacks and Their Detection Traits: A Comprehensive Survey                                                                               | Sundar, Agnideven Palanisamy; Li, Feng; Zou, Xukai; Gao, Tianchong; Russomanno, Evan D.                                                                                 | 2020             | journalArticle  | IEEE Access                                                                                                                                                                                         | https://ieeexplore.ieee.org/document/9205244/?arnumber=9205244                                                                                                                                                                                                                |                                     | 2169-3536                       | 10.1109/ACCESS.2020.3022962           | The internet is the home for huge volumes of useful data that is constantly being created making it difficult for users to find information relevant to them. Recommendation System is a special type of information filtering system adapted by online vendors to provide recommendations to their customers based on their requirements. Collaborative filtering is one of the most widely used recommendation systems; unfortunately, it is prone to shilling/profile injection attacks. Such attacks alter the recommendation process to promote or demote a particular product. Over the years, multiple attack models and detection techniques have been developed to mitigate the problem. This paper aims to be a comprehensive survey of the shilling attack models, detection attributes, and detection algorithms. Additionally, we unravel and classify the intrinsic traits of the injected profiles that are exploited by the detection algorithms, which has not been explored in previous works. We also briefly discuss recent works in the development of robust algorithms that alleviate the impact of shilling attacks, attacks on multi-criteria systems, and intrinsic feedback based collaborative filtering methods.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | 2020                                                                                                                                                                                    | 2024-10-22 09:01:30                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:19                                                                                                                                  | 2024-10-22 09:01:30                                                                                                                                                                                                                                                                                                                                 | 171703-171715                                                                                                                                                                                    | citationNumber: 15; ccfInfo: CCF-None ACCESS        | /unread; Biological system modeling; Collaboration; Collaborative filtering; Detection algorithms; detection traits and algorithms; Information science; profile injection attacks; Recommender systems; robust algorithms; Robustness; shilling attacks                                                                                                                                                                               |
| Shilling attacks against collaborative recommender systems: a review                                                                                            | Si, Mingdan; Li, Qingshan                                                                                                                                               | 2020             | journalArticle  | Artificial Intelligence Review                                                                                                                                                                      | http://link.springer.com/10.1007/s10462-018-9655-x                                                                                                                                                                                                                            |                                     | 0269-2821, 1573-7462            | 10.1007/s10462-018-9655-x             |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | 2020-01                                                                                                                                                                                 | 2024-09-13 13:18:21                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:28                                                                                                                                  | 2024-09-13 13:18:20                                                                                                                                                                                                                                                                                                                                 | 291-319                                                                                                                                                                                          | citationNumber: Not Found; ccfInfo: CCF-None AIR    | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Ada3D : Exploiting the Spatial Redundancy with Adaptive Inference for Efficient 3D Object Detection                                                             | Zhao, Tianchen; Ning, Xuefei; Hong, Ke; Qiu, Zhongyuan; Lu, Pu; Zhao, Yali; Zhang, Linfeng; Zhou, Lipu; Dai, Guohao; Yang, Huazhong; Wang, Yu                           | 2023             | conferencePaper |                                                                                                                                                                                                     | https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_Ada3D__Exploiting_the_Spatial_Redundancy_with_Adaptive_Inference_for_ICCV_2023_paper.html                                                                                                                            |                                     |                                 |                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | 2023                                                                                                                                                                                    | 2024-10-10 02:44:12                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:19:56                                                                                                                                  | 2024-10-10 02:44:12                                                                                                                                                                                                                                                                                                                                 | 17728-17738                                                                                                                                                                                      | citationNumber: 1; ccfInfo: CCF-A ICCV              | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| SEDAT: A Stacked Ensemble Learning-Based Detection Model for Multiscale Network Attacks                                                                         | Feng, Yan; Yang, Zhihai; Sun, Qindong; Liu, Yanxiao                                                                                                                     | 2024             | journalArticle  | Electronics                                                                                                                                                                                         | https://www.mdpi.com/2079-9292/13/15/2953                                                                                                                                                                                                                                     |                                     |                                 |                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | 2024                                                                                                                                                                                    | 2024-09-16 06:07:32                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:28                                                                                                                                  | 2024-09-16 06:07:32                                                                                                                                                                                                                                                                                                                                 | 2953                                                                                                                                                                                             | citationNumber: 0; ccfInfo: Not Found               | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| A Deep Dive into Fairness, Bias, Threats, and Privacy in Recommender Systems: Insights and Future Research                                                      | Roy, Falguni; Ding, Xiaofeng; Choo, K.-K. R.; Zhou, Pan                                                                                                                 | 2024             | preprint        |                                                                                                                                                                                                     | http://arxiv.org/abs/2409.12651                                                                                                                                                                                                                                               |                                     |                                 | 10.48550/arXiv.2409.12651             | Recommender systems are essential for personalizing digital experiences on e-commerce sites, streaming services, and social media platforms. While these systems are necessary for modern digital interactions, they face fairness, bias, threats, and privacy challenges. Bias in recommender systems can result in unfair treatment of specific users and item groups, and fairness concerns demand that recommendations be equitable for all users and items. These systems are also vulnerable to various threats that compromise reliability and security. Furthermore, privacy issues arise from the extensive use of personal data, making it crucial to have robust protection mechanisms to safeguard user information. This study explores fairness, bias, threats, and privacy in recommender systems. It examines how algorithmic decisions can unintentionally reinforce biases or marginalize specific user and item groups, emphasizing the need for fair recommendation strategies. The study also looks at the range of threats in the form of attacks that can undermine system integrity and discusses advanced privacy-preserving techniques. By addressing these critical areas, the study highlights current limitations and suggests future research directions to improve recommender systems' robustness, fairness, and privacy. Ultimately, this research aims to help develop more trustworthy and ethical recommender systems that better serve diverse user populations.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | 2024-09-19                                                                                                                                                                              | 2024-10-15 14:03:18                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:21                                                                                                                                  | 2024-10-15 14:03:18                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                  | citationNumber: 0; ccfInfo: CCF-None CORR           | /unread; Computer Science - Cryptography and Security; Computer Science - Information Retrieval; Computer Science - Human-Computer Interaction                                                                                                                                                                                                                                                                                         |
| Recent developments in recommender systems: A survey                                                                                                            | Li, Yang; Liu, Kangbo; Satapathy, Ranjan; Wang, Suhang; Cambria, Erik                                                                                                   | 2024             | journalArticle  | IEEE Computational Intelligence Magazine                                                                                                                                                            | https://ieeexplore.ieee.org/abstract/document/10494051/                                                                                                                                                                                                                       |                                     |                                 |                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | 2024                                                                                                                                                                                    | 2024-09-29 12:19:35                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:32                                                                                                                                  | 2024-09-29 12:19:35                                                                                                                                                                                                                                                                                                                                 | 78–95                                                                                                                                                                                            | citationNumber: 1; ccfInfo: CCF-None CORR           | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Manipulating Recommender Systems: A Survey of Poisoning Attacks and Countermeasures                                                                             | Nguyen, Thanh Toan; Quoc Viet Hung, Nguyen; Nguyen, Thanh Tam; Huynh, Thanh Trung; Nguyen, Thanh Thi; Weidlich, Matthias; Yin, Hongzhi                                  | 2024             | journalArticle  | ACM Computing Surveys                                                                                                                                                                               | https://dl.acm.org/doi/10.1145/3677328                                                                                                                                                                                                                                        |                                     | 0360-0300, 1557-7341            | 10.1145/3677328                       | Recommender systems have become an integral part of online services due to their ability to help users locate specific information in a sea of data. However, existing studies show that some recommender systems are vulnerable to poisoning attacks particularly those that involve learning schemes. A poisoning attack is where an adversary injects carefully crafted data into the process of training a model, with the goal of manipulating the system’s final recommendations. Based on recent advancements in artificial intelligence (AI), such attacks have gained importance recently. At present, we do not have a full and clear picture of why adversaries mount such attacks, nor do we have comprehensive knowledge of the full capacity to which such attacks can undermine a model or the impacts that might have. While numerous countermeasures to poisoning attacks have been developed, they have not yet been systematically linked to the properties of the attacks. Consequently, assessing the respective risks and potential success of mitigation strategies is difficult, if not impossible. This survey aims to fill this gap by primarily focusing on poisoning attacks and their countermeasures. This is in contrast to prior surveys that mainly focus on attacks and their detection methods. Through an exhaustive literature review, we provide a novel taxonomy for poisoning attacks, formalise its dimensions, and accordingly organise 31 attacks described in the literature. Further, we review 43 countermeasures to detect and/or prevent poisoning attacks, evaluating their effectiveness against specific types of attacks. This comprehensive survey should serve as a point of reference for protecting recommender systems against poisoning attacks. The article concludes with a discussion on open issues in the field and impactful directions for future research. A rich repository of resources associated with poisoning attacks is available at https://github.com/tamlhp/awesome-recsys-poisoning.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | 2024-07-25                                                                                                                                                                              | 2024-09-07 01:07:00                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:32                                                                                                                                  | 2024-09-07 01:07:00                                                                                                                                                                                                                                                                                                                                 | 3677328                                                                                                                                                                                          | citationNumber: 0; ccfInfo: CCF-None CORR           |                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| Uplift Modeling for Target User Attacks on Recommender Systems                                                                                                  | Wang, Wenjie; Wang, Changsheng; Feng, Fuli; Shi, Wentao; Ding, Daizong; Chua, Tat-Seng                                                                                  | 2024             | conferencePaper | Proceedings of the ACM Web Conference 2024                                                                                                                                                          | https://dl.acm.org/doi/10.1145/3589334.3645403                                                                                                                                                                                                                                | 979-8-4007-0171-9                   |                                 | 10.1145/3589334.3645403               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | 2024-05-13                                                                                                                                                                              | 2024-09-13 13:31:00                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:30                                                                                                                                  | 2024-09-13 13:30:58                                                                                                                                                                                                                                                                                                                                 | 3343-3354                                                                                                                                                                                        | citationNumber: 0; ccfInfo: CCF-A WWW               | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Poisoning Attacks and Defenses in Recommender Systems: A Survey                                                                                                 | Wang, Zongwei; Yu, Junliang; Gao, Min; Yuan, Wei; Ye, Guanhua; Sadiq, Shazia; Yin, Hongzhi                                                                              | 2024             | preprint        |                                                                                                                                                                                                     | http://arxiv.org/abs/2406.01022                                                                                                                                                                                                                                               |                                     |                                 |                                       | Modern recommender systems (RS) have profoundly enhanced user experience across digital platforms, yet they face significant threats from poisoning attacks. These attacks, aimed at manipulating recommendation outputs for unethical gains, exploit vulnerabilities in RS through injecting malicious data or intervening model training. This survey presents a unique perspective by examining these threats through the lens of an attacker, offering fresh insights into their mechanics and impacts. Concretely, we detail a systematic pipeline that encompasses four stages of a poisoning attack: setting attack goals, assessing attacker capabilities, analyzing victim architecture, and implementing poisoning strategies. The pipeline not only aligns with various attack tactics but also serves as a comprehensive taxonomy to pinpoint focuses of distinct poisoning attacks. Correspondingly, we further classify defensive strategies into two main categories: poisoning data filtering and robust training from the defender's perspective. Finally, we highlight existing limitations and suggest innovative directions for further exploration in this field.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 2024-06-05                                                                                                                                                                              | 2024-10-21 12:43:35                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:29                                                                                                                                  | 2024-10-21 12:43:35                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                  | citationNumber: 0; ccfInfo: CCF-None CORR           | /unread; Computer Science - Cryptography and Security; Computer Science - Information Retrieval                                                                                                                                                                                                                                                                                                                                        |
| Poisoning Attacks against Recommender Systems: A Survey                                                                                                         | Wang, Zongwei; Gao, Min; Yu, Junliang; Ma, Hao; Yin, Hongzhi; Sadiq, Shazia                                                                                             | 2024             | preprint        |                                                                                                                                                                                                     | http://arxiv.org/abs/2401.01527                                                                                                                                                                                                                                               |                                     |                                 |                                       | Modern recommender systems (RS) have seen substantial success, yet they remain vulnerable to malicious activities, notably poisoning attacks. These attacks involve injecting malicious data into the training datasets of RS, thereby compromising their integrity and manipulating recommendation outcomes for gaining illicit profits. This survey paper provides a systematic and up-to-date review of the research landscape on Poisoning Attacks against Recommendation (PAR). A novel and comprehensive taxonomy is proposed, categorizing existing PAR methodologies into three distinct categories: Component-Specific, Goal-Driven, and Capability Probing. For each category, we discuss its mechanism in detail, along with associated methods. Furthermore, this paper highlights potential future research avenues in this domain. Additionally, to facilitate and benchmark the empirical comparison of PAR, we introduce an open-source library, ARLib, which encompasses a comprehensive collection of PAR models and common datasets. The library is released at https://github.com/CoderWZW/ARLib.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | 2024-01-13                                                                                                                                                                              | 2024-09-12 07:36:37                                                                                                                                                                                                                                                                                                                                       | 2024-12-06 21:31:36                                                                                                                                  | 2024-09-12 07:36:37                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                  | ccfInfo: CCF-None CORR; citationNumber: 0           | /unread; Computer Science - Information Retrieval                                                                                                                                                                                                                                                                                                                                                                                      |
| Poisoning Decentralized Collaborative Recommender System and Its Countermeasures                                                                                | Zheng, Ruiqi; Qu, Liang; Chen, Tong; Zheng, Kai; Shi, Yuhui; Yin, Hongzhi                                                                                               | 2024             | conferencePaper | Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval                                                                                     | https://dl.acm.org/doi/10.1145/3626772.3657814                                                                                                                                                                                                                                | 979-8-4007-0431-4                   |                                 | 10.1145/3626772.3657814               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | 2024-07-10                                                                                                                                                                              | 2024-09-13 13:31:00                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:29                                                                                                                                  | 2024-09-13 13:31:00                                                                                                                                                                                                                                                                                                                                 | 1712-1721                                                                                                                                                                                        | citationNumber: 0; ccfInfo: CCF-A SIGIR             | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Personalized route recommendation using big trajectory data                                                                                                     | Dai, Jian; Yang, Bin; Guo, Chenjuan; Ding, Zhiming                                                                                                                      | 2015             | conferencePaper | 2015 IEEE 31st International Conference on Data Engineering                                                                                                                                         | https://ieeexplore.ieee.org/document/7113313/?arnumber=7113313                                                                                                                                                                                                                |                                     |                                 | 10.1109/ICDE.2015.7113313             | When planning routes, drivers usually consider a multitude of different travel costs, e.g., distances, travel times, and fuel consumption. Different drivers may choose different routes between the same source and destination because they may have different driving preferences (e.g., time-efficient driving v.s. fuel-efficient driving). However, existing routing services support little in modeling multiple travel costs and personalization-they usually deliver the same routes that minimize a single travel cost (e.g., the shortest routes or the fastest routes) to all drivers. We study the problem of how to recommend personalized routes to individual drivers using big trajectory data. First, we provide techniques capable of modeling and updating different drivers' driving preferences from the drivers' trajectories while considering multiple travel costs. To recommend personalized routes, we provide techniques that enable efficient selection of a subset of trajectories from all trajectories according to a driver's preference and the source, destination, and departure time specified by the driver. Next, we provide techniques that enable the construction of a small graph with appropriate edge weights reflecting how the driver would like to use the edges based on the selected trajectories. Finally, we recommend the shortest route in the small graph as the personalized route to the driver. Empirical studies with a large, real trajectory data set from 52,211 taxis in Beijing offer insight into the design properties of the proposed techniques and suggest that they are efficient and effective.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | 2015-04                                                                                                                                                                                 | 2024-11-02 02:39:45                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:29                                                                                                                                  | 2024-11-02 02:39:45                                                                                                                                                                                                                                                                                                                                 | 543-554                                                                                                                                                                                          | ccfInfo: CCF-A ICDE; citationNumber: 201            | /unread; Fuels; Global Positioning System; Indexes; Roads; Trajectory; Vehicles                                                                                                                                                                                                                                                                                                                                                        |
| Learning path personalization and recommendation methods: A survey of the state-of-the-art                                                                      | Nabizadeh, Amir Hossein; Leal, José Paulo; Rafsanjani, Hamed N.; Shah, Rajiv Ratn                                                                                       | 2020             | journalArticle  | Expert Systems with Applications                                                                                                                                                                    | https://linkinghub.elsevier.com/retrieve/pii/S0957417420304206                                                                                                                                                                                                                |                                     | 09574174                        | 10.1016/j.eswa.2020.113596            | A learning path is the implementation of a curriculum design. It consists of a set of learning activities that help users achieve particular learning goals. Personalizing these paths became a signiﬁcant task due to differences in users’ limitations, backgrounds, goals, etc. Since the last decade, researchers have proposed a variety of learning path personalization methods using different techniques and approaches. In this paper, we present an overview of the methods that are applied to personalize learning paths as well as their advantages and disadvantages. The main parameters for personalizing learning paths are also described. In addition, we present approaches that are used to evaluate path personalization methods. Finally, we highlight the most signiﬁcant challenges of these methods, which need to be tackled in order to enhance the quality of the personalization.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | 2020-11                                                                                                                                                                                 | 2024-11-02 06:42:25                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:24                                                                                                                                  | 2024-11-02 06:42:25                                                                                                                                                                                                                                                                                                                                 | 113596                                                                                                                                                                                           | ccfInfo: CCF-C ESWA; citationNumber: 77             | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| NeuroMLR: Robust & Reliable Route Recommendation on Road Networks                                                                                               | Jain, Jayant; Bagadia, Vrittika; Manchanda, Sahil; Ranu, Sayan                                                                                                          | 2021             | conferencePaper | Advances in Neural Information Processing Systems                                                                                                                                                   | https://proceedings.neurips.cc/paper/2021/hash/b922ede9c9eb9eabec1c1fecbdecb45d-Abstract.html                                                                                                                                                                                 |                                     |                                 |                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | 2021                                                                                                                                                                                    | 2024-11-02 04:40:19                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:27                                                                                                                                  | 2024-11-02 04:40:19                                                                                                                                                                                                                                                                                                                                 | 22070–22082                                                                                                                                                                                      | ccfInfo: CCF-A NeurIPS; citationNumber: 10          | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Personalized route recommendation through historical travel behavior analysis                                                                                   | de Oliveira e Silva, Rodrigo Augusto; Cui, Ge; Rahimi, Seyyed Mohammadreza; Wang, Xin                                                                                   | 2022             | journalArticle  | GeoInformatica                                                                                                                                                                                      | https://doi.org/10.1007/s10707-021-00453-y                                                                                                                                                                                                                                    |                                     | 1573-7624                       | 10.1007/s10707-021-00453-y            | Popular navigation applications and services optimize routes based on either distance or time, disregarding drivers’ preferences when suggesting routes. Various unknown circumstances may affect users’ travel behaviors between two locations on the road network, hence it is complicated to provide satisfactory personalized route recommendations. In this paper, it is believed that users’ travel behaviors are implicitly reflected and can be learned from their historical Global Positioning System (GPS) trajectories. The Behavior-based Route Recommendation (BR2) method is proposed to compute personalized routes based exclusively on users’ travel preferences. The concepts of appearance and transition behaviors are defined to describe users’ travel behaviors. The behaviors are extracted from users’ past travels and the missing behaviors, of unvisited locations, are estimated with the Optimized Random Walk with Restart technique. Furthermore, the temporal dependency of travel behaviors is considered by constructing a time difference interval histogram. A behavior graph is generated to allow the maximum probability route computation with the shortest path algorithm, resulting in the most likely route to be taken by a user. An extension is proposed, named BR2+, to better consider the temporal dependency and incorporate distance in the recommendation process. Experiments conducted on two real GPS trajectory data sets demonstrate the efficiency and effectiveness of the proposed method. In addition, a web-based geographic information system (GIS) called MPR is implemented to demonstrate differences in route recommendation when time, distance, or users’ preferences are considered, besides providing insight about users’ movement through data visualization of their spatial and temporal coverage.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | 2022-07-01                                                                                                                                                                              | 2024-11-02 08:22:34                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:29                                                                                                                                  | 2024-11-02 08:22:34                                                                                                                                                                                                                                                                                                                                 | 505-540                                                                                                                                                                                          | ccfInfo: CCF-B; citationNumber: 4                   | /unread; GPS trajectories; Personalized travel route recommendation; Random walk with restart; Temporal dependency                                                                                                                                                                                                                                                                                                                     |
| FineRoute: Personalized and Time-Aware Route Recommendation Based on Check-Ins                                                                                  | Zhu, Xiaoyan; Hao, Ripei; Chi, Haotian; Du, Xiaojiang                                                                                                                   | 2017             | journalArticle  | IEEE Transactions on Vehicular Technology                                                                                                                                                           | https://ieeexplore.ieee.org/document/8078272/?arnumber=8078272                                                                                                                                                                                                                |                                     | 1939-9359                       | 10.1109/TVT.2017.2764999              | The rapid expansion of urbanization and the fast pace of life result in abundant choices with little time for people to manage routes. A proper planning route enables us to enjoy life better with fewer time and energy costs. Therefore, route planning becomes too valuable to be ignored. At the same time, with the popularity of mobile devices, location sensing, and Web 2.0 technologies, location-based social networks (e.g., Facebook Palaces and Foursquare) have attracted millions of users to share their visited locations and other information, which generates large amounts of user check-in data. These data can be used to mine users' preferences and time information for recommending routes. In this paper, we propose FineRoute, a personalized and time-sensitive route recommendation system. We take three factors that user' preferences, proper visiting time, and transition time into consideration for the route generation. First, we infer users' preferences by constructing a three-dimensional tensor, with three dimensions representing users, locations, and time, respectively. Second, we obtain the proper visiting time for certain locations, as well as the transition time between two locations from the check-in dataset. Moreover, we adopt Kullback-Leibler divergence in order to measure the quality of a route in terms of the proper visiting month and the proper visiting hour. Finally, we propose a route generation algorithm by extending the classic longest path algorithm. We conduct experiments on a real-world check-in dataset and the results demonstrate the effectiveness of our scheme.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | 2017-11                                                                                                                                                                                 | 2024-11-02 01:47:22                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:04                                                                                                                                  | 2024-11-02 01:47:22                                                                                                                                                                                                                                                                                                                                 | 10461-10469                                                                                                                                                                                      | ccfInfo: CCF-None TVT; citationNumber: 22           | /unread; Collaboration; Trajectory; Check-in data; History; location-based social networks; Planning; Position measurement; route recommendation; Social network services; Tensile stress; tensor                                                                                                                                                                                                                                      |
| Model poisoning attack in differential privacy-based federated learning                                                                                         | Yang, Ming; Cheng, Hang; Chen, Fei; Liu, Ximeng; Wang, Meiqing; Li, Xibin                                                                                               | 2023             | journalArticle  | Information Sciences                                                                                                                                                                                | https://linkinghub.elsevier.com/retrieve/pii/S0020025523002141                                                                                                                                                                                                                |                                     | 00200255                        | 10.1016/j.ins.2023.02.025             | Although federated learning can provide privacy protection for individual raw data, some studies have shown that the shared parameters or gradients under federated learning may still reveal user privacy. Differential privacy is a promising solution to the above problem due to its small computational overhead. At present, differential privacy-based federated learning generally focuses on the trade-off between privacy and model convergence. Even though differential privacy obscures sensitive information by adding a controlled amount of noise to the confidential data, it opens a new door for model poisoning attacks: attackers can use noise to escape anomaly detection. In this paper, we propose a novel model poisoning attack called Model Shuffle Attack (MSA), which designs a unique way to shuffle and scale the model parameters. If we treat the model as a black box, it behaves like a benign model on test set. Unlike other model poisoning attacks, the malicious model after MSA has high accuracy on test set while reducing the global model convergence speed and even causing the model to diverge. Extensive experiments show that under FedAvg and robust aggregation rules, MSA is able to significantly degrade performance of the global model while guaranteeing stealthiness. 尽管联合学习可以为单个原始数据提供隐私保护，但一些研究表明，联合学习下的共享参数或梯度仍有可能泄露用户隐私。由于计算开销较小，差分隐私是解决上述问题的一个很有前景的方案。目前，基于差分隐私的联合学习一般侧重于隐私和模型收敛性之间的权衡。尽管差分隐私通过在机密数据中添加一定量的噪声来掩盖敏感信息，但它为模型中毒攻击打开了一扇新的门：攻击者可以利用噪声逃避异常检测。在本文中，我们提出了一种名为 “模型洗牌攻击”（MSA）的新型模型中毒攻击，它设计了一种独特的方法来洗牌和缩放模型参数。如果我们将模型视为黑盒，它在测试集上就会表现得像一个良性模型。与其他模型中毒攻击不同，MSA 后的恶意模型在测试集上具有很高的准确率，同时降低了全局模型的收敛速度，甚至导致模型发散。大量实验表明，在 FedAvg 和稳健聚合规则下，MSA 能够显著降低全局模型的性能，同时保证隐蔽性。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | 2023-06                                                                                                                                                                                 | 2024-10-25 02:54:41                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:27                                                                                                                                  | 2024-10-25 02:54:41                                                                                                                                                                                                                                                                                                                                 | 158-172                                                                                                                                                                                          | ccfInfo: CCF-B; citationNumber: 4                   | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Planning Data Poisoning Attacks on Heterogeneous Recommender Systems in a Multiplayer Setting                                                                   | Yeh, Chin-Yuan; Chen, Hsi-Wen; Yang, De-Nian; Lee, Wang-Chien; Yu, Philip S.; Chen, Ming-Syan                                                                           | 2023             | conferencePaper | 2023 IEEE 39th International Conference on Data Engineering (ICDE)                                                                                                                                  | https://ieeexplore.ieee.org/document/10184597/?arnumber=10184597                                                                                                                                                                                                              |                                     |                                 | 10.1109/ICDE55515.2023.00193          | Data poisoning attacks against recommender systems (RecSys) often assume a single seller as the adversary. However, in reality, there are usually multiple sellers attempting to promote their items through RecSys manipulation. To obtain the best data poisoning plan, it is important for an attacker to anticipate and withstand the actions of his opponents. This work studies the problem of Multiplayer Comprehensive Attack (MCA) from the perspective of the attacker, considering the subsequent attacks by his opponents. In MCA, we target the Heterogeneous RecSys, where user-item interaction records, user social network, and item correlation graph are used for recommendations. To tackle MCA, we present the Multilevel Stackelberg Optimization over Progressive Differentiable Surrogate (MSOPDS). The Multilevel Stackelberg Optimization (MSO) method is used to form the optimum strategies by solving the Stackelberg game equilibrium between the attacker and his opponents, while the Progressive Differentiable Surrogate (PDS) addresses technical challenges in deriving gradients for candidate poisoning actions. Experiments on Heterogeneous RecSys trained with public datasets show that MSOPDS outperforms all examined prior works by up to 10.6% in average predicted ratings and up to 11.4% in HitRate@3 for an item targeted by an attacker facing one opponent. Source code provided in https://github.com/jimmy-academia/MSOPDS. 针对推荐系统（RecSys）的数据中毒攻击通常假定对手是单个卖家。但实际上，通常会有多个卖家试图通过操纵 RecSys 来推销自己的商品。为了获得最佳的数据投毒计划，攻击者必须预测并抵御对手的行动。本作品从攻击者的角度研究了多人综合攻击（MCA）问题，并考虑了对手的后续攻击。在 MCA 中，我们的目标是异构推荐系统（Heterogeneous RecSys），其中用户与物品的交互记录、用户社交网络和物品相关图被用于推荐。为了解决 MCA 问题，我们提出了渐进可微分代理多层次堆栈伯格优化法（MSOPDS）。多层次斯台克尔伯格优化（MSO）方法通过求解攻击者与其对手之间的斯台克尔伯格博弈均衡来形成最优策略，而渐进可微分代理（PDS）则解决了推导候选中毒行动梯度的技术难题。在使用公共数据集训练的异构 RecSys 上进行的实验表明，对于攻击者面对一个对手的目标项目，MSOPDS 的平均预测评分最高可达 10.6%，HitRate@3 最高可达 11.4%，优于所有已考察过的先前作品。源代码见 https://github.com/jimmy-academia/MSOPDS。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | 2023-04                                                                                                                                                                                 | 2024-10-24 02:36:54                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:30                                                                                                                                  | 2024-10-24 02:36:54                                                                                                                                                                                                                                                                                                                                 | 2510-2523                                                                                                                                                                                        | citationNumber: 0; ccfInfo: CCF-A ICDE              | /unread; Analytical models; Correlation; Data engineering; data poisoning attack; Games; graph neural network; Neural networks; recommender system; Social networking (online); Source coding; Stackelberg game                                                                                                                                                                                                                        |
| Adversarial Attacks for Black-Box Recommender Systems via Copying Transferable Cross-Domain User Profiles                                                       | Fan, Wenqi; Zhao, Xiangyu; Li, Qing; Derr, Tyler; Ma, Yao; Liu, Hui; Wang, Jianping; Tang, Jiliang                                                                      | 2023             | journalArticle  | IEEE Transactions on Knowledge and Data Engineering                                                                                                                                                 | https://ieeexplore.ieee.org/document/10114977/?arnumber=10114977                                                                                                                                                                                                              |                                     | 1558-2191                       | 10.1109/TKDE.2023.3272652             | As widely used in data-driven decision-making, recommender systems have been recognized for their capabilities to provide users with personalized services in many user-oriented online services, such as E-commerce (e.g., Amazon, Taobao, etc.) and Social Media sites (e.g., Facebook and Twitter). Recent works have shown that deep neural networks-based recommender systems are highly vulnerable to adversarial attacks, where adversaries can inject carefully crafted fake user profiles (i.e., a set of items that fake users have interacted with) into a target recommender system to promote or demote a set of target items. Instead of generating users with fake profiles from scratch, in this article, we introduce a novel strategy to obtain “fake” user profiles via copying cross-domain user profiles, where a reinforcement learning based black-box attacking framework (CopyAttack+) is developed to effectively and efficiently select cross-domain user profiles from the source domain to attack the target system. Moreover, we propose to train a local surrogate system for mimicking adversarial black-box attacks in the source domain, so as to provide transferable signals with the purpose of enhancing the attacking strategy in the target black-box recommender system. Comprehensive experiments on three real-world datasets are conducted to demonstrate the effectiveness of the proposed attacking framework. 推荐系统被广泛应用于数据驱动决策中，在许多面向用户的在线服务中，如电子商务（如亚马逊、淘宝等）和社交媒体网站（如Facebook和Twitter），推荐系统为用户提供个性化服务的能力已得到认可。最近的研究表明，基于深度神经网络的推荐系统极易受到敌方攻击，敌方可以向目标推荐系统注入精心制作的虚假用户配置文件（即一组虚假用户曾与之互动的项目），以提升或降低一组目标项目的等级。本文介绍了一种通过复制跨域用户配置文件来获取 “虚假 ”用户配置文件的新策略，而不是从头开始生成具有虚假配置文件的用户。我们开发了一个基于强化学习的黑盒攻击框架（CopyAttack+），可以有效地从源域中选择跨域用户配置文件来攻击目标系统。此外，我们还建议训练一个本地代理系统来模仿源域中的对抗性黑盒攻击，从而提供可转移的信号，以增强目标黑盒推荐系统的攻击策略。我们在三个真实数据集上进行了综合实验，以证明所提出的攻击框架的有效性。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | 2023-12                                                                                                                                                                                 | 2024-10-24 05:12:47                                                                                                                                                                                                                                                                                                                                       | 2024-12-06 21:32:56                                                                                                                                  | 2024-10-24 05:12:47                                                                                                                                                                                                                                                                                                                                 | 12415-12429                                                                                                                                                                                      | ccfInfo: CCF-A  TKDE; citationNumber: Not Found     | /unread; adversarial attacks; Behavioral sciences; black-box attacks; Closed box; Computational modeling; cross-domain recommendations; Data models; Motion pictures; Recommender systems; Reinforcement learning; trustworthy recommender systems                                                                                                                                                                                     |
| PAGCL: An unsupervised graph poisoned attack for graph contrastive learning model                                                                               | Li, Qing; Wang, Ziyue; Li, Zehao                                                                                                                                        | 2023             | journalArticle  | Future Generation Computer Systems                                                                                                                                                                  | https://linkinghub.elsevier.com/retrieve/pii/S0167739X23002558                                                                                                                                                                                                                |                                     | 0167739X                        | 10.1016/j.future.2023.07.009          | Graph-contrastive learning has aided the development of unsupervised graph representation learning, comparable to supervised models in terms of performance. However, the robustness of the graph contrastive learning model still has a bottleneck problem, most of the current adversarial attacks are supervised, and the acquisition of labels cannot be guaranteed when attacking unsupervised graph contrastive learning models. We propose an unsupervised attack method for graph contrastive learning because the traditional supervised graph adversarial attack method is unsuitable for the attack graph contrastive learning model. It combines the graph inject attack with the poison feature matrix and uses gradients in different contrast views of the poison adjacency matrix. Extensive experiments are conducted on various datasets and our method shows notable superiority among relevant methods, even compared to supervised ones. The code is publicly available at https://github. com/lizehaodashuaibi/paper. 图对比学习有助于无监督图表示学习的发展，其性能可与有监督模型相媲美。然而，图对比学习模型的鲁棒性仍然存在瓶颈问题，目前的对抗攻击大多是有监督的，在攻击无监督图对比学习模型时无法保证标签的获取。由于传统的有监督图对抗攻击方法不适合攻击图对比学习模型，因此我们提出了一种图对比学习的无监督攻击方法。它将图注入攻击与毒药特征矩阵相结合，并使用毒药邻接矩阵不同对比视图中的梯度。我们在各种数据集上进行了广泛的实验，结果表明我们的方法在相关方法中具有明显的优势，即使与有监督的方法相比也不遑多让。代码可在 https://github. com/lizehaodashuaibi/paper 上公开获取。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | 2023-12                                                                                                                                                                                 | 2024-10-24 05:11:30                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:28                                                                                                                                  | 2024-10-24 05:11:30                                                                                                                                                                                                                                                                                                                                 | 240-249                                                                                                                                                                                          | citationNumber: 1; ccfInfo: CCF-C FGCS              | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Topic-oriented Adversarial Attacks against Black-box Neural Ranking Models                                                                                      | Liu, Yu-An; Zhang, Ruqing; Guo, Jiafeng; De Rijke, Maarten; Chen, Wei; Fan, Yixing; Cheng, Xueqi                                                                        | 2023             | conferencePaper | Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval                                                                                     | https://dl.acm.org/doi/10.1145/3539618.3591777                                                                                                                                                                                                                                | 978-1-4503-9408-6                   |                                 | 10.1145/3539618.3591777               | Neural ranking models (NRMs) have attracted considerable attention in information retrieval. Unfortunately, NRMs may inherit the adversarial vulnerabilities of general neural networks, which might be leveraged by black-hat search engine optimization practitioners. Recently, adversarial attacks against NRMs have been explored in the paired attack setting, generating an adversarial perturbation to a target document for a specific query. In this paper, we focus on a more general type of perturbation and introduce the topic-oriented adversarial ranking attack task against NRMs, which aims to find an imperceptible perturbation that can promote a target document in ranking for a group of queries with the same topic. We define both static and dynamic settings for the task and focus on decision-based black-box attacks. We propose a novel framework to improve topicoriented attack performance based on a surrogate ranking model. The attack problem is formalized as a Markov decision process (MDP) and addressed using reinforcement learning. Specifically, a topic-oriented reward function guides the policy to find a successful adversarial example that can be promoted in rankings to as many queries as possible in a group. Experimental results demonstrate that the proposed framework can significantly outperform existing attack strategies, and we conclude by re-iterating that there exist potential risks for applying NRMs in the real world. 神经排序模型（NRMs）在信息检索领域备受关注。不幸的是，NRM 可能继承了一般神经网络的对抗性漏洞，这可能会被黑帽搜索引擎优化从业者利用。最近，人们在配对攻击设置中探索了针对 NRM 的对抗性攻击，即针对特定查询对目标文档产生对抗性扰动。在本文中，我们将重点放在了一种更普遍的扰动类型上，并介绍了针对 NRMs 的面向主题的对抗性排名攻击任务，其目的是找到一种不易察觉的扰动，以促进目标文档在一组具有相同主题的查询中的排名。我们定义了任务的静态和动态设置，并重点关注基于决策的黑盒攻击。我们提出了一个新颖的框架，以提高基于代理排名模型的面向主题的攻击性能。攻击问题被形式化为马尔可夫决策过程（Markov decision process，MDP），并通过强化学习来解决。具体来说，一个面向主题的奖励函数会引导策略找到一个成功的对抗示例，该示例可以在一组尽可能多的查询中提升排名。实验结果表明，所提出的框架能明显优于现有的攻击策略，最后我们重申，在现实世界中应用 NRMs 存在潜在风险。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | 2023-07-19                                                                                                                                                                              | 2024-10-24 05:10:43                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:18                                                                                                                                  | 2024-10-24 05:10:43                                                                                                                                                                                                                                                                                                                                 | 1700-1709                                                                                                                                                                                        | ccfInfo: CCF-A SIGIR; citationNumber: 6             | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Targeted Shilling Attacks on GNN-based Recommender Systems                                                                                                      | Guo, Sihan; Bai, Ting; Deng, Weihong                                                                                                                                    | 2023             | conferencePaper | Proceedings of the 32nd ACM International Conference on Information and Knowledge Management                                                                                                        | https://dl.acm.org/doi/10.1145/3583780.3615073                                                                                                                                                                                                                                | 979-8-4007-0124-5                   |                                 | 10.1145/3583780.3615073               | GNN-based recommender systems have shown their vulnerability to shilling attacks in recent studies. By conducting shilling attacks on recommender systems, the attackers aim to have homogeneous impacts on all users. However, such indiscriminate attacks suffer from a waste of resources because even if the target item is promoted to users who are not interested, they are unlikely to click on them. In this paper, we conduct targeted shilling attacks in GNN-based recommender systems. By automatically constructing the features and edges of the fake users, our proposed framework AutoAttack achieves accurate attacks on a specific group of users while minimizing the impact on non-target users. Specifically, the features of fake users are generated based on a similarity function, which is optimized according to the features of target users. The structure of fake users is learned by conducting spectral clustering on the target users based on their graph Laplacian matrix, which contains the degree and adjacency information that provides guidance to the edge generation of fake users. We conduct extensive experiments on four real-world datasets in different GNN-based RS and evaluate the performance of our method on the shilling attack and recommendation tasks comprehensively, showing the effectiveness and flexibility of our framework. 在最近的研究中，基于 GNN 的推荐系统显示出其易受 “托攻击的弱点。通过对推荐系统进行 “贿赂 ”攻击，攻击者的目的是对所有用户产生同质影响。然而，这种无差别攻击会造成资源浪费，因为即使向不感兴趣的用户推广目标项目，他们也不太可能点击。在本文中，我们在基于 GNN 的推荐系统中进行了有针对性的 “推销 ”攻击。通过自动构建虚假用户的特征和边缘，我们提出的框架 AutoAttack 可以实现对特定用户群的精确攻击，同时将对非目标用户的影响降至最低。具体来说，虚假用户的特征是基于相似性函数生成的，该函数根据目标用户的特征进行了优化。虚假用户的结构是通过对目标用户进行基于其图拉普拉卡矩阵的光谱聚类来学习的，该矩阵包含度和邻接信息，可为虚假用户的边缘生成提供指导。我们在基于 GNN 的不同 RS 中的四个真实世界数据集上进行了广泛的实验，并全面评估了我们的方法在假冒用户攻击和推荐任务上的性能，展示了我们框架的有效性和灵活性。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 2023-10-21                                                                                                                                                                              | 2024-10-24 05:12:12                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:17                                                                                                                                  | 2024-10-24 05:12:12                                                                                                                                                                                                                                                                                                                                 | 649-658                                                                                                                                                                                          | ccfInfo: CCF-B CIKM; citationNumber: 0              | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Poisoning Self-supervised Learning Based Sequential Recommendations                                                                                             | Wang, Yanling; Liu, Yuchen; Wang, Qian; Wang, Cong; Li, Chenliang                                                                                                       | 2023             | conferencePaper | Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval                                                                                     | https://dl.acm.org/doi/10.1145/3539618.3591751                                                                                                                                                                                                                                | 978-1-4503-9408-6                   |                                 | 10.1145/3539618.3591751               | Self-supervised learning (SSL) has been recently applied to sequential recommender systems to provide high-quality user representations. However, while facilitating the learning process recommender systems, SSL is not without security threats: carefully crafted inputs can poison the pre-trained models driven by SSL, thus reducing the effectiveness of the downstream recommendation model. This work shows that poisoning attacks against the pretraining stage threaten sequential recommender systems. Without any background knowledge of the model architecture and parameters, nor any API queries, our strategy proves the feasibility of poisoning attacks on mainstream SSL-based recommender schemes as well as on commonly used datasets. By injecting only a tiny amount of fake users, we get the target item recommended to real users more than thousands of times as before, demonstrating that recommender systems have a new attack surface due to SSL. We further show our attack is challenging for recommendation platforms to detect and defend. Our work highlights the weakness of self-supervised recommender systems and shows the necessity for researchers to be aware of this security threat. Our source code is available at https://github.com/CongGroup/Poisoning-SSL-basedRS. 自监督学习（SSL）最近被应用于顺序推荐系统，以提供高质量的用户表征。然而，在促进推荐系统学习过程的同时，自监督学习并非没有安全威胁：精心设计的输入可能会毒害由自监督学习驱动的预训练模型，从而降低下游推荐模型的有效性。这项研究表明，针对预训练阶段的中毒攻击会威胁到顺序推荐系统。在对模型架构和参数没有任何背景知识，也没有任何应用程序接口查询的情况下，我们的策略证明了对基于 SSL 的主流推荐程序以及常用数据集进行中毒攻击的可行性。只需注入极少量的虚假用户，我们就能像以前一样向真实用户推荐目标项目数千次以上，这表明由于 SSL 的存在，推荐系统有了新的攻击面。我们进一步表明，我们的攻击对推荐平台的检测和防御具有挑战性。我们的工作凸显了自监督推荐系统的弱点，表明研究人员有必要意识到这一安全威胁。我们的源代码可在 https://github.com/CongGroup/Poisoning-SSL-basedRS 上获取。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | 2023-07-19                                                                                                                                                                              | 2024-10-24 03:10:26                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:11                                                                                                                                  | 2024-10-24 03:10:26                                                                                                                                                                                                                                                                                                                                 | 300-310                                                                                                                                                                                          | citationNumber: 1; ccfInfo: CCF-A SIGIR             | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| The Dark Side of Explanations: Poisoning Recommender Systems with Counterfactual Examples                                                                       | Chen, Ziheng; Silvestri, Fabrizio; Wang, Jia; Zhang, Yongfeng; Tolomei, Gabriele                                                                                        | 2023             | conferencePaper | Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval                                                                                     | https://dl.acm.org/doi/10.1145/3539618.3592070                                                                                                                                                                                                                                | 978-1-4503-9408-6                   |                                 | 10.1145/3539618.3592070               | Deep learning-based recommender systems have become an integral part of several online platforms. However, their black-box nature emphasizes the need for explainable artificial intelligence (XAI) approaches to provide human-understandable reasons why a specific item gets recommended to a given user. One such method is counterfactual explanation (CF). While CFs can be highly beneficial for users and system designers, malicious actors may also exploit these explanations to undermine the system’s security. In this work, we propose H-CARS, a novel strategy to poison recommender systems via CFs. Specifically, we first train a logicalreasoning-based surrogate model on training data derived from counterfactual explanations. By reversing the learning process of the recommendation model, we thus develop a proficient greedy algorithm to generate fabricated user profiles and their associated interaction records for the aforementioned surrogate model. Our experiments, which employ a well-known CF generation method and are conducted on two distinct datasets, show that H-CARS yields significant and successful attack performance. 基于深度学习的推荐系统已成为多个在线平台不可或缺的一部分。然而，它们的黑箱性质强调了对可解释人工智能（XAI）方法的需求，以提供人类可理解的原因，说明为什么特定项目会被推荐给特定用户。反事实解释（CF）就是这样一种方法。虽然反事实解释对用户和系统设计者大有裨益，但恶意行为者也可能利用这些解释来破坏系统的安全性。在这项工作中，我们提出了 H-CARS，一种通过反事实解释毒害推荐系统的新策略。具体来说，我们首先在反事实解释的训练数据上训练一个基于逻辑推理的代理模型。通过逆转推荐模型的学习过程，我们开发出一种熟练的贪婪算法，为上述代理模型生成虚构的用户配置文件及其相关的交互记录。我们的实验采用了一种著名的 CF 生成方法，并在两个不同的数据集上进行了实验，结果表明 H-CARS 具有显著而成功的攻击性能。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | 2023-07-19                                                                                                                                                                              | 2024-10-25 02:53:33                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:18                                                                                                                                  | 2024-10-25 02:53:33                                                                                                                                                                                                                                                                                                                                 | 2426-2430                                                                                                                                                                                        | ccfInfo: CCF-A SIGIR; citationNumber: 12            | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Targeted Data Poisoning Attack on News Recommendation System by Content Perturbation                                                                            | Zhang, Xudong; Wang, Zan; Zhao, Jingke; Wang, Lanjun                                                                                                                    | 2022             | preprint        |                                                                                                                                                                                                     | http://arxiv.org/abs/2203.03560                                                                                                                                                                                                                                               |                                     |                                 |                                       | News Recommendation System(NRS) has become a fundamental technology to many online news services. Meanwhile, several studies show that recommendation systems(RS) are vulnerable to data poisoning attacks, and the attackers have the ability to mislead the system to perform as their desires. A widely studied attack approach, injecting fake users, can be applied on the NRS when the NRS is treated the same as the other systems whose items are fixed. However, in the NRS, as each item (i.e. news) is more informative, we propose a novel approach to poison the NRS, which is to perturb contents of some browsed news that results in the manipulation of the rank of the target news. Intuitively, an attack is useless if it is highly likely to be caught, i.e., exposed. To address this, we introduce a notion of the exposure risk and propose a novel problem of attacking a history news dataset by means of perturbations where the goal is to maximize the manipulation of the target news rank while keeping the risk of exposure under a given budget. We design a reinforcement learning framework, called TDP-CP, which contains a two-stage hierarchical model to reduce the searching space. Meanwhile, influence estimation is also applied to save the time on retraining the NRS for rewards. We test the performance of TDP-CP under three NRSs and on different target news. Our experiments show that TDP-CP can increase the rank of the target news successfully with a limited exposure budget. 新闻推荐系统（NRS）已成为许多在线新闻服务的基础技术。同时，一些研究表明，推荐系统（RS）很容易受到数据中毒攻击，攻击者有能力误导系统按照自己的意愿运行。一种被广泛研究的攻击方法是注入虚假用户，当 NRS 被视为与其他项目固定的系统相同时，这种方法也可应用于 NRS。然而，在 NRS 中，由于每个条目（即新闻）的信息量更大，我们提出了一种新的方法来毒害 NRS，即扰乱某些浏览新闻的内容，从而操纵目标新闻的排名。直观地说，如果攻击极有可能被发现，即被暴露，那么这种攻击就是无用的。为了解决这个问题，我们引入了暴露风险的概念，并提出了一个通过扰动攻击历史新闻数据集的新问题，其目标是最大限度地操纵目标新闻的排名，同时将暴露风险控制在给定的预算范围内。我们设计了一个名为 TDP-CP 的强化学习框架，其中包含一个两阶段分层模型，以缩小搜索空间。同时，我们还应用了影响估计来节省重新训练 NRS 以获得奖励的时间。我们测试了 TDP-CP 在三种 NRS 和不同目标新闻下的性能。实验结果表明，TDP-CP 能在有限的曝光预算内成功提高目标新闻的排名。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | 2022-03-10                                                                                                                                                                              | 2024-10-25 02:58:54                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:17                                                                                                                                  | 2024-10-25 02:58:54                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                  | citationNumber: 0; ccfInfo: CCF-None CORR           | /unread; Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security; Computer Science - Information Retrieval; Computer Science - Machine Learning                                                                                                                                                                                                                                                       |
| PipAttack: Poisoning Federated Recommender Systems for Manipulating Item Promotion                                                                              | Zhang, Shijie; Yin, Hongzhi; Chen, Tong; Huang, Zi; Nguyen, Quoc Viet Hung; Cui, Lizhen                                                                                 | 2022             | conferencePaper | Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining                                                                                                             | https://dl.acm.org/doi/10.1145/3488560.3498386                                                                                                                                                                                                                                | 978-1-4503-9132-0                   |                                 | 10.1145/3488560.3498386               | Due to the growing privacy concerns, decentralization emerges rapidly in personalized services, especially recommendation. Also, recent studies have shown that centralized models are vulnerable to poisoning attacks, compromising their integrity. In the context of recommender systems, a typical goal of such poisoning attacks is to promote the adversary’s target items by interfering with the training dataset and/or process. Hence, a common practice is to subsume recommender systems under the decentralized federated learning paradigm, which enables all user devices to collaboratively learn a global recommender while retaining all the sensitive data locally. Without exposing the full knowledge of the recommender and entire dataset to end-users, such federated recommendation is widely regarded ‘safe’ towards poisoning attacks. In this paper, we present a systematic approach to backdooring federated recommender systems for targeted item promotion. The core tactic is to take advantage of the inherent popularity bias that commonly exists in data-driven recommenders. As popular items are more likely to appear in the recommendation list, our innovatively designed attack model enables the target item to have the characteristics of popular items in the embedding space. Then, by uploading carefully crafted gradients via a small number of malicious users during the model update, we can effectively increase the exposure rate of a target (unpopular) item in the resulted federated recommender. Evaluations on two real-world datasets show that 1) our attack model significantly boosts the exposure rate of the target item in a stealthy way, without harming the accuracy of the poisoned recommender; and 2) existing defenses are not effective enough, highlighting the need for new defenses against our local model poisoning attacks to federated recommender systems. 由于隐私问题日益受到关注，去中心化在个性化服务（尤其是推荐）中迅速兴起。最近的研究还表明，集中式模型很容易受到中毒攻击，从而破坏其完整性。在推荐系统中，这种中毒攻击的典型目标是通过干扰训练数据集和/或过程来提升对手的目标项目。因此，常见的做法是将推荐系统归入分散式联合学习范式，使所有用户设备都能协同学习全局推荐，同时在本地保留所有敏感数据。由于不会向终端用户暴露推荐器和整个数据集的全部知识，这种联合推荐被广泛认为对中毒攻击是 “安全的”。在本文中，我们提出了一种系统化的方法，用于对联合推荐系统进行反向操作，以实现有针对性的项目推广。其核心策略是利用数据驱动推荐器中普遍存在的固有流行度偏差。由于热门项目更有可能出现在推荐列表中，我们创新设计的攻击模型使目标项目在嵌入空间中具有热门项目的特征。然后，在模型更新过程中，通过少数恶意用户上传精心制作的梯度，我们就能有效提高目标（不受欢迎）项目在最终联合推荐器中的曝光率。在两个真实数据集上进行的评估表明：1）我们的攻击模型以隐蔽的方式显著提高了目标项目的曝光率，而不会损害中毒推荐器的准确性；2）现有的防御措施不够有效，这凸显了针对联合推荐系统本地模型中毒攻击的新防御措施的必要性。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | 2022-02-11                                                                                                                                                                              | 2024-10-24 05:14:33                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:29                                                                                                                                  | 2024-10-24 05:14:33                                                                                                                                                                                                                                                                                                                                 | 1415-1423                                                                                                                                                                                        | citationNumber: 10; ccfInfo: CCF-B WSDM             | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Reverse Attack: Black-box Attacks on Collaborative Recommendation                                                                                               | Zhang, Yihe; Yuan, Xu; Li, Jin; Lou, Jiadong; Chen, Li; Tzeng, Nian-Feng                                                                                                | 2021             | conferencePaper | Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security                                                                                                               | https://dl.acm.org/doi/10.1145/3460120.3484805                                                                                                                                                                                                                                | 978-1-4503-8454-4                   |                                 | 10.1145/3460120.3484805               | Collaborative filtering (CF) recommender systems have been extensively developed and widely deployed in various social websites, promoting products or services to the users of interest. Meanwhile, work has been attempted at poisoning attacks to CF recommender systems for distorting the recommend results to reap commercial or personal gains stealthily. While existing poisoning attacks have demonstrated their effectiveness with the offline social datasets, they are impractical when applied to the real setting on online social websites. This paper develops a novel and practical poisoning attack solution toward the CF recommender systems without knowing involved specific algorithms nor historical social data information a priori. Instead of directly attacking the unknown recommender systems, our solution performs certain operations on the social websites to collect a set of sampling data for use in constructing a surrogate model for deeply learning the inherent recommendation patterns. This surrogate model can estimate the item proximities, learned by the recommender systems. By attacking the surrogate model, the corresponding solutions (for availability and target attacks) can be directly migrated to attack the original recommender systems. Extensive experiments validate the generated surrogate model’s reproductive capability and demonstrate the effectiveness of our attack upon various CF recommender algorithms. 协作过滤（CF）推荐系统已被广泛开发并广泛应用于各种社交网站，向感兴趣的用户推广产品或服务。与此同时，也有人尝试对 CF 推荐系统进行中毒攻击，以歪曲推荐结果，偷偷获取商业或个人利益。虽然现有的中毒攻击已在离线社交数据集上证明了其有效性，但将其应用到在线社交网站的真实环境中并不实用。本文针对 CF 推荐系统开发了一种新颖实用的中毒攻击解决方案，它既不涉及具体算法，也不涉及先验的历史社交数据信息。我们的解决方案不是直接攻击未知的推荐系统，而是在社交网站上执行某些操作，收集一组采样数据，用于构建一个代用模型，深入学习固有的推荐模式。这种代理模型可以估算出推荐系统学习到的项目邻近度。通过攻击代理模型，相应的解决方案（可用性和目标攻击）可以直接迁移到攻击原始推荐系统。大量实验验证了生成的代理模型的复制能力，并证明了我们对各种 CF 推荐算法的攻击效果。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | 2021-11-12                                                                                                                                                                              | 2024-10-24 03:25:39                                                                                                                                                                                                                                                                                                                                       | 2024-12-06 22:31:48                                                                                                                                  | 2024-10-24 03:25:39                                                                                                                                                                                                                                                                                                                                 | 51-68                                                                                                                                                                                            | ccfInfo: CCF-A CCS; citationNumber: 13              | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Poisoning GNN-based Recommender Systems with Generative Surrogate-based Attacks                                                                                 | Nguyen Thanh, Toan; Quach, Nguyen Duc Khang; Nguyen, Tam; Huynh, Thanh; Vu, Viet; Nguyen, Phi Le; Jo, Jun; Hung, Nguyen                                                 | 2022             | journalArticle  | ACM Transactions on Information Systems                                                                                                                                                             |                                                                                                                                                                                                                                                                               |                                     |                                 | 10.1145/3567420                       | With recent advancements in graph neural networks (GNN), GNN-based recommender systems (gRS) have achieved remarkable success in the past few years. Despite this success, existing research reveals that gRSs are still vulnerable to poison attacks , in which the attackers inject fake data to manipulate recommendation results as they desire. This might be due to the fact that existing poison attacks (and countermeasures) are either model-agnostic or specifically designed for traditional recommender algorithms (e.g., neighbourhood-based, matrix-factorisation-based, or deep-learning-based RSs) that are not gRS. As gRSs are widely adopted in the industry, the problem of how to design poison attacks for gRSs has become a need for robust user experience. Herein, we focus on the use of poison attacks to manipulate item promotion in gRSs. Compared to standard GNNs, attacking gRSs is more challenging due to the heterogeneity of network structure and the entanglement between users and items. To overcome such challenges, we propose GSPAttack – a generative surrogate-based poison attack framework for gRSs. GSPAttack tailors a learning process to surrogate a recommendation model as well as generate fake users and user-item interactions while preserving the data correlation between users and items for recommendation accuracy. Although maintaining high accuracy for other items rather than the target item seems counterintuitive, it is equally crucial to the success of a poison attack. Extensive evaluations on four real-world datasets revealed that GSPAttack outperforms all baselines with competent recommendation performance and is resistant to various countermeasures. 随着图神经网络（GNN）的最新进展，基于 GNN 的推荐系统（gRS）在过去几年中取得了显著的成功。尽管取得了这些成就，但现有研究表明，gRS 仍然容易受到中毒攻击，即攻击者通过注入虚假数据来操纵推荐结果。这可能是由于现有的中毒攻击（和应对措施）要么与模型无关，要么是专门为传统推荐算法（如基于邻域、矩阵因子或深度学习的 RS）设计的，而这些算法并非 gRS。随着 gRS 被业界广泛采用，如何为 gRS 设计中毒攻击已成为强大用户体验的需要。在此，我们将重点研究如何利用毒药攻击来操纵 gRS 中的项目推广。与标准的 GNN 相比，由于网络结构的异质性以及用户与物品之间的纠缠，攻击 gRS 更具挑战性。为了克服这些挑战，我们提出了 GSPAttack--一种基于生成代理的 gRS 毒药攻击框架。GSPAttack 可定制学习过程，以代理推荐模型，并生成虚假用户和用户与项目的交互，同时保留用户与项目之间的数据相关性，以保证推荐的准确性。虽然保持其他项目而非目标项目的高准确性似乎有悖常理，但这对毒药攻击的成功同样至关重要。在四个真实数据集上进行的广泛评估表明，GSPAttack 的推荐性能优于所有基线方法，并能抵御各种反制措施。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | 2022-10-19                                                                                                                                                                              | 2024-10-24 03:19:24                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:32                                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                  | ccfInfo: CCF-A  TOIS; citationNumber: 8             | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Triple Adversarial Learning for Influence based Poisoning Attack in Recommender Systems                                                                         | Wu, Chenwang; Lian, Defu; Ge, Yong; Zhu, Zhihao; Chen, Enhong                                                                                                           | 2021             | conferencePaper | Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining                                                                                                                  | https://dl.acm.org/doi/10.1145/3447548.3467335                                                                                                                                                                                                                                | 978-1-4503-8332-5                   |                                 | 10.1145/3447548.3467335               | As an important means to solve information overload, recommender  systems have been widely applied in many fields, such as e-commerce  and advertising. However, recent studies have shown that recommender systems are vulnerable to poisoning attacks; that is, injecting a group of carefully designed user profiles into the recommender  system can severely affect recommendation quality. Despite the  development from shilling attacks to optimization-based attacks,  the imperceptibility and harmfulness of the generated data in most  attacks are arduous to balance. To this end, we propose a triple  adversarial learning for influence based poisoning attack (TrialAttack), a flexible end-to-end poisoning framework to generate nonnotable and harmful user profiles. Specifically, given the input noise,  TrialAttack directly generates malicious users through triple adversarial learning of the generator, discriminator, and influence  module. Besides, to provide reliable influence for TrialAttack training, we explore a new approximation approach for estimating each  fake user’s influence. Through theoretical analysis, we prove that  the distribution characterized by TrialAttack approximates to the  rating distribution of real users under the premise of performing  an efficient attack. This property allows the injected users to attack  in an unremarkable way. Experiments on three real-world datasets  show that TrialAttack’s attack performance outperforms state-ofthe-art attacks, and the generated fake profiles are more difficult to  detect compared to baselines. 作为解决信息过载问题的重要手段，推荐系统已被广泛应用于电子商务和广告等多个领域。然而，最近的研究表明，推荐系统很容易受到中毒攻击，即向推荐系统注入一组精心设计的用户配置文件，会严重影响推荐质量。尽管从 “shilling ”攻击发展到了基于优化的攻击，但大多数攻击中生成数据的不可感知性和有害性难以平衡。为此，我们提出了基于影响的三重对抗学习中毒攻击（TrialAttack），这是一种灵活的端到端中毒框架，可生成不可察觉且有害的用户配置文件。具体来说，在给定输入噪声的情况下，TrialAttack 通过生成器、判别器和影响模块的三重对抗学习直接生成恶意用户。此外，为了给 TrialAttack 训练提供可靠的影响力，我们探索了一种新的近似方法来估计每个虚假用户的影响力。通过理论分析，我们证明了在进行有效攻击的前提下，TrialAttack 所描述的分布近似于真实用户的评分分布。这一特性允许注入用户以一种不引人注目的方式进行攻击。在三个真实数据集上进行的实验表明，TrialAttack 的攻击性能优于最先进的攻击，而且与基线相比，生成的虚假配置文件更难检测。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | 2021-08-14                                                                                                                                                                              | 2024-10-24 04:33:05                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:18                                                                                                                                  | 2024-10-24 04:33:05                                                                                                                                                                                                                                                                                                                                 | 1830-1840                                                                                                                                                                                        | ccfInfo: CCF-A SIGKDD; citationNumber: 45           | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Attacking Recommender Systems With Plausible Profile                                                                                                            | Zhang, Xuxin; Chen, Jian; Zhang, Rui; Wang, Chen; Liu, Ling                                                                                                             | 2021             | journalArticle  | IEEE Transactions on Information Forensics and Security                                                                                                                                             | https://ieeexplore.ieee.org/document/9555630/?arnumber=9555630                                                                                                                                                                                                                |                                     | 1556-6021                       | 10.1109/TIFS.2021.3117078             | Recommender systems (RS) have become an essential component of web services due to their excellent performance. Despite their great success, RS have proved to be vulnerable to data poisoning attacks, which inject well-crafted fake profiles into RS, so that the target items can be maliciously recommended. In this paper, we first reveal that existing poisoning attacks in RS can be detected effortlessly, as the features of the generated fake profiles cannot be inconsistent with those of normal profiles all the time. We further propose RecUP, a poisoning attack in RS that can generate plausible profiles whose features stay almost the same as the normal ones, based on Generative Adversarial Networks (GAN). To tailor GAN for poisoning in RS, we develop HRGAN and devise a loss function to guide the training of the generator, along with a masking operation with selected potentially powerful profiles, so that the final generated profiles can perform malicious recommendations as expected. Evaluations against various defense methods using three real-world datasets show that, RecUP can generate the most plausible profiles while maintaining comparable attacking performance compared with state-of-the-art attacks. 推荐系统（RS）因其卓越的性能已成为网络服务的重要组成部分。尽管取得了巨大的成功，但事实证明，推荐系统很容易受到数据中毒攻击，这种攻击会将精心制作的虚假资料注入推荐系统，从而恶意推荐目标项目。在本文中，我们首先揭示了 RS 中现有的中毒攻击可以毫不费力地检测出来，因为生成的虚假配置文件的特征不可能始终与正常配置文件的特征不一致。我们进一步提出了 RecUP，一种基于生成对抗网络（GAN）的 RS 中毒攻击，它能生成与正常配置文件特征几乎相同的可信配置文件。为了使 GAN 适合 RS 中的中毒，我们开发了 HRGAN，并设计了一个损失函数来指导生成器的训练，同时使用选定的潜在强大配置文件进行掩蔽操作，这样最终生成的配置文件就能像预期的那样执行恶意推荐。利用三个真实数据集对各种防御方法进行的评估表明，RecUP 可以生成最可信的配置文件，同时与最先进的攻击方法相比保持相当的攻击性能。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 2021                                                                                                                                                                                    | 2024-10-24 03:28:46                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:22                                                                                                                                  | 2024-10-24 03:28:46                                                                                                                                                                                                                                                                                                                                 | 4788-4800                                                                                                                                                                                        | ccfInfo: CCF-A TIFS; citationNumber: 14             | /unread; Recommender systems; Correlation; Feature extraction; generative adversarial network; Generative adversarial networks; Generators; shilling attack; Training; Wavelength division multiplexing                                                                                                                                                                                                                                |
| Understanding the Manipulation on Recommender Systems through Web Injection                                                                                     | Zhang, Yubao; Xiao, Jidong; Hao, Shuai; Wang, Haining; Zhu, Sencun; Jajodia, Sushil                                                                                     | 2020             | journalArticle  | IEEE Transactions on Information Forensics and Security                                                                                                                                             | https://ieeexplore.ieee.org/document/8907865/?arnumber=8907865                                                                                                                                                                                                                |                                     | 1556-6021                       | 10.1109/TIFS.2019.2954737             | Recommender systems have been increasingly used in a variety of web services, providing a list of recommended items in which a user may have an interest. While important, recommender systems are vulnerable to various malicious attacks. In this paper, we study a new security vulnerability in recommender systems caused by web injection, through which malicious actors stealthily tamper any unprotected in-transit HTTP webpage content and force victims to visit specific items in some web services (even running HTTPS), e.g., YouTube. By doing so, malicious actors can promote their targeted items in those web services. To obtain a deeper understanding on the recommender systems of our interest (including YouTube, Yelp, Taobao, and 360 App market), we first conduct a measurement-based analysis on several real-world recommender systems by leveraging machine learning algorithms. Then, web injection is implemented in three different types of devices (i.e., computer, router, and proxy server) to investigate the scenarios where web injection could occur. Based on the implementation of web injection, we demonstrate that it is feasible and sometimes effective to manipulate the real-world recommender systems through web injection. We also present several countermeasures against such manipulations. 推荐系统越来越多地应用于各种网络服务，提供用户可能感兴趣的推荐项目列表。推荐系统虽然重要，但也容易受到各种恶意攻击。在本文中，我们研究了由网页注入引起的推荐系统中的一个新安全漏洞，恶意行为者通过网页注入篡改任何未受保护的传输中 HTTP 网页内容，并强迫受害者访问某些网络服务（甚至是运行 HTTPS 的网络服务）中的特定项目，例如 YouTube。这样，恶意行为者就可以在这些网络服务中推广他们的目标项目。为了深入了解我们感兴趣的推荐系统（包括 YouTube、Yelp、淘宝和 360 应用市场），我们首先利用机器学习算法对几个真实世界的推荐系统进行了基于测量的分析。然后，在三种不同类型的设备（即电脑、路由器和代理服务器）中实施网页注入，以研究可能发生网页注入的场景。基于网页注入的实现，我们证明了通过网页注入操纵真实世界的推荐系统是可行的，有时甚至是有效的。我们还提出了针对此类操纵的几种对策。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | 2020                                                                                                                                                                                    | 2024-10-24 04:49:11                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:30                                                                                                                                  | 2024-10-24 04:49:11                                                                                                                                                                                                                                                                                                                                 | 3807-3818                                                                                                                                                                                        | citationNumber: 8; ccfInfo: CCF-A TIFS              | /unread; Recommender systems; History; Browsers; Pollution; recommendation manipulation; Videos; web injection; Web services; YouTube                                                                                                                                                                                                                                                                                                  |
| Simulating real profiles for shilling attacks: A generative approach                                                                                            | Barbieri, Julio; Alvim, Leandro G.M.; Braida, Filipe; Zimbrão, Geraldo                                                                                                  | 2021             | journalArticle  | Knowledge-Based Systems                                                                                                                                                                             | https://linkinghub.elsevier.com/retrieve/pii/S0950705121006523                                                                                                                                                                                                                |                                     | 09507051                        | 10.1016/j.knosys.2021.107390          | Collaborative Filtering (CF) approaches are vulnerable to Shilling Attacks, in which malicious users or companies inject a large number of fake profiles in a system in order to manipulate its recommendations. One problem of current Shilling Attack models is that they commonly use straightforward statistical templates, producing profiles with different rating patterns than actual system data, which facilitates its detection, requiring a larger amount of profiles to achieve its goals. To address this problem and create profiles closer to reality, we propose using a generative model, Variational Autoencoder (VAE) to map original data distribution. With VAE, it is possible to generate new profiles based on real data, without explicit copying their actual ratings. Its generated profiles are converted to malicious profiles by adding target item rating value. We test our attack model on MovieLens 100k data set and compare to literature attack models. Our results indicate that our model outperforms all other models in model-based CF system, especially using low attack sizes (from 3% to 5%). Also, analysis comparing profiles generated from it and other approaches shows that our model ratings pattern are very similar to real profiles, which may indicate that attacks mounted using our approach may be less likely to be detected by detection approaches. Thus, we show that our attack model represents an advance on Shilling Attack models, since its superior results in model-based CF and possible indistinction from real profiles may be useful as a baseline to test detection techniques and other tasks among Shilling Attack area. 协同过滤（CF）方法很容易受到 “贿赂攻击”（Shilling Attacks）的影响，恶意用户或公司会在系统中注入大量虚假资料，以操纵系统的推荐。当前 Shilling 攻击模型存在的一个问题是，它们通常使用直接的统计模板，生成的配置文件与实际系统数据的评级模式不同，这为其检测提供了便利，需要更多的配置文件才能实现其目标。为解决这一问题并创建更贴近现实的配置文件，我们建议使用一种生成模型--变异自动编码器（VAE）来映射原始数据的分布。有了 VAE，就可以根据真实数据生成新的配置文件，而无需明确复制其实际评级。其生成的配置文件通过添加目标项目评级值转换为恶意配置文件。我们在 MovieLens 100k 数据集上测试了我们的攻击模型，并与文献中的攻击模型进行了比较。结果表明，在基于模型的 CF 系统中，我们的模型优于所有其他模型，尤其是在使用较小的攻击规模（从 3% 到 5%）时。此外，通过分析比较该模型和其他方法生成的配置文件，我们发现我们的模型评级模式与真实配置文件非常相似，这可能表明使用我们的方法安装的攻击不太可能被检测方法检测到。因此，我们表明，我们的攻击模型代表了 Shilling 攻击模型的进步，因为它在基于模型的 CF 和 POS 中取得了卓越的结果。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 2021-10                                                                                                                                                                                 | 2024-10-24 04:50:23                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:16                                                                                                                                  | 2024-10-24 04:50:23                                                                                                                                                                                                                                                                                                                                 | 107390                                                                                                                                                                                           | citationNumber: 3; ccfInfo: CCF-C KBS               | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Hdgt: Heterogeneous driving graph transformer for multi-agent trajectory prediction via scene encoding                                                          | Jia, Xiaosong; Wu, Penghao; Chen, Li; Liu, Yu; Li, Hongyang; Yan, Junchi                                                                                                | 2023             | journalArticle  | IEEE transactions on pattern analysis and machine intelligence                                                                                                                                      | https://ieeexplore.ieee.org/abstract/document/10192373/                                                                                                                                                                                                                       |                                     |                                 |                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | 2023                                                                                                                                                                                    | 2024-11-04 09:41:52                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:24                                                                                                                                  | 2024-11-04 09:41:51                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                  | ccfInfo: CCF-A TPAMI; citationNumber: 51            | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Robust Federated Contrastive Recommender System against Model Poisoning Attack                                                                                  | Yuan, Wei; Yang, Chaoqun; Qu, Liang; Ye, Guanhua; Nguyen, Quoc Viet Hung; Yin, Hongzhi                                                                                  | 2024             | preprint        |                                                                                                                                                                                                     | http://arxiv.org/abs/2403.20107                                                                                                                                                                                                                                               |                                     |                                 |                                       | Federated Recommender Systems (FedRecs) have garnered increasing attention recently, thanks to their privacy-preserving benefits. However, the decentralized and open characteristics of current FedRecs present two dilemmas. First, the performance of FedRecs is compromised due to highly sparse on-device data for each client. Second, the system's robustness is undermined by the vulnerability to model poisoning attacks launched by malicious users. In this paper, we introduce a novel contrastive learning framework designed to fully leverage the client's sparse data through embedding augmentation, referred to as CL4FedRec. Unlike previous contrastive learning approaches in FedRecs that necessitate clients to share their private parameters, our CL4FedRec aligns with the basic FedRec learning protocol, ensuring compatibility with most existing FedRec implementations. We then evaluate the robustness of FedRecs equipped with CL4FedRec by subjecting it to several state-of-the-art model poisoning attacks. Surprisingly, our observations reveal that contrastive learning tends to exacerbate the vulnerability of FedRecs to these attacks. This is attributed to the enhanced embedding uniformity, making the polluted target item embedding easily proximate to popular items. Based on this insight, we propose an enhanced and robust version of CL4FedRec (rCL4FedRec) by introducing a regularizer to maintain the distance among item embeddings with different popularity levels. Extensive experiments conducted on four commonly used recommendation datasets demonstrate that CL4FedRec significantly enhances both the model's performance and the robustness of FedRecs. 联合推荐系统（Federated Recommender Systems，简称 FedRecs）因其保护隐私的优势，近来受到越来越多的关注。然而，当前 FedRecs 的分散性和开放性特点带来了两个难题。首先，由于每个客户端的设备数据非常稀疏，FedRecs 的性能受到影响。其次，由于容易受到恶意用户发起的模型中毒攻击，系统的鲁棒性受到了削弱。在本文中，我们介绍了一种新颖的对比学习框架，旨在通过嵌入增强充分利用客户端的稀疏数据，简称为 CL4FedRec。与以往需要客户端共享私人参数的 FedRec 对比学习方法不同，我们的 CL4FedRec 与基本的 FedRec 学习协议保持一致，确保与大多数现有的 FedRec 实现兼容。然后，我们对配备了 CL4FedRec 的 FedRec 进行了鲁棒性评估，使其遭受了几种最先进的模型中毒攻击。令人惊讶的是，我们的观察发现，对比学习往往会加剧 FedRec 在这些攻击面前的脆弱性。这归因于嵌入均匀性的增强，使得被污染的目标项目嵌入很容易接近流行项目。基于这一洞察力，我们提出了一种增强的、稳健的 CL4FedRec 版本（rCL4FedRec），通过引入正则器来保持不同流行度的项目嵌入之间的距离。在四个常用推荐数据集上进行的广泛实验表明，CL4FedRec 显著提高了模型的性能和 FedRec 的鲁棒性。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 2024-03-29                                                                                                                                                                              | 2024-11-06 02:17:57                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:27                                                                                                                                  | 2024-11-06 02:17:57                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                  | citationNumber: 0; ccfInfo: CCF-None CORR           | /unread; Computer Science - Information Retrieval                                                                                                                                                                                                                                                                                                                                                                                      |
| 针对协同过滤推荐系统的混淆托攻击模型                                                                                                                                              | 卫星君, 顾清华                                                                                                                                                                |                  | journalArticle  | 计算机与数字工程                                                                                                                                                                                            | http://jsj.journal.cssc709.net/CN/abstract/abstract943.shtml                                                                                                                                                                                                                  |                                     | 1672-9722                       |                                       | 为了躲避现有的托攻击检测,攻击者利用混淆技术降低攻击概貌和普通概貌之间的区别,使之成为更多用户 的近邻进而影响推荐系统的预测评分。对物品按属性划分,在混淆技术的基础上,给出噪音系数、评分偏移函数和目标偏移 函数,提出混淆流行交叉托攻击模型。设计攻击概貌自动产生器注入系统数据库,同最流行项中添加平均项构造的混淆托 攻击对比。实验结果表明,该托攻击模型对推荐系统的危害性更大,应加强防范。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |                                                                                                                                                                                         | 2024-10-25 03:04:32                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:20                                                                                                                                  | 2024-10-25 03:04:32                                                                                                                                                                                                                                                                                                                                 | 1575-1579,1696                                                                                                                                                                                   | citationNumber: 1; ccfInfo: Not Found               | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Multi-Armed-Bandit-based Shilling Attack on Collaborative Filtering Recommender Systems                                                                         | Sundar, Agnideven Palanisamy; Li, Feng; Zou, Xukai; Hu, Qin; Gao, Tianchong                                                                                             | 2020             | conferencePaper | 2020 IEEE 17th International Conference on Mobile Ad Hoc and Sensor Systems (MASS)                                                                                                                  | https://ieeexplore.ieee.org/document/9356081/                                                                                                                                                                                                                                 | 978-1-7281-9866-8                   |                                 | 10.1109/MASS50613.2020.00050          | Collaborative Filtering (CF) is a popular recommendation system that makes recommendations based on similar users’ preferences. Though it is widely used, CF is prone to Shilling/Proﬁle Injection attacks, where fake proﬁles are injected into the CF system to alter its outcome. Most of the existing shilling attacks do not work on online systems and cannot be efﬁciently implemented in real-world applications. In this paper, we introduce an efﬁcient Multi-Armed-Bandit-based reinforcement learning method to practically execute online shilling attacks. Our method works by reducing the uncertainty associated with the item selection process and ﬁnds the most optimal items to enhance attack reach. Such practical online attacks open new avenues for research in building more robust recommender systems. We treat the recommender system as a black box, making our method effective irrespective of the type of CF used. Finally, we also experimentally test our approach against popular state-of-the-art shilling attacks. 协同过滤（CF）是一种流行的推荐系统，它根据相似用户的偏好进行推荐。虽然它被广泛使用，但协同过滤系统很容易受到 “填充/信息注入 ”攻击，即在协同过滤系统中注入虚假信息以改变其结果。现有的大多数 “伪造 ”攻击都无法在在线系统中使用，也无法在实际应用中有效实施。在本文中，我们介绍了一种基于多臂比特强化学习的高效方法，用于实际执行在线 “行贿 ”攻击。我们的方法通过减少与项目选择过程相关的不确定性，找到最优项目，从而提高攻击命中率。这种实用的在线攻击为建立更强大的推荐系统开辟了新的研究途径。我们将推荐系统视为一个黑盒，因此无论使用哪种 CF，我们的方法都是有效的。最后，我们还通过实验测试了我们的方法是否能抵御流行的、最先进的shilling攻击。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | 2020-12                                                                                                                                                                                 | 2024-10-25 02:40:23                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:27                                                                                                                                  | 2024-10-25 02:40:23                                                                                                                                                                                                                                                                                                                                 | 347-355                                                                                                                                                                                          | citationNumber: 1; ccfInfo: CCF-C MASS              | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Data Poisoning Attacks on Cross-domain Recommendation                                                                                                           | Chen, Huiyuan; Li, Jing                                                                                                                                                 | 2019             | conferencePaper | Proceedings of the 28th ACM International Conference on Information and Knowledge Management                                                                                                        | https://dl.acm.org/doi/10.1145/3357384.3358116                                                                                                                                                                                                                                | 978-1-4503-6976-3                   |                                 | 10.1145/3357384.3358116               | Cross-domain recommendation has attracted growing interests given their simplicity and effectiveness. In the cross-domain scenarios, we may improve predictive accuracy in one domain by transferring knowledge from the other, which alleviates the data sparsity issue. However, the relatedness of these domains can be exploited by a malicious party to launch data poisoning attacks. Here we study the vulnerability of cross-domain recommendation under data poisoning attacks. We show that data poisoning attacks can be formulated as a bilevel optimization problem. Our experimental results show that cross-domain system can be compromised under attacks, highlighting the need for countermeasures against data poisoning attacks in cross-domain recommendation. 跨领域推荐因其简单有效而受到越来越多的关注。在跨领域应用场景中，我们可以通过从一个领域转移知识来提高另一个领域的预测准确性，从而缓解数据稀缺问题。然而，这些领域之间的关联性可能会被恶意方利用，发起数据中毒攻击。在这里，我们研究了跨域推荐在数据中毒攻击下的脆弱性。我们发现，数据中毒攻击可以被表述为一个双层优化问题。我们的实验结果表明，跨域系统可能会在攻击下受到破坏，这凸显了在跨域推荐中针对数据中毒攻击采取对策的必要性。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 2019-11-03                                                                                                                                                                              | 2024-10-24 05:08:38                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:23                                                                                                                                  | 2024-10-24 05:08:38                                                                                                                                                                                                                                                                                                                                 | 2177-2180                                                                                                                                                                                        | ccfInfo: CCF-B CIKM; citationNumber: 10             | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Shilling attack based on item popularity and rated item correlation against collaborative filtering                                                             | Chen, Keke; Chan, Patrick P. K.; Zhang, Fei; Li, Qiaoqiao                                                                                                               | 2019             | journalArticle  | International Journal of Machine Learning and Cybernetics                                                                                                                                           | https://doi.org/10.1007/s13042-018-0861-2                                                                                                                                                                                                                                     |                                     | 1868-808X                       | 10.1007/s13042-018-0861-2             | Although collaborative filtering achieves satisfying performance in recommender systems, many studies suggest that it is vulnerable by shilling attack aimed to manipulate the recommending frequency of a target item by injecting malicious user profiles. The existing attack methods usually generate malicious profiles by rating the item selected randomly. However, as these rating patterns are different from the real users, who have their own preferences on items, these attack methods can be easily detected by shilling attack detection, which significantly reduces the attack ability. Although some attack methods consider disguise ability, these methods require too much information from real users. This study proposes a shilling attack which generates malicious samples with strong attack ability and similarity to real users. To imitate the rating behavior of genuine users, our attack model considers both rated item correlation and item popularity when choosing items to rate. The profiles generated by our attack model is expected to be more similar to real user profiles, which increases the disguise ability. We also investigate whether and how rated item correlation of real user profiles is different from the ones generated by our method and the existing shilling attack. The experimental results confirm that our method achieves the highest attack ability after removing the suspected profiles identified by PCA-based and SVM-based shilling attack detection. The study confirms the correlation of rated item is a critical factor of the robustness of recommender systems. 尽管协同过滤在推荐系统中取得了令人满意的性能，但许多研究表明，协同过滤容易受到托攻击，其目的是通过注入恶意用户配置文件来操纵目标项目的推荐频率。现有的攻击方法通常是通过对随机选择的项目进行评分来生成恶意档案。然而，由于这些评分模式与真实用户不同，真实用户对物品有自己的偏好，因此这些攻击方法很容易被托攻击检测到，从而大大降低了攻击能力。虽然有些攻击方法考虑了伪装能力，但这些方法需要真实用户提供太多信息。本研究提出了一种 “托”（shilling）攻击，即生成攻击能力强且与真实用户相似的恶意样本。为了模仿真实用户的评分行为，我们的攻击模型在选择评分项目时同时考虑了评分项目相关性和项目流行度。我们的攻击模型生成的用户配置文件有望与真实用户配置文件更加相似，从而提高伪装能力。我们还研究了真实用户配置文件的评分项目相关性与我们的方法生成的配置文件和现有的托攻击是否不同以及如何不同。 实验结果证实，在剔除基于 PCA 和基于 SVM 的托攻击检测所识别出的可疑配置文件后，我们的方法获得了最高的攻击能力。这项研究证实了评级项目的相关性是影响推荐系统鲁棒性的关键因素。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | 2019-07-01                                                                                                                                                                              | 2024-10-24 05:05:01                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:15                                                                                                                                  | 2024-10-24 05:05:01                                                                                                                                                                                                                                                                                                                                 | 1833-1845                                                                                                                                                                                        | ccfInfo: CCF-None MLC; citationNumber: 20           | /unread; Collaborative filtering; Recommender systems; Artificial Intelligence; Rated item correlation; Shilling attack                                                                                                                                                                                                                                                                                                                |
| A burst-based unsupervised method for detecting review spammer groups                                                                                           | Ji, Shu-juan; Zhang, Qi; Li, Jinpeng; Chiu, Dickson K.W.; Xu, Shaohua; Yi, Lei; Gong, Maoguo                                                                            | 2020             | journalArticle  | Information Sciences                                                                                                                                                                                | https://linkinghub.elsevier.com/retrieve/pii/S0020025520305120                                                                                                                                                                                                                |                                     | 00200255                        | 10.1016/j.ins.2020.05.084             | With the development of e-commerce, online shopping has become a part of people’s life. As customers often refer to online product reviews for shopping, sellers often collude with review spammers in writing fake reviews to promote or demote target products. In particular, spammers working in groups are more harmful than individual attacks. To detect such spammer groups, previous researchers proposed some frequent item mining based algorithms and graph-based algorithms. In this paper, we propose a method called GSDB (Group Spam Detection algorithm based on review Burst). Our algorithm ﬁrst locates target products attacked by spammers by detecting the abnormality of product rating distribution. As group spammers usually post many fake reviews within a short period, we design a burst-based algorithm that discovers candidate spammer groups in reviewbursts using the Kernel Density Estimation algorithm. As some innocent reviewers may coincidently review during the burst period, we formulate a variety of individual spam indicators to measure the spamicity of the reviewers to isolate the candidate spammer groups. Finally, we design a series of group spam indicators to measure and classify the spamicity of spammer groups. Experimental results show that our proposed GSDB algorithm outperforms state-of-the-art algorithms. 随着电子商务的发展，网上购物已成为人们生活的一部分。由于顾客在购物时经常会参考网上的产品评论，因此卖家往往会与评论垃圾邮件发送者勾结，撰写虚假评论来推销或贬低目标产品。特别是，垃圾邮件发送者团伙作案比个人攻击更具危害性。为了检测这类垃圾评论群，之前的研究人员提出了一些基于频繁项挖掘的算法和基于图的算法。在本文中，我们提出了一种名为 GSDB（基于评论突发的群组垃圾邮件检测算法）的方法。我们的算法首先通过检测产品评分分布的异常情况来定位被垃圾邮件发送者攻击的目标产品。由于群组垃圾邮件发送者通常会在短时间内发布许多虚假评论，因此我们设计了一种基于突发的算法，利用核密度估计算法在评论突发中发现候选的垃圾邮件发送者群组。由于一些无辜的评论者可能会在突发期间不约而同地进行评论，因此我们制定了多种个体垃圾邮件指标来衡量评论者的动态性，从而分离出候选的垃圾邮件发送者群体。最后，我们设计了一系列群体垃圾邮件指标来衡量垃圾邮件发送者群体的活跃程度并对其进行分类。实验结果表明，我们提出的 GSDB 算法优于最先进的算法。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 2020-10                                                                                                                                                                                 | 2024-10-24 04:43:28                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:21                                                                                                                                  | 2024-10-24 04:43:28                                                                                                                                                                                                                                                                                                                                 | 454-469                                                                                                                                                                                          | ccfInfo: CCF-B; citationNumber: 35                  | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Comprehensive Privacy Analysis on Federated Recommender System against Attribute Inference Attacks                                                              | Zhang, Shijie; Yuan, Wei; Yin, Hongzhi                                                                                                                                  | 2023             | preprint        |                                                                                                                                                                                                     | http://arxiv.org/abs/2205.11857                                                                                                                                                                                                                                               |                                     |                                 |                                       | In recent years, recommender systems are crucially important for the delivery of personalized services that satisfy users' preferences. With personalized recommendation services, users can enjoy a variety of recommendations such as movies, books, ads, restaurants, and more. Despite the great benefits, personalized recommendations typically require the collection of personal data for user modelling and analysis, which can make users susceptible to attribute inference attacks. Specifically, the vulnerability of existing centralized recommenders under attribute inference attacks leaves malicious attackers a backdoor to infer users' private attributes, as the systems remember information of their training data (i.e., interaction data and side information). An emerging practice is to implement recommender systems in the federated setting, which enables all user devices to collaboratively learn a shared global recommender while keeping all the training data on device. However, the privacy issues in federated recommender systems have been rarely explored. In this paper, we first design a novel attribute inference attacker to perform a comprehensive privacy analysis of the state-of-the-art federated recommender models. The experimental results show that the vulnerability of each model component against attribute inference attack is varied, highlighting the need for new defense approaches. Therefore, we propose a novel adaptive privacy-preserving approach to protect users' sensitive data in the presence of attribute inference attacks and meanwhile maximize the recommendation accuracy. Extensive experimental results on two real-world datasets validate the superior performance of our model on both recommendation effectiveness and resistance to inference attacks. 近年来，推荐系统对于提供满足用户偏好的个性化服务至关重要。通过个性化推荐服务，用户可以享受到电影、书籍、广告、餐厅等各种推荐。尽管个性化推荐有很多好处，但通常需要收集用户个人数据来建模和分析，这可能会使用户容易受到属性推断攻击。具体来说，现有集中式推荐器在属性推断攻击下的脆弱性给恶意攻击者留下了推断用户私人属性的后门，因为系统会记住其训练数据信息（即交互数据和侧面信息）。一种新兴的做法是在联合环境中实施推荐系统，这使得所有用户设备都能协同学习一个共享的全局推荐器，同时将所有训练数据保留在设备上。然而，联盟推荐系统中的隐私问题却很少被探讨。在本文中，我们首先设计了一种新型属性推理攻击器，对最先进的联合推荐模型进行了全面的隐私分析。实验结果表明，每个模型组件对属性推理攻击的脆弱性各不相同，这凸显了对新防御方法的需求。因此，我们提出了一种新颖的自适应隐私保护方法，在属性推理攻击面前保护用户的敏感数据，同时最大限度地提高推荐准确性。在两个真实数据集上的广泛实验结果验证了我们的模型在推荐效果和抵御推理攻击方面的卓越性能。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | 2023-03-19                                                                                                                                                                              | 2024-10-29 00:49:03                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:22                                                                                                                                  | 2024-10-29 00:49:03                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                  | citationNumber: 1; ccfInfo: CCF-A  TKDE             | /unread; Computer Science - Information Retrieval                                                                                                                                                                                                                                                                                                                                                                                      |
| QoS Prediction and Adversarial Attack Protection for Distributed Services Under DLaaS                                                                           | Liang, Wei; Li, Yuhui; Xu, Jianlong; Qin, Zheng; Zhang, Dafang; Li, Kuan-Ching                                                                                          | 2024             | journalArticle  | IEEE Transactions on Computers                                                                                                                                                                      | https://ieeexplore.ieee.org/document/10195840/                                                                                                                                                                                                                                |                                     | 0018-9340, 1557-9956, 2326-3814 | 10.1109/TC.2021.3077738               | Deep-Learning-as-a-service (DLaaS) has received increasing attention due to its novelty as a diagram for deploying deep learning techniques. However, DLaaS faces performance and security issues that urgently need to be addressed. Given the limited computation resources and concern of beneﬁts, Quality-of-Service (QoS) metrics should be revised to optimize the performance and reliability of distributed DLaaS systems. New users and services dynamically and continuously join and leave such a system, resulting in cold start issues, and additionally, the increasing demand for robust network connections requires the model to evaluate the uncertainty. To address such performance problems, we propose in this article a deep learning-based model called embedding enhanced probability neural network, in which information is extracted from inside the graph structure and then estimated the mean and variance values for the prediction distribution. The adversarial attack is a severe threat to model security under DLaaS. Due to such, the service recommender system’s vulnerability is tackled, and adversarial training with uncertainty-aware loss to protect the model in noisy and adversarial environments is investigated and proposed. Extensive experiments on a large-scale real-world QoS dataset are conducted, and comprehensive analysis veriﬁes the robustness and effectiveness of the proposed model. 深度学习即服务（DLaaS）作为一种部署深度学习技术的新颖图表，受到越来越多的关注。然而，DLaaS 面临着急需解决的性能和安全问题。考虑到有限的计算资源和对收益的担忧，应修订服务质量（QoS）指标，以优化分布式 DLaaS 系统的性能和可靠性。新用户和新服务会动态、持续地加入和离开此类系统，从而导致冷启动问题，此外，对稳健网络连接的需求不断增加，这就要求模型对不确定性进行评估。为解决此类性能问题，我们在本文中提出了一种基于深度学习的模型，称为嵌入增强概率神经网络，该模型从图结构内部提取信息，然后估计预测分布的均值和方差值。对抗性攻击是对 DLaaS 模型安全性的严重威胁。因此，本文针对服务推荐系统的脆弱性，研究并提出了具有不确定性感知损失的对抗训练，以保护模型在噪声和对抗环境中的安全。在大规模真实 QoS 数据集上进行了广泛的实验，并通过综合分析验证了所提模型的鲁棒性和有效性。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | 2024-03                                                                                                                                                                                 | 2024-10-29 01:19:21                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:32                                                                                                                                  | 2024-10-29 01:19:21                                                                                                                                                                                                                                                                                                                                 | 669-682                                                                                                                                                                                          | citationNumber: 5; ccfInfo: CCF-A TC                | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Toward Robust Recommendation via Real-time Vicinal Defense                                                                                                      | Xu, Yichang; Wu, Chenwang; Lian, Defu                                                                                                                                   | 2023             | preprint        |                                                                                                                                                                                                     | http://arxiv.org/abs/2309.17278                                                                                                                                                                                                                                               |                                     |                                 |                                       | Recommender systems have been shown to be vulnerable to poisoning attacks, where malicious data is injected into the dataset to cause the recommender system to provide biased recommendations. To defend against such attacks, various robust learning methods have been proposed. However, most methods are model-specific or attack-specific, making them lack generality, while other methods, such as adversarial training, are oriented towards evasion attacks and thus have a weak defense strength in poisoning attacks. In this paper, we propose a general method, Real-time Vicinal Defense (RVD), which leverages neighboring training data to fine-tune the model before making a recommendation for each user. RVD works in the inference phase to ensure the robustness of the specific sample in real-time, so there is no need to change the model structure and training process, making it more practical. Extensive experimental results demonstrate that RVD effectively mitigates targeted poisoning attacks across various models without sacrificing accuracy. Moreover, the defensive effect can be further amplified when our method is combined with other strategies. 事实证明，推荐系统很容易受到 “中毒 ”攻击，即向数据集注入恶意数据，使推荐系统提供有偏见的推荐。为了抵御这种攻击，人们提出了各种稳健的学习方法。然而，大多数方法都是针对特定模型或特定攻击的，因此缺乏通用性，而其他方法（如对抗训练）则是面向规避攻击的，因此在中毒攻击中的防御能力较弱。在本文中，我们提出了一种通用方法--实时虚拟防御（RVD），它利用邻近的训练数据对模型进行微调，然后再为每个用户做出推荐。RVD 在推理阶段工作，实时确保特定样本的鲁棒性，因此无需改变模型结构和训练过程，更加实用。广泛的实验结果表明，RVD 能有效缓解各种模型的定向中毒攻击，而不会牺牲准确性。此外，当我们的方法与其他策略相结合时，防御效果还能进一步放大。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | 2023-09-29                                                                                                                                                                              | 2024-10-29 01:16:14                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:18                                                                                                                                  | 2024-10-29 01:16:14                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                  | citationNumber: 0; ccfInfo: CCF-None CORR           | /unread; Computer Science - Cryptography and Security; Computer Science - Information Retrieval; Computer Science - Machine Learning                                                                                                                                                                                                                                                                                                   |
| Federated Unlearning for On-Device Recommendation                                                                                                               | Yuan, Wei; Yin, Hongzhi; Wu, Fangzhao; Zhang, Shijie; He, Tieke; Wang, Hao                                                                                              | 2023             | conferencePaper | Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining                                                                                                             | https://doi.org/10.1145/3539597.3570463                                                                                                                                                                                                                                       | 978-1-4503-9407-9                   |                                 | 10.1145/3539597.3570463               | "The increasing data privacy concerns in recommendation systems have made federated recommendations attract more and more attention. Existing federated recommendation systems mainly focus on how to effectively and securely learn personal interests and preferences from their on-device interaction data. Still                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | none of them considers how to efficiently erase a user's contribution to the federated training process. We argue that such a dual setting is necessary. First                          | from the privacy protection perspective                                                                                                                                                                                                                                                                                                                   | ""the right to be forgotten (RTBF)"" requires that users have the right to withdraw their data contributions. Without the reversible ability         | federated recommendation systems risk breaking data protection regulations. On the other hand                                                                                                                                                                                                                                                       | enabling a federated recommender to forget specific users can improve its robustness and resistance to malicious clients' attacks.To support user unlearning in federated recommendation systems | New York, NY, USA                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| A genre trust model for defending shilling attacks in recommender systems                                                                                       | Yang, Li; Niu, Xinxin                                                                                                                                                   | 2023             | journalArticle  | Complex & Intelligent Systems                                                                                                                                                                       | https://doi.org/10.1007/s40747-021-00357-2                                                                                                                                                                                                                                    |                                     | 2198-6053                       | 10.1007/s40747-021-00357-2            | Shilling attacks have been a significant vulnerability of collaborative filtering (CF) recommender systems, and trust in CF recommender algorithms has been proven to be helpful for improving the accuracy of system recommendations. As a few studies have been devoted to trust in this area, we explore the benefits of using trust to resist shilling attacks. Rather than simply using user-generated trust values, we propose the genre trust degree, which differ in terms of the genres of items and take both trust value and user credibility into consideration. This paper introduces different types of shilling attack methods in an attempt to study the impact of users’ trust values and behavior features on defending against shilling attacks. Meanwhile, it improves the approach used to calculate user similarities to form a recommendation model based on genre trust degrees. The performance of the genre trust-based recommender system is evaluated on the Ciao dataset. Experimental results demonstrated the superior and comparable genre trust degrees recommended for defending against different types of shilling attacks. 托攻击一直是协同过滤（CF）推荐系统的一个重要漏洞，而事实证明，对CF推荐算法的信任有助于提高系统推荐的准确性。由于该领域专门针对信任的研究很少，我们探讨了利用信任抵御托攻击的好处。我们没有简单地使用用户生成的信任值，而是提出了流派信任度，即根据项目的流派不同，同时考虑信任值和用户可信度。本文介绍了不同类型的托攻击方法，试图研究用户的信任值和行为特征对防御托攻击的影响。同时，本文改进了计算用户相似度的方法，形成了基于流派信任度的推荐模型。基于流派信任度的推荐系统在 Ciao 数据集上进行了性能评估。实验结果表明，所推荐的流派信任度在防御不同类型的托攻击方面具有优越性和可比性。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | 2023-06-01                                                                                                                                                                              | 2024-10-29 00:45:47                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:19:54                                                                                                                                  | 2024-10-29 00:45:47                                                                                                                                                                                                                                                                                                                                 | 2929-2942                                                                                                                                                                                        | citationNumber: 7; ccfInfo: Not Found               | /unread; Collaborative filtering; Shilling attack; Recommender system; Genre trust; Trust value                                                                                                                                                                                                                                                                                                                                        |
| Alleviating the data sparsity problem of recommender systems by clustering nodes in bipartite networks                                                          | Zhang, Fuguo; Qi, Shumei; Liu, Qihua; Mao, Mingsong; Zeng, An                                                                                                           | 2020             | journalArticle  | Expert Systems with Applications                                                                                                                                                                    | https://linkinghub.elsevier.com/retrieve/pii/S0957417420301718                                                                                                                                                                                                                |                                     | 09574174                        | 10.1016/j.eswa.2020.113346            | Recommender systems help users to ﬁnd information that ﬁts their preferences in an overloaded search space. Collaborative ﬁltering systems suffer from increasingly severe data sparsity problem because more and more products are sold in commercial websites, which largely constrains the performance of recommendation algorithms. User clustering has already been applied to recommendation on sparse data in the literature, but in a completely different way. In most existing works, user clustering is directly used to identify the similar users of the target user to whom we want to make recommendation. More speciﬁcally, the users who are clustered in the same group of the target user are considered as similar users. However, in this paper we use user clustering to reconstruct the user-item bipartite network such that the network density is signiﬁcantly improved. The recommendation made on this dense network thus can achieve much higher accuracy than on the original sparse network. The experimental results on three benchmark data sets demonstrate that, when facing the problem of data sparsity, our proposed recommendation algorithm based on node clustering achieves a signiﬁcant improvement in accuracy and coverage of recommendation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | 2020-07                                                                                                                                                                                 | 2024-10-29 00:55:25                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:19:57                                                                                                                                  | 2024-10-29 00:55:25                                                                                                                                                                                                                                                                                                                                 | 113346                                                                                                                                                                                           | ccfInfo: CCF-C ESWA; citationNumber: 40             | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Inference of Suspicious Co-Visitation and Co-Rating Behaviors and Abnormality Forensics for Recommender Systems                                                 | Yang, Zhihai; Sun, Qindong; Zhang, Yaling; Zhu, Lei; Ji, Wenjiang                                                                                                       | 2020             | journalArticle  | IEEE Transactions on Information Forensics and Security                                                                                                                                             | https://ieeexplore.ieee.org/document/9020128/?arnumber=9020128                                                                                                                                                                                                                |                                     | 1556-6021                       | 10.1109/TIFS.2020.2977023             | The pervasiveness of personalized collaborative recommender systems has shown the powerful capability in a wide range of E-commerce services such as Amazon, TripAdvisor, Yelp, etc. However, fundamental vulnerabilities of collaborative recommender systems leave space for malicious users to affect the recommendation results as the attackers desire. A vast majority of existing detection methods assume certain properties of malicious attacks are given in advance. In reality, improving the detection performance is usually constrained due to the challenging issues: (a) various types of malicious attacks coexist, (b) limited representations of malicious attack behaviors, and (c) practical evidences for exploring and spotting anomalies on real-world data are scarce. In this paper, we investigate a unified detection framework in an eye for an eye manner without being bothered by the details of the attacks. Firstly, co-visitation and co-rating graphs are constructed using association rules. Then, attribute representations of nodes are empirically developed from the perspectives of linkage pattern, structure-based property and inherent association of nodes. Finally, both attribute information and connective coherence of graph are combined in order to infer suspicious nodes. Extensive experiments on both synthetic and real-world data demonstrate the effectiveness of the proposed detection approach compared with competing benchmarks. Additionally, abnormality forensics metrics including distribution of rating intention, time aggregation of suspicious ratings, degree distributions before as well as after removing suspicious nodes and time series analysis of historical ratings, are provided so as to discover interesting findings such as suspicious nodes (items or ratings) on real-world data.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | 2020                                                                                                                                                                                    | 2024-10-28 09:54:21                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:25                                                                                                                                  | 2024-10-28 09:54:21                                                                                                                                                                                                                                                                                                                                 | 2766-2781                                                                                                                                                                                        | citationNumber: 6; ccfInfo: CCF-A TIFS              | /unread; Collaboration; Recommender systems; recommender system; Anomaly detection; Measurement; Abnormality forensics; attack detection; Couplings; Forensics; malicious attack; Time series analysis                                                                                                                                                                                                                                 |
| On the Detection of Shilling Attacks in Federated Collaborative Filtering                                                                                       | Jiang, Yangfan; Zhou, Yipeng; Wu, Di; Li, Chao; Wang, Yan                                                                                                               | 2020             | conferencePaper | 2020 International Symposium on Reliable Distributed Systems (SRDS)                                                                                                                                 | https://ieeexplore.ieee.org/document/9252070/?arnumber=9252070                                                                                                                                                                                                                |                                     |                                 | 10.1109/SRDS51746.2020.00026          | Federated collaborative filtering (Fed-CF) is a variant of federated learning (FL) models, which can protect user privacy in recommender systems. In Fed-CF, the recommendation model is collectively trained across multiple decentralized clients by exchanging gradients only. However, the decentralized nature of Fed-CF makes it vulnerable to shilling attacks, which can be realized by inserting fake ratings of target items to distort recommendation results. Unfortunately, previous detection algorithms cannot work well in the FL framework, as all original data samples are not disclosed at all. In this paper, we are the first to systematically study the problem of shilling attacks in the context of federated learning, and propose an effective detection method called Federated Shilling Attack Detector (FSAD) to detect shilling attackers in Fed-CF. We first show the feasibility of shilling attacks in Fed-CF. Next, we dedicatedly design four novel features based on exchanged gradients among clients. By incorporating these gradient-based features, we train a semi-supervised Bayes classifier to identify shilling attackers effectively. Finally, we conduct extensive experiments based on real-world datasets to evaluate the performance of our proposed FSAD method. The experimental results show that FSAD can detect shilling attackers in Fed-CF with high accuracy, with the F1 value as high as 0.90 on the Netflix dataset, which approaches the performance of the optimal detector that utilizes complete private user information for detection.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | 2020-09                                                                                                                                                                                 | 2024-10-28 09:56:56                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:09                                                                                                                                  | 2024-10-28 09:56:56                                                                                                                                                                                                                                                                                                                                 | 185-194                                                                                                                                                                                          | citationNumber: 10; ccfInfo: CCF-B SRDS             | /unread; Collaborative filtering; Recommender systems; Privacy; Attack Detection; Collaborative work; Detectors; Fake Rating; Federated Collaborative Filtering; Information filters; Reliability; Shilling Attack                                                                                                                                                                                                                     |
| 基于混合特征值的托攻击检测算法                                                                                                                                                 | 雷梦宁; 丁爱玲; 王新美; 韩佳倩; 曹苗                                                                                                                                                  | 2021             | journalArticle  | 计算机技术与发展                                                                                                                                                                                            | https://kns.cnki.net/kcms2/article/abstract?v=64ENavj7QCA3bf5rdCEz04Fct25czDdEhhrkOGVJ-5hguHv9nzVSj7xTQ6nSevSYy-mr2kRMVbp_W-eqO7XnDFhSr5qjnt5iManFsLJ1c6Y85VceNd0ALi8gra4xEUPJCSityynITm5SdlSwsVgLS525I0uXJMLeSvgUhpKRBTUT7WustI8Cdtw9x_EDA0xc&uniplatform=NZKPT&language=CHS |                                     | 1673-629X                       |                                       | 传统的托攻击检测方法多采用基于评分值差异的算法,其在小规模情况下易造成误判率过高的问题。通过分析真实用户和攻击用户评分项目选择方式的差异,文中提出了一种基于混合特征值的托攻击检测算法。该算法在Degsim、MeanVar、WDA特征检测指标组成的特征模型基础上,加入了流行项目卡方估计值(Chi-square of popular item, CHIP)、新颖项目卡方估计值(Chi-square of novel item, CHIN)两个特征检测指标,构成一种新的特征模型。该特征模型在传统方法的基础上,提出对项目与流行项目、项目与新颖项目之间的关联程度的考量,依据特征属性选择K-means聚类与阈值判断相结合的分类方法,可有效区分攻击用户和正常用户。实验对比表明,该算法在小规模情况下可有效解决误判率高的问题,具有更好的检测准确度。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | 2021                                                                                                                                                                                    | 2024-10-25 05:19:32                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:19:50                                                                                                                                  | 2024-10-25 05:19:32                                                                                                                                                                                                                                                                                                                                 | 87-92                                                                                                                                                                                            | citationNumber: 0; ccfInfo: Not Found               | /unread; 推荐系统; 托攻击; 混合特征; 聚类算法; 卡方估计值                                                                                                                                                                                                                                                                                                                                                                                                  |
| 一种基于卷积自动编码器的推荐系统攻击检测方法                                                                                                                                          | 费艳; 缪骞云; 刘学军                                                                                                                                                            | 2021             | journalArticle  | 小型微型计算机系统                                                                                                                                                                                           | https://kns.cnki.net/kcms2/article/abstract?v=64ENavj7QCAZayDIQsc2xymqTiPDUBOdYLwHrnKvh7W4fsk55ngBNTtNMozVYNhLJPqEDIaHpzKgr-Eajm0lef4i5eoyA1UnIMMcuBzBUpt1As7moU2PuGN07EwicaCKfnaviQzQD9x5TbfUAk9P-_dv-__4iPgiHzuekW6YngtO-nJJtAp-tucf4V9O3ZKx&uniplatform=NZKPT&language=CHS |                                     | 1000-1220                       |                                       | 协同过滤推荐已经成为解决互联网上信息过载的有效方法之一,但是,协同过滤推荐系统本身所具有的高度开放性,容易受到恶意用户的攻击,导致产生欺诈性的推荐结果,因此,有效的攻击检测对于提高推荐系统的可用性具有重要的意义.特征工程的质量很大程度上决定了攻击检测性能,而目前大多数攻击检测方法都是基于人工方式来提取用户特征,面对不同的攻击模型,构建通用的、合适的特征指标往往是非常困难的,因此,本文提出了一种基于卷积自动编码器的推荐系统攻击检测方法,将自动特征提取和人工设计特征相结合来构造攻击检测特征,将自动编码器与卷积神经网络相结合,以卷积神经网络的卷积操作完成自动编码器的编码和解码功能,实现特征自动提取,采用深度学习方法进行攻击检测.实验验证了本文提出方法的有效性.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | 2021                                                                                                                                                                                    | 2024-10-25 05:18:06                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:20                                                                                                                                  | 2024-10-25 05:18:06                                                                                                                                                                                                                                                                                                                                 | 1088-1092                                                                                                                                                                                        | citationNumber: 1; ccfInfo: Not Found               | /unread; 推荐系统; 攻击检测; 卷积自动编码器; 深度学习                                                                                                                                                                                                                                                                                                                                                                                                     |
| 基于项目评分行为序列的群组攻击检测算法                                                                                                                                             | 胡玉琦; 李雪; 曲越奇                                                                                                                                                            | 2021             | journalArticle  | 燕山大学学报                                                                                                                                                                                              | https://kns.cnki.net/kcms2/article/abstract?v=64ENavj7QCCevu5zTaYqd7sy1sqYLuPGIp397EZOBxMp-OrcS4rTUxm7GzX-cOHJBHExn7fho8uBBnWWkpJK2_mO9B2Gt3sQAS8sDK3LvnCkLiGbeyAJV-Id7KZEVwfyAbOwhtot-AZ_u2yL1wlRbFfMM5gNSYMII09708KMEHYqdWLO1j0-R7Lm1XKiKlxN&uniplatform=NZKPT&language=CHS |                                     | 1007-791X                       |                                       | 在推荐系统中,攻击用户的行为正逐渐从个体攻击转变为群组攻击。相对于个体攻击用户而言,攻击群组的迷惑性更强,威胁性更大。针对这种情况,本文提出了一种基于评分行为序列的群组攻击检测算法。该算法通过对每个项目提取评分行为序列,划分评分行为区间作为候选群组,然后根据攻击群组评分极端的特性、利用层次聚类筛选候选群组,并计算可疑度,最后再使用K-means聚类判定攻击群组,并在Amazon数据集和Netflix数据集上验证了该算法的有效性。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | 2021                                                                                                                                                                                    | 2024-10-25 05:17:41                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:19:51                                                                                                                                  | 2024-10-25 05:17:41                                                                                                                                                                                                                                                                                                                                 | 87-94                                                                                                                                                                                            | citationNumber: 0; ccfInfo: Not Found               | /unread; 推荐系统; 层次聚类; 评分行为序列; 群组攻击; K-means聚类                                                                                                                                                                                                                                                                                                                                                                                           |
| On Detecting Data Pollution Attacks On Recommender Systems Using Sequential GANs                                                                                | Shahrasbi, Behzad; Mani, Venugopal; Arrabothu, Apoorv Reddy; Sharma, Deepthi; Achan, Kannan; Kumar, Sushant                                                             | 2020             | preprint        |                                                                                                                                                                                                     | http://arxiv.org/abs/2012.02509                                                                                                                                                                                                                                               |                                     |                                 |                                       | Recommender systems are an essential part of any e-commerce platform. Recommendations are typically generated by aggregating large amounts of user data. A malicious actor may be motivated to sway the output of such recommender systems by injecting malicious datapoints to leverage the system for financial gain. In this work, we propose a semi-supervised attack detection algorithm to identify the malicious datapoints. We do this by leveraging a portion of the dataset that has a lower chance of being polluted to learn the distribution of genuine datapoints. Our proposed approach modifies the Generative Adversarial Network architecture to take into account the contextual information from user activity. This allows the model to distinguish legitimate datapoints from the injected ones.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 2020-12-04                                                                                                                                                                              | 2024-10-29 01:01:27                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:27                                                                                                                                  | 2024-10-29 01:01:27                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                  | citationNumber: 0; ccfInfo: CCF-None CORR           | /unread; Computer Science - Machine Learning                                                                                                                                                                                                                                                                                                                                                                                           |
| hPSD: A Hybrid PU-Learning-Based Spammer Detection Model for Product Reviews                                                                                    | Wu, Zhiang; Cao, Jie; Wang, Yaqiong; Wang, Youquan; Zhang, Lu; Wu, Junjie                                                                                               | 2020             | journalArticle  | IEEE Transactions on Cybernetics                                                                                                                                                                    | https://ieeexplore.ieee.org/document/8520779/?arnumber=8520779                                                                                                                                                                                                                |                                     | 2168-2275                       | 10.1109/TCYB.2018.2877161             | Spammers, who manipulate online reviews to promote or suppress products, are flooding in online commerce. To combat this trend, there has been a great deal of research focused on detecting review spammers, most of which design diversified features and thus develop various classifiers. The widespread growth of crowdsourcing platforms has created largescale deceptive review writers who behave more like normal users, that the way they can more easily evade detection by the classifiers that are purely based on fixed characteristics. In this paper, we propose a hybrid semisupervised learning model titled hybrid PU-learning-based spammer detection (hPSD) for spammer detection to leverage both the users' characteristics and the user-product relations. Specifically, the hPSD model can iteratively detect multitype spammers by injecting different positive samples, and allows the construction of classifiers in a semisupervised hybrid learning framework. Comprehensive experiments on movie dataset with shilling injection confirm the superior performance of hPSD over existing baseline methods. The hPSD is then utilized to detect the hidden spammers from real-life Amazon data. A set of spammers and their underlying employers (e.g., book publishers) are successfully discovered and validated. These demonstrate that hPSD meets the real-world application scenarios and can thus effectively detect the potentially deceptive review writers.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | 2020-04                                                                                                                                                                                 | 2024-10-17 12:22:29                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:24                                                                                                                                  | 2024-10-25 06:03:38                                                                                                                                                                                                                                                                                                                                 | 1595-1606                                                                                                                                                                                        | ccfInfo: CCF-B; citationNumber: 52                  | /unread; Data models; Motion pictures; Feature extraction; Buildings; Economics; Positive and unlabeled dataset learning (PU-learning); semisupervised learning; Semisupervised learning; spammer detection; Unsolicited electronic mail; user–product relations                                                                                                                                                                       |
| AttriGuard: A Practical Defense Against Attribute Inference Attacks via Adversarial Machine Learning                                                            | Jia, Jinyuan; Gong, Neil Zhenqiang                                                                                                                                      | 2020             | preprint        |                                                                                                                                                                                                     | http://arxiv.org/abs/1805.04810                                                                                                                                                                                                                                               |                                     |                                 |                                       | Users in various web and mobile applications are vulnerable to attribute inference attacks, in which an attacker leverages a machine learning classifier to infer a target user's private attributes (e.g., location, sexual orientation, political view) from its public data (e.g., rating scores, page likes). Existing defenses leverage game theory or heuristics based on correlations between the public data and attributes. These defenses are not practical. Specifically, game-theoretic defenses require solving intractable optimization problems, while correlation-based defenses incur large utility loss of users' public data. In this paper, we present AttriGuard, a practical defense against attribute inference attacks. AttriGuard is computationally tractable and has small utility loss. Our AttriGuard works in two phases. Suppose we aim to protect a user's private attribute. In Phase I, for each value of the attribute, we find a minimum noise such that if we add the noise to the user's public data, then the attacker's classifier is very likely to infer the attribute value for the user. We find the minimum noise via adapting existing evasion attacks in adversarial machine learning. In Phase II, we sample one attribute value according to a certain probability distribution and add the corresponding noise found in Phase I to the user's public data. We formulate finding the probability distribution as solving a constrained convex optimization problem. We extensively evaluate AttriGuard and compare it with existing methods using a real-world dataset. Our results show that AttriGuard substantially outperforms existing methods. Our work is the first one that shows evasion attacks can be used as defensive techniques for privacy protection.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 2020-04-13                                                                                                                                                                              | 2024-10-25 06:02:24                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:22                                                                                                                                  | 2024-10-25 06:02:24                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                  | ccfInfo: CCF-A USENIX Security; citationNumber: 158 | /unread; Computer Science - Cryptography and Security; Statistics - Machine Learning                                                                                                                                                                                                                                                                                                                                                   |
| 一种利用半监督Fisher判别分析检测推荐攻击的方法                                                                                                                                      | 武锦霞; 周全强; 段亮亮                                                                                                                                                           | 2020             | journalArticle  | 小型微型计算机系统                                                                                                                                                                                           | https://kns.cnki.net/kcms2/article/abstract?v=64ENavj7QCAG-zpJTJLs27hBLgPuzvsFJ3-ZbvMA8ER-soDg3OdNfAATie8PGYwbFcyMxz-YY_2bvMy8kkt2VS7I-MP2bLqQ2hC81p22_5fKNiQ9obFNx3ZeqQ5ZCEa84dFFNNofkU6yYnPSH_7u3-HPb_3e8y6_mpP9fD5U6R1imsPMwjpKiQtsoaNVRF-5&uniplatform=NZKPT&language=CHS |                                     | 1000-1220                       |                                       | 协同推荐系统容易受到推荐攻击,为了检测该攻击,很多无监督、有监督及半监督检测方法被提出,其中,半监督检测方法的优势在于可以利用无标签用户概貌提升检测性能.然而,已有半监督检测方法的准确率较低,针对该问题,本文提出一种基于半监督Fisher判别分析(Semi-supervised Fisher Discriminant Analysis,SFDA)的推荐攻击检测方法 RAD-SFDA来提升半监督检测方法的准确率.首先,利用Fisher判别分析(Fisher Discriminant Analysis,FDA)技术结合有标签用户概貌确定投影向量,在投影后的空间中最大化真实概貌和攻击概貌的离散度的同时最小化同类用户概貌间的离散度;然后,利用主元分析(Principal Components Analysis,PCA)技术从有标签和无标签用户概貌建立的数据集中提取全局结构;最后,综合上述由有标签用户概貌确定的判别结构和由所有用户概貌确定的全局结构确定最佳投影向量,在最终的投影空间中训练贝叶斯分类器检测推荐攻击.在MovieLens数据集上的实验结果表明,本文方法在保持较高召回率的前提下有效提升了准确率.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | 2020                                                                                                                                                                                    | 2024-10-25 05:09:52                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:21                                                                                                                                  | 2024-10-25 05:09:52                                                                                                                                                                                                                                                                                                                                 | 2649-2656                                                                                                                                                                                        | citationNumber: 0; ccfInfo: Not Found               | /unread; 半监督Fisher判别分析; 贝叶斯分类器; 推荐攻击检测; 协同推荐系统                                                                                                                                                                                                                                                                                                                                                                                         |
| 基于信任的托攻击用户检测算法                                                                                                                                                  | 张鑫; 黄刚                                                                                                                                                                  | 2020             | journalArticle  | 计算机应用与软件                                                                                                                                                                                            | https://kns.cnki.net/kcms2/article/abstract?v=64ENavj7QCChXl9grokq57jDVeKXhahBx5RQHdfW0vRQHblYVNcTMlugLY769uRwzxNU-bchHdGsPwlBjLVATvLW-qB4RVOYqJBW8JH80XaV0fXGxdQn8F-lxF-7_C5Ntehx3AMzbI6XjV1sJytEC3hJYO9vvgHUGUacHpolulcgh0kETaLEHxahYKdWbdUC&uniplatform=NZKPT&language=CHS |                                     | 1000-386X                       |                                       | 随着电子商务的迅速发展,协同过滤技术在推荐领域中,得到了广泛的运用。托攻击问题和数据稀疏性问题,导致推荐结果不理想。研究证明,用户间信任关系可以极大缓解数据稀疏问题,使得推荐更为准确。但是,包含信任关系的推荐算法,大多未能考虑到托攻击对于推荐的影响,使得系统的鲁棒性下降。通过研究包含信任信息的推荐情景中托攻击用户的统计量表现特征,提出一种在信任网络下,检测托攻击用户的TSAD算法。实验证明,该算法能够准确地识别托攻击用户,增强系统的鲁棒性。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 2020                                                                                                                                                                                    | 2024-10-25 05:06:45                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:19:51                                                                                                                                  | 2024-10-25 05:06:45                                                                                                                                                                                                                                                                                                                                 | 286-291                                                                                                                                                                                          | citationNumber: 0; ccfInfo: Not Found               | /unread; 托攻击; 统计量; 推荐; 协同过滤; 信任                                                                                                                                                                                                                                                                                                                                                                                                        |
| 基于深度自动编码器的托攻击集成检测方法                                                                                                                                             | 郝耀军; 张付志                                                                                                                                                                | 2019             | journalArticle  | 计算机工程与应用                                                                                                                                                                                            | https://kns.cnki.net/kcms2/article/abstract?v=64ENavj7QCAm9f_RpmdF6H8SXsvUyJoRTLCHt4uIRzABl30iqhwTPCOQcjgBiwC1_wamPMK-uNUvokY-P6fFxiOE2GCd-86P3iTtkwjfhqlsSW9IH8CcNW2-UsXRwXuc0S-tGaZXiJYrStzrswXEcA_WOe8IRzBRps7kKvMhI_AgEcqLbqD06tJUPT8RIdJh&uniplatform=NZKPT&language=CHS |                                     | 1002-8331                       |                                       | 在采用协同过滤技术的推荐系统中,恶意用户通过注入大量虚假概貌使系统的推荐结果产生偏离,达到其攻击目的。为了检测托攻击,根据用户的评分值或基于攻击时间的集中性假设,从不同视角提取攻击概貌的特征。但是,这些基于人工特征的检测方法严重依赖于特征工程的质量,而且人工提取的检测特征多限于特定类型的攻击,提取特征也需要较高的知识成本。针对这些问题,从用户评分项目的时间偏好信息入手,提出一种利用深度稀疏自动编码器自动提取检测特征的托攻击集成检测方法。利用小波变换将项目在不同时间间隔内的流行度设定为多个等级,对用户的评分数据预处理得到用户-项目时间流行度等级矩阵。然后,采用深度稀疏自动编码器对用户-项目时间流行度等级矩阵自动进行特征提取,得到用户评分模式的低层特征表达,消除了传统的人工特征工程。以SVM作为基分类器,在深度稀疏自动编码器的每层提取特征并进行攻击检测,生成最终的集成检测结果。在Netflix数据集上的实验表明,提出的检测方法对均值攻击、AoP攻击、偏移攻击、高级项目攻击、高级用户攻击具有较好的检测效果。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | 2019                                                                                                                                                                                    | 2024-10-25 05:06:21                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:19:50                                                                                                                                  | 2024-10-25 05:06:21                                                                                                                                                                                                                                                                                                                                 | 9-22+88                                                                                                                                                                                          | citationNumber: 0; ccfInfo: Not Found               | /unread; 托攻击; 协同过滤; 深度稀疏自动编码器; 托攻击检测; 项目时间流行度等级                                                                                                                                                                                                                                                                                                                                                                                        |
| 基于特征指标推荐系统托攻击半监督检测                                                                                                                                              | 卫星君; 顾清华                                                                                                                                                                | 2018             | journalArticle  | 计算机应用研究                                                                                                                                                                                             | https://kns.cnki.net/kcms2/article/abstract?v=UjEBX92ALNFjxvbfF0snhrgSC8gcpQxAncQC-DGYMPfz77TKfd6tchWZk_CP52oH1dZcDaHEyU3sIAE5K4sRlDy9A7tJst7micsqkr2fhmh7zF_7OSPk4vpkeuQKq_3BxGrwRqG5DfN6aFI-ITZMHJWYRxtBw6XCFGj-pstpor9Xcul-3XUJ6Jbxy4FPOR3L&uniplatform=NZKPT&language=CHS |                                     | 1001-3695                       |                                       | 针对托攻击提出一种半监督托检测模型,对标记用户分类计算簇中心,给出中心用户相似度特征属性。对不同攻击选择合适的特征指标,把输入用户划分到不同的簇集中,通过簇集中输入用户全部评分项为最大值的均值与标记用户对该项均值差,确定攻击项。依据特征指标对不同簇集进行两次分类,进而确定攻击对象。实验证明,该检测算法对不同的托攻击有较高的检测准确率。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | 2018                                                                                                                                                                                    | 2024-10-25 05:05:13                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:19:51                                                                                                                                  | 2024-10-29 01:21:57                                                                                                                                                                                                                                                                                                                                 | 2185-2188                                                                                                                                                                                        | citationNumber: 1; ccfInfo: Not Found               | /unread; 半监督; 聚类; 特征指标; 推荐系统; 托攻击                                                                                                                                                                                                                                                                                                                                                                                                      |
| 面向服务质量感知云API推荐系统的数据投毒攻击检测方法                                                                                                                                     | 陈真; 乞文超; 鲍泰宇; 申利民                                                                                                                                                       | 2023             | journalArticle  | 通信学报                                                                                                                                                                                                | https://kns.cnki.net/kcms2/article/abstract?v=64ENavj7QCAEID81ndDtawXR0BI562SBwEdlkjAlDRCRd0YOr5HMWuPMCf30PnmpV4T4YnOXG24xQNqjQIb2UPz31Fiw1NaHzslESbuHoFMtMLDiA3y2mCCZrZZsrLATVi9T04WQ23RPAhK75WVmyZBKzYhbKPCUIk1WUL1F0TRMw1VuyaQcgOlSq2FTx1dj&uniplatform=NZKPT&language=CHS |                                     | 1000-436X                       |                                       | 针对现有研究通常假设云API推荐系统的服务质量数据是可靠的，忽略了开放网络环境中恶意用户对云API推荐系统的数据投毒攻击的问题，提出了一种基于多特征融合的数据投毒攻击检测方法。首先，依据设计的相似性度量函数构建用户连通网络图，并利用Node2vec捕获用户邻域特征；其次，采用稀疏自编码器挖掘用户服务质量深度特征，并构建基于服务质量数据加权平均偏差的用户解释特征。进一步，融合用户邻域特征、服务质量深度特征和解释特征建立基于支持向量机的虚假用户检测模型，并使用网格搜索和交替迭代优化策略学习模型参数，继而实现虚假用户检测。最后，通过多组实验验证了所提方法的有效性和优越性，实现了服务质量感知云API推荐系统在数据端的投毒攻击防御。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | 2023                                                                                                                                                                                    | 2024-10-25 05:19:06                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:19:51                                                                                                                                  | 2024-10-25 05:19:06                                                                                                                                                                                                                                                                                                                                 | 155-167                                                                                                                                                                                          | ccfInfo: Not Found; citationNumber: Not Found       | /unread; 推荐系统; 攻击检测; 服务质量; 数据投毒; 云API                                                                                                                                                                                                                                                                                                                                                                                                  |
| 基于评分离散度的托攻击检测算法                                                                                                                                                 | 贾俊杰; 段超强                                                                                                                                                                | 2022             | journalArticle  | 计算机工程与科学                                                                                                                                                                                            | https://kns.cnki.net/kcms2/article/abstract?v=64ENavj7QCCI6zXQ4qSOFHME0TIPQCMtwxjAkX69b-m1pRvw05CQRWIElthY2_Hg7gO6ukqTuY-4yVYc91pAxIlOvPdRoJ7GYxbNfCVS2SM7dPsmolk1i8gGSyGdWevLklf1oboVpuKlhB_l7zShWH0w8G1meZyS-b3OcRrUjibJVBV2QxsH44WkCsgrXFrV&uniplatform=NZKPT&language=CHS |                                     | 1007-130X                       |                                       | 检测托攻击的本质是对真实用户和虚假用户进行分类，现有的检测算法对于具有选择项的流行攻击、段攻击等攻击方式的检测鲁棒性较差。针对这一问题，通过分析真实用户和虚假用户的评分分布情况，结合ID3决策树提出基于用户评分离散度的托攻击检测Dispersion-C算法。算法通过用户评分极端评分比、去极端评分方差和用户评分标准差3个特征衡量用户评分离散度，并将其作为ID3决策树算法的分类特征，根据不同特征的信息增益选择特征作为分类属性，训练分类器。实验结果表明，Dispersion-C算法对各类托攻击均有良好的检测效果，具有较好的鲁棒性。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | 2022                                                                                                                                                                                    | 2024-10-25 05:18:39                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:19:50                                                                                                                                  | 2024-10-25 05:18:39                                                                                                                                                                                                                                                                                                                                 | 554-562                                                                                                                                                                                          | citationNumber: 0; ccfInfo: Not Found               | /unread; 推荐系统; 托攻击检测; 决策树; 评分离散度                                                                                                                                                                                                                                                                                                                                                                                                       |
| 基于CNN和犹豫模糊决策的欺诈攻击检测                                                                                                                                             | 蔡红云; 袁世林; 温玉; 任继超; 孟洁                                                                                                                                                   | 2022             | journalArticle  | 工程科学与技术                                                                                                                                                                                             | https://kns.cnki.net/kcms2/article/abstract?v=64ENavj7QCCrKxGjB1196qjQt8mD7ER9wdhRATCVdK0uGDCT3IE_iIlS4WZW7IUQU5CbPw-B-zhGfrezEseP5j0z9b9c6LPscdO8m5Rk6zVsbHVG-ZsQ--N3SI7mW1q6Nz8RxgqrDyTD6yNSZZIFgh24zWPzPyfy2bq44fRS_dKqFbXBb5KFZUrJFZ7AhSit&uniplatform=NZKPT&language=CHS |                                     | 2096-3246                       | 10.15961/j.jsuese.202100979           | 推荐系统能够有效缓解互联网的迅猛发展带来的信息过载问题，但欺诈攻击的存在制约了推荐系统的健康发展，因此如何准确、高效地检测欺诈攻击是推荐系统安全领域的重要问题。已有检测方法往往依赖专家知识人工提取检测特征或基于深度学习自动获取某一视角下的检测特征，在此基础上通过硬分类找出攻击用户，导致检测性能不佳。因此，本文同时考虑多视角下的特征自动提取，引入模糊决策，提出了一种基于CNN和犹豫模糊集的欺诈攻击检测方法（简称CNN-HFS）。首先，对每个用户分别从评分值、评分偏好和评分时间视角抽取3个行为矩阵，利用双三次插值法对3个矩阵进行缩放得到对应的密集评分矩阵、密集偏好矩阵和密集时间矩阵；然后，将每个用户任意视角下的缩放矩阵视为一个图像，在3个不同视角下分别训练CNN，并计算任意用户在每个视角下属于攻击用户类的隶属度；最后，引入模糊犹豫集对多视角下的检测结果进行综合决策，根据决策结果识别出攻击用户。为了验证CNN-HFS的有效性，选取SVM-TIA、CoDetector、CNN-SAD、SDAEs-PCA、CNN-R、CNN-P和CNN-T作为对比方法，在MovieLens 1M和Amazon数据集上对精确率、召回率和F1-measure值3个评价指标进行实验评估。实验结果表明，本文所提方法在3个评价指标上明显优于其他7种对比方法，可以获得更高的检测性能。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 2022                                                                                                                                                                                    | 2024-10-25 05:18:21                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:19:51                                                                                                                                  | 2024-10-25 05:18:21                                                                                                                                                                                                                                                                                                                                 | 80-90                                                                                                                                                                                            | citationNumber: 0; ccfInfo: Not Found               | /unread; 推荐系统; 攻击检测; 卷积神经网络; 犹豫模糊集                                                                                                                                                                                                                                                                                                                                                                                                     |
| A Distributed Deep Learning System for Web Attack Detection on Edge Devices                                                                                     | Tian, Zhihong; Luo, Chaochao; Qiu, Jing; Du, Xiaojiang; Guizani, Mohsen                                                                                                 | 2020             | journalArticle  | IEEE Transactions on Industrial Informatics                                                                                                                                                         | https://ieeexplore.ieee.org/document/8821336/?arnumber=8821336                                                                                                                                                                                                                |                                     | 1941-0050                       | 10.1109/TII.2019.2938778              | With the development of Internet of Things (IoT) and cloud technologies, numerous IoT devices and sensors transmit huge amounts of data to cloud data centers for further processing. While providing us considerable convenience, cloud-based computing and storage also bring us many security problems, such as the abuse of information collection and concentrated web servers in the cloud. Traditional intrusion detection systems and web application firewalls are becoming incompatible with the new network environment, and related systems with machine learning or deep learning are emerging. However, cloud-IoT systems increase attacks against web servers, since data centralization carries a more attractive reward. In this article, based on distributed deep learning, we propose a web attack detection system that takes advantage of analyzing URLs. The system is designed to detect web attacks and is deployed on edge devices. The cloud handles the above challenges in the paradigm of the Edge of Things. Multiple concurrent deep models are used to enhance the stability of the system and the convenience in updating. We implemented experiments on the system with two concurrent deep models and compared the system with existing systems by using several datasets. The experimental results with 99.410% in accuracy, 98.91% in true positive rate (TPR), and 99.55% in detection rate of normal requests (DRN) demonstrate the system is competitive in detecting web attacks.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | 2020-03                                                                                                                                                                                 | 2024-10-28 09:49:11                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:19:54                                                                                                                                  | 2024-10-28 09:49:11                                                                                                                                                                                                                                                                                                                                 | 1963-1971                                                                                                                                                                                        | ccfInfo: CCF-C TII; citationNumber: 220             | /unread; Feature extraction; Cloud computing; Computer hacking; Deep learning; Distributed deep dearning; distributed system; edge of things; Image edge detection; Internet of Things; Uniform resource locators; web attack detection                                                                                                                                                                                                |
| On the negative impact of social influence in recommender systems: A study of bribery in collaborative hybrid algorithms                                        | Ramos, Guilherme; Boratto, Ludovico; Caleiro, Carlos                                                                                                                    | 2020             | journalArticle  | Information Processing & Management                                                                                                                                                                 | https://linkinghub.elsevier.com/retrieve/pii/S0306457318308768                                                                                                                                                                                                                |                                     | 03064573                        | 10.1016/j.ipm.2019.102058             | Recommender systems are based on inherent forms of social influence. Indeed, suggestions are provided to the users based on the opinions of peers. Given the relevance that ratings have nowadays to push the sales of an item, sellers might decide to bribe users so that they rate or change the ratings given to items, thus increasing the sellers’ reputation. Hence, by exploiting the fact that influential users can lead an item to get recommended, bribing can become an effective way to negatively exploit social influence and introduce a bias in the recommendations. Given that bribing is forbidden but still employed by sellers, we propose a novel matrix completion algorithm that performs hybrid memory-based collaborative filtering using an approximation of Kolmogorov complexity. We also propose a framework to study the bribery effect and the bribery resistance of our approach. Our theoretical analysis, validated through experiments on real-world datasets, shows that our approach is an effective way to counter bribing while, with state-of-theart algorithms, sellers can bribe a large part of the users.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | 2020-03                                                                                                                                                                                 | 2024-10-28 09:57:28                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:09                                                                                                                                  | 2024-10-28 09:57:28                                                                                                                                                                                                                                                                                                                                 | 102058                                                                                                                                                                                           | ccfInfo: CCF-B IPM; citationNumber: 34              | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Adversarial Attacks and Detection on Reinforcement Learning-Based Interactive Recommender Systems                                                               | Cao, Yuanjiang; Chen, Xiaocong; Yao, Lina; Wang, Xianzhi; Zhang, Wei Emma                                                                                               | 2020             | conferencePaper | Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval                                                                                     | https://doi.org/10.1145/3397271.3401196                                                                                                                                                                                                                                       | 978-1-4503-8016-4                   |                                 | 10.1145/3397271.3401196               | Adversarial attacks pose significant challenges for detecting adversarial attacks at an early stage. We propose attack-agnostic detection on reinforcement learning-based interactive recommendation systems. We first craft adversarial examples to show their diverse distributions and then augment recommendation systems by detecting potential attacks with a deep learning-based classifier based on the crafted data. Finally, we study the attack strength and frequency of adversarial examples and evaluate our model on standard datasets with multiple crafting methods. Our extensive experiments show that most adversarial attacks are effective, and both attack strength and attack frequency impact the attack performance. The strategically-timed attack achieves comparative attack performance with only 1/3 to 1/2 attack frequency. Besides, our black-box detector trained with one crafting method has the generalization ability over several crafting methods.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | 2020-07-25                                                                                                                                                                              | 2024-10-28 09:50:59                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:19:57                                                                                                                                  | 2024-10-28                                                                                                                                                                                                                                                                                                                                          | 1669–1672                                                                                                                                                                                        | ccfInfo: CCF-A SIGIR; citationNumber: 57            | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| How Dataset Characteristics Affect the Robustness of Collaborative Recommendation Models                                                                        | Deldjoo, Yashar; Di Noia, Tommaso; Di Sciascio, Eugenio; Merra, Felice Antonio                                                                                          | 2020             | conferencePaper | Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval                                                                                     | https://dl.acm.org/doi/10.1145/3397271.3401046                                                                                                                                                                                                                                | 978-1-4503-8016-4                   |                                 | 10.1145/3397271.3401046               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | 2020-07-25                                                                                                                                                                              | 2024-10-24 04:47:34                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:24                                                                                                                                  | 2024-10-25 05:48:23                                                                                                                                                                                                                                                                                                                                 | 951-960                                                                                                                                                                                          | ccfInfo: CCF-A SIGIR; citationNumber: 55            | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Preventing the Popular Item Embedding Based Attack in Federated Recommendations                                                                                 | Zhang, Jun; Li, Huan; Rong, Dazhong; Zhao, Yan; Chen, Ke; Shou, Lidan                                                                                                   | 2024             | conferencePaper | 2024 IEEE 40th International Conference on Data Engineering (ICDE)                                                                                                                                  | https://ieeexplore.ieee.org/document/10597721/?arnumber=10597721                                                                                                                                                                                                              |                                     |                                 | 10.1109/ICDE60146.2024.00173          | Privacy concerns have led to the rise of federated recommender systems (FRS), which can create personalized models across distributed clients. However, FRS is vulnerable to poisoning attacks, where malicious users manipulate gradients to promote their target items intentionally. Existing attacks against FRS have limitations, as they depend on specific models and prior knowledge, restricting their real-world applicability. In our exploration of practical FRS vulnerabilities, we devise a model-agnostic and prior-knowledge-free attack, named PIECK (Popular Item Embedding based Attack). The core module of PIECK is popular item mining, which leverages embedding changes during FRS training to effectively identify the popular items. Built upon the core module, PIECK branches into two diverse solutions: The PIECKIPE solution employs an item popularity enhancement module, which aligns the embeddings of targeted items with the mined popular items to increase item exposure. The PIECKUEA further enhances the robustness of the attack by using a user embedding approximation module, which approximates private user embeddings using mined popular items. Upon identifying PIECK, we evaluate existing federated defense methods and find them ineffective against PIECK, as poisonous gradients inevitably overwhelm the cold target items. We then propose a novel defense method by introducing two regularization terms during user training, which constrain item popularity enhancement and user embedding approximation while preserving FRS performance. We evaluate PIECK and its defense across two base models, three real datasets, four top-tier attacks, and six general defense methods, affirming the efficacy of both PIECK and its defense. 出于对隐私的考虑，联合推荐系统（FRS）应运而生，它可以在分布式客户端上创建个性化模型。然而，联合推荐系统容易受到 “中毒 ”攻击，即恶意用户操纵梯度，有意推广其目标项目。现有的针对 FRS 的攻击具有局限性，因为它们依赖于特定的模型和先验知识，限制了它们在现实世界中的适用性。在对 FRS 实际漏洞的探索中，我们设计了一种不依赖模型和先验知识的攻击，命名为 PIECK（基于流行项目嵌入的攻击）。PIECK 的核心模块是流行项目挖掘，它利用 FRS 训练过程中的嵌入变化来有效识别流行项目。在核心模块的基础上，PIECK 衍生出两种不同的解决方案： PIECKIPE 解决方案采用了项目流行度增强模块，该模块将目标项目的嵌入与挖掘出的流行项目相一致，以增加项目曝光率。PIECKUEA 通过使用用户嵌入近似模块进一步增强了攻击的鲁棒性，该模块使用挖掘出的热门项目近似用户私人嵌入。识别出 PIECK 后，我们对现有的联合防御方法进行了评估，发现这些方法对 PIECK 无效，因为有毒梯度不可避免地会淹没冷目标项。随后，我们提出了一种新的防御方法，即在用户训练过程中引入两个正则化项，从而在保持 FRS 性能的同时，限制项目流行度提升和用户嵌入近似。我们通过两个基础模型、三个真实数据集、四种顶级攻击和六种一般防御方法对 PIECK 及其防御进行了评估，肯定了 PIECK 及其防御的有效性。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | 2024-05                                                                                                                                                                                 | 2024-11-10 02:53:02                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:32                                                                                                                                  | 2024-11-10 02:53:02                                                                                                                                                                                                                                                                                                                                 | 2179-2191                                                                                                                                                                                        | citationNumber: 0; ccfInfo: CCF-A ICDE              | /unread; Recommender systems; Robustness; Data engineering; Training; Federated Recommendation; Poisoning Attack; Privacy                                                                                                                                                                                                                                                                                                              |
| Recommendation attack detection based on improved Meta Pseudo Labels                                                                                            | Zhou, Quanqiang; Li, Kang; Duan, Liangliang                                                                                                                             | 2023             | journalArticle  | Knowledge-Based Systems                                                                                                                                                                             | https://linkinghub.elsevier.com/retrieve/pii/S0950705123006810                                                                                                                                                                                                                |                                     | 09507051                        | 10.1016/j.knosys.2023.110931          | Attackers attempt to bias the outputs of collaborative recommender systems by maliciously rating goods or services. To detect such attacks, many deep learning-based detection methods have been proposed and shown to be feasible. However, most methods require a large number of labeled user profiles for training to ensure good detection performance. To address this issue, in this paper, we propose a deep semisupervised detection approach based on the improved Meta Pseudo Labels, named DSSD-ImMPL. DSSD-ImMPL can achieve high detection performance given a small number of labeled training samples and a certain number of unlabeled training samples. We first improve the Meta Pseudo Labels method by generating a group of student networks by an experienced teacher network instead of only one student network in the original Meta Pseudo Labels method to improve the classification performance. Then, we use the group of student networks to detect the recommendation attack. The detection performance is verified with classical, mixed, GSA-GANs, and real attacks on three benchmark datasets by comparing DSSD-ImMPL with the state-of-the-art detection methods. 攻击者试图通过对商品或服务进行恶意评级，使协作推荐系统的输出结果产生偏差。为了检测这类攻击，人们提出了许多基于深度学习的检测方法，并证明这些方法是可行的。然而，大多数方法都需要大量有标签的用户配置文件进行训练，以确保良好的检测性能。为了解决这个问题，我们在本文中提出了一种基于改进元伪标签的深度半监督检测方法，命名为 DSSD-ImMPL。DSSD-ImMPL 可以在少量标注训练样本和一定数量未标注训练样本的条件下实现较高的检测性能。我们首先改进了元伪标签方法，由一个经验丰富的教师网络生成一组学生网络，而不是原始元伪标签方法中的只有一个学生网络，从而提高了分类性能。然后，我们利用这组学生网络来检测推荐攻击。通过比较 DSSD-ImMPL 和最先进的检测方法，我们在三个基准数据集上验证了经典、混合、GSA-GANs 和真实攻击的检测性能。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | 2023-11                                                                                                                                                                                 | 2024-10-29 00:47:20                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:33                                                                                                                                  | 2024-10-29 00:47:19                                                                                                                                                                                                                                                                                                                                 | 110931                                                                                                                                                                                           | citationNumber: 0; ccfInfo: CCF-C KBS               | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Detecting Group Shilling Attacks In Recommender Systems Based On User Multi-dimensional Features And Collusive Behaviour Analysis                               | Xu, Yishu; Zhang, Peng; Yu, Hongtao; Zhang, Fuzhi                                                                                                                       | 2024             | journalArticle  | The Computer Journal                                                                                                                                                                                | https://doi.org/10.1093/comjnl/bxad003                                                                                                                                                                                                                                        |                                     | 0010-4620                       | 10.1093/comjnl/bxad003                | Group shilling attacks are more threatening than individual shilling attacks due to the collusive behaviours among group members, which pose a great challenge to the credibility of recommender systems. Detection of group shilling attacks can reduce the risk caused by such attacks and ensure the credibility of recommendations. The existing methods for detecting group shilling attacks mainly extract features from the rating patterns of users at group level to measure the shilling behaviours of groups. However, they may become ineffective with the change of attack strategy, resulting in a decrease in detection performance. Aiming at this problem, a new solution based on user multi-dimensional features and collusive behaviour analysis is presented for detecting group shilling attacks. First, we employ the information entropy and latent semantic analysis to analyse the user behavioural patterns from dimensions of item, rating, time and interest, and propose a suite of indicators to measure the anomaly behaviours of users. Second, we propose a measure based on the multi-dimensional features of users to capture the collusion of group members from the perspective of their synchronized behaviours and abnormal behaviours, and treat the groups with high collusion as candidate groups. Finally, based on the multi-dimensional features of users, we construct the user behaviour similarity matrix using Gaussian radial basis function (Gaussian-RBF) and adopt the spectral clustering algorithm to spot group shilling attackers in the candidate groups. Experiments show that the detection performance (F1-measure) of the proposed method can achieve 0.965, 0.964, 0.991 and 0.868 on the Netflix, CiaoDVD, Epinions and Amazon datasets, respectively, which is better than that of state-of-the-art methods. 由于群体成员之间的串通行为，群体托儿攻击比个体托儿攻击更具威胁性，这对推荐系统的可信度提出了巨大挑战。检测群体托儿攻击可以降低此类攻击带来的风险，确保推荐的可信度。现有的检测群体托客攻击的方法主要是从群体层面的用户评分模式中提取特征来衡量群体的托客行为。然而，随着攻击策略的变化，这些方法可能会失效，导致检测性能下降。针对这一问题，我们提出了一种基于用户多维特征和串通行为分析的新方案，用于检测群体托儿攻击。首先，我们采用信息熵和潜在语义分析法，从项目、评分、时间和兴趣等维度分析用户行为模式，并提出了一套衡量用户异常行为的指标。其次，我们提出了一种基于用户多维特征的测量方法，从用户同步行为和异常行为的角度来捕捉群组成员的串通行为，并将串通程度高的群组作为候选群组。最后，基于用户的多维特征，利用高斯径向基函数（Gaussian-RBF）构建用户行为相似性矩阵，并采用光谱聚类算法发现候选组中的托群攻击者。实验表明，所提方法在 Netflix、CiaoDVD、Epinions 和 Amazon 数据集上的检测性能（F1-measure）分别达到 0.965、0.964、0.991 和 0.868，优于最先进的方法。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | 2024-02-01                                                                                                                                                                              | 2024-10-29 00:44:58                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:02                                                                                                                                  | 2024-10-29 00:44:57                                                                                                                                                                                                                                                                                                                                 | 604-616                                                                                                                                                                                          | citationNumber: 0; ccfInfo: CCF-B                   | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Robust Graph Embedding Recommendation Against Data Poisoning Attack                                                                                             | Zhong, Junyan; Liu, Chang; Wang, Huibin; Tian, Lele; Zhu, Han; Lam, Chan-Tong                                                                                           | 2023             | conferencePaper | Big Data Intelligence and Computing                                                                                                                                                                 |                                                                                                                                                                                                                                                                               | 978-981-99-2233-8                   |                                 | 10.1007/978-981-99-2233-8_8           | With the development of recommendation system technology, more and more Internet services are applied to recommendation systems. In recommendation systems, matrix factoring is the most widely used technique. However, matrix factoring algorithms are very susceptible to shilling attacks (trust or espionage). The former defends methods against data poisoning attacks focused on detecting individual attack behaviors. But there are few detection methods for group data poisoning attacks. Therefore, we propose a detection method based on Graph Neural Network (GNN) and adversarial learning. We train user-item nodes and edges through a semi-supervised learning approach, improving the robustness of the GNN recommendation system. Our work can be divided into the following parts: Firstly, we review the former recommendation systems and the graph representation learning recommendation systems. Secondly, we analyze the main vulnerabilities of the graph representation learning recommendation systems. Furthermore, the detection methods of data poisoning attacks are analyzed, and the difference between individual data poisoning attacks and group data poisoning attacks are discussed. Finally, we propose a per-process Robust-GNN semi-supervised detection model to conduct group detection on different types of attacks. In addition, we also analyze the sensitivity of the proposed methods. From the experiments results, it can be concluded that we should apply the attention mechanism to the proposed methods which makes it more generalized. 随着推荐系统技术的发展，越来越多的互联网服务被应用到推荐系统中。 在推荐系统中，矩阵因子法是应用最广泛的技术。然而，矩阵因式分解算法非常容易受到托攻击（信任攻击或间谍攻击）。前者抵御数据中毒攻击的方法侧重于检测单个攻击行为。但针对群体数据中毒攻击的检测方法却很少。因此，我们提出了一种基于图神经网络（GNN）和对抗学习的检测方法。我们通过半监督学习方法训练用户项目节点和边，从而提高了 GNN 推荐系统的鲁棒性。我们的工作可分为以下几个部分： 首先，我们回顾了以前的推荐系统和图表示学习推荐系统。其次，我们分析了图表示学习推荐系统的主要漏洞。此外，我们还分析了数据中毒攻击的检测方法，并讨论了个体数据中毒攻击和群体数据中毒攻击的区别。最后，我们提出了一种每进程 Robust-GNN 半监督检测模型，对不同类型的攻击进行群体检测。此外，我们还分析了所提方法的灵敏度。从实验结果中可以得出结论，我们应该将注意力机制应用到所提出的群组检测方法中。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | 2023                                                                                                                                                                                    | 2024-10-29 00:32:52                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:27                                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                     | 113-126                                                                                                                                                                                          | citationNumber: 0; ccfInfo: CCF-None DATACOM        | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Understanding and defending against White-box membership inference attack in deep learning                                                                      | Wu, Di; Qi, Saiyu; Qi, Yong; Li, Qian; Cai, Bowen; Guo, Qi; Cheng, Jingxian                                                                                             | 2023             | journalArticle  | Knowledge-Based Systems                                                                                                                                                                             | https://linkinghub.elsevier.com/retrieve/pii/S0950705122011078                                                                                                                                                                                                                |                                     | 09507051                        | 10.1016/j.knosys.2022.110014          | Membership inference attacks (MIA) exploit the fact that deep learning algorithms leak information about their training data through the learned model. It has been treated as an indicator which reveals the privacy leakage of machine learning models. In this work, we aim to understand the advantage achieved by White-box MIA, and defend against White-box MIA. Firstly, we estimate the KL divergence on the hidden layers’ features between training set and test set as the internal generalization gap. By comparing the internal generalization gap and the generalization gap on the output layer, we raise two insights including (1) the existence of larger generalization gaps on hidden layers and (2) feasibility of generalization gap minimization in defending White-box MIA. Based on our insights, we further design a novel defense method named Nirvana. It intentionally minimizes generalization gap to defend White-box MIA. Formally, Nirvana works by selecting a hidden layer with large generalization gaps and executing a multi-samples convex combination among features on the layer during the training to defend against White-box MIA. Finally, we empirically evaluate Nirvana with state-of-the-art defense methods on CIFAR100 dataset, Purchase100 dataset, and Texas dataset. The experiment results show that Nirvana achieves a trade-off between utility and privacy. It can defend both White-box MIA and Black-box MIA while the test accuracy of the model is maintained. It outperforms previous defense methods in defending against White-box MIA. 成员推理攻击（MIA）利用了深度学习算法通过所学模型泄露训练数据信息的事实。它被视为揭示机器学习模型隐私泄露的一个指标。在这项工作中，我们旨在了解白盒 MIA 所取得的优势，并防御白盒 MIA。首先，我们将训练集和测试集之间隐藏层特征的 KL 发散估计为内部泛化差距。通过比较内部泛化间隙和输出层的泛化间隙，我们提出了两个见解，包括：（1）存在较大的隐藏层泛化间隙；（2）泛化间隙最小化在防御白盒 MIA 中的可行性。基于我们的见解，我们进一步设计了一种名为 “涅槃 ”的新型防御方法。它有意最小化泛化间隙，以防御白盒 MIA。从形式上看，Nirvana 的工作原理是选择具有较大泛化间隙的隐藏层，并在训练过程中对该层的特征执行多采样凸组合，以防御白盒 MIA。最后，我们在 CIFAR100 数据集、Purchase100 数据集和 Texas 数据集上对 Nirvana 与最先进的防御方法进行了实证评估。实验结果表明，Nirvana 在实用性和隐私性之间实现了权衡。它既能防御白盒 MIA，也能防御黑盒 MIA，同时还能保持模型的测试精度。在防御白盒 MIA 方面，它优于以往的防御方法。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | 2023-01                                                                                                                                                                                 | 2024-10-29 00:53:33                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:28                                                                                                                                  | 2024-10-29 01:19:29                                                                                                                                                                                                                                                                                                                                 | 110014                                                                                                                                                                                           | ccfInfo: CCF-C KBS; citationNumber: 2               | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| On the Vulnerability of Graph Learning-based Collaborative Filtering                                                                                            | Xu, Senrong; Li, Liangyue; Li, Zenan; Yao, Yuan; Xu, Feng; Chen, Zulong; Lu, Quan; Tong, Hanghang                                                                       | 2023             | journalArticle  | ACM Transactions on Information Systems                                                                                                                                                             | https://dl.acm.org/doi/10.1145/3572834                                                                                                                                                                                                                                        |                                     | 1046-8188, 1558-2868            | 10.1145/3572834                       | Graph learning-based collaborative filtering (GLCF), which is built upon the message-passing mechanism of graph neural networks (GNNs), has received great recent attention and exhibited superior performance in recommender systems. However, although GNNs can be easily compromised by adversarial attacks as shown by the prior work, little attention has been paid to the vulnerability of GLCF. Questions like can GLCF models be just as easily fooled as GNNs remain largely unexplored. In this article, we propose to study the vulnerability of GLCF. Specifically, we first propose an adversarial attack against CLCF. Considering the unique challenges of attacking GLCF, we propose to adopt the greedy strategy in searching for the local optimal perturbations and design a reasonable attacking utility function to handle the non-differentiable ranking-oriented metrics. Next, we propose a defense to robustify GCLF. The defense is based on the observation that attacks usually introduce suspicious interactions into the graph to manipulate the message-passing process. We then propose to measure the suspicious score of each interaction and further reduce the message weight of suspicious interactions. We also give a theoretical guarantee of its robustness. Experimental results on three benchmark datasets show the effectiveness of both our attack and defense. 基于图学习的协同过滤（GLCF）建立在图神经网络（GNN）的消息传递机制之上，近年来受到了广泛关注，并在推荐系统中表现出了卓越的性能。然而，尽管先前的研究表明 GNNs 很容易受到恶意攻击，但人们却很少关注 GLCF 的脆弱性。诸如 GLCF 模型是否会像 GNN 一样容易被欺骗等问题在很大程度上仍未得到探讨。在本文中，我们提议研究 GLCF 的脆弱性。具体来说，我们首先提出一种针对 CLCF 的对抗性攻击。考虑到攻击 GLCF 所面临的独特挑战，我们建议采用贪婪策略来寻找局部最优扰动，并设计合理的攻击效用函数来处理无差别的排名导向指标。接下来，我们提出了一种稳健化 GCLF 的防御方法。该防御方法基于以下观察：攻击通常会在图中引入可疑的交互来操纵消息传递过程。然后，我们提出测量每次交互的可疑分数，并进一步降低可疑交互的消息权重。我们还从理论上保证了其鲁棒性。在三个基准数据集上的实验结果表明了我们的攻击和防御的有效性。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 2023-10-31                                                                                                                                                                              | 2024-10-25 05:46:14                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:09                                                                                                                                  | 2024-10-25 05:46:14                                                                                                                                                                                                                                                                                                                                 | 1-28                                                                                                                                                                                             | citationNumber: 3; ccfInfo: CCF-A  TOIS             | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| T&TRS: robust collaborative filtering recommender systems against attacks                                                                                       | Rezaimehr, Fatemeh; Dadkhah, Chitra                                                                                                                                     | 2023             | journalArticle  | Multimedia Tools and Applications                                                                                                                                                                   | https://link.springer.com/10.1007/s11042-023-16641-x                                                                                                                                                                                                                          |                                     | 1573-7721                       | 10.1007/s11042-023-16641-x            | In recent years, the Internet has had a main and important contribution to human life and the amount of data on the World Wide Web such as books, movies, videos and, etc. increase rapidly. Recommender systems allow users to quickly access items that are closer to their interests. One of the most popular and easiest models of recommender systems is the Collaborative Filtering (CF) model, which uses the items ratings given by users. The important challenge of CF is robust against the attacks which manipulated by fake users to reduce the efficiency of the system. Therefore, the impact of attacks on the item recommendations will increase and fake items will be easily recommended to users. The purpose of this paper is to design a robust CF recommender system, T&TRS, Time and Trust Recommender System, against user attacks. Our proposed system improves the performance of users clustering for detecting the fake users based on a novel community detection algorithm that is introduced in this paper. Our proposed system calculates the reliably value for all items ratings and tags them as suspicious or correct. T&TRS considered the rating time and implicit and explicit trust among users for constructing the weighted user-user network and detects communities as the nearest neighbors of the users to predict unknown items ratings. After detecting the suspect users and items using a novel community detection method, our proposed system removes them from rating matrix and predict the rating of unobserved items and generate the Top@k items according to the user interests. We inject the random and average attacks into the Epinion data set and evaluate our proposed systems based on Precision, Recall, F1, MAE, RMSE, and RC measures before and after attacks. The experimental results indicated that the precision of items recommendations increase after attack detection and show the effectiveness of T&TRS in comparison to the two base K-means methods such as KMCF-U, KMCF-I and graph-based methods such as TRACCF, and TOTAR 近年来，互联网为人类生活做出了重要贡献，万维网上的书籍、电影、视频等数据量迅速增加。推荐系统可以让用户快速访问与其兴趣更接近的项目。协作过滤（CF）模型是推荐系统中最流行、最简单的模型之一，它使用用户对项目的评分。协同过滤模型面临的重要挑战是如何抵御虚假用户为降低系统效率而操纵的攻击。因此，攻击对项目推荐的影响会增大，虚假项目会很容易被推荐给用户。本文旨在设计一种稳健的 CF 推荐系统--T&TRS（时间与信任推荐系统），以抵御用户攻击。我们提出的系统基于本文介绍的一种新型社区检测算法，提高了用户聚类检测虚假用户的性能。我们提出的系统会计算所有项目评分的可靠值，并将其标记为可疑或正确。T&TRS 在构建加权用户-用户网络时考虑了评分时间、用户之间的隐性和显性信任，并检测用户的近邻社区，以预测未知项目的评分。在使用新颖的社群检测方法检测出可疑用户和项目后，我们提出的系统将其从评分矩阵中移除，并预测未观察到的项目的评分，根据用户兴趣生成 Top@k 项目。我们在 Epinion 数据集中注入了随机攻击和平均攻击，并根据攻击前后的精度、召回率、F1、MAE、RMSE 和 RC 指标对我们提出的系统进行了评估。实验结果表明，与 KMCF-U 和 KMCF-I 这两种基本 K-means 方法以及 TRACCF 和 TOTAR 这两种基于图的方法相比，攻击检测后项目推荐的精确度有所提高，并显示了 T&TRS 的有效性。                                                                                                                                                                                                                                                                                                                                                                                                 | 2023-09-18                                                                                                                                                                              | 2024-11-06 02:18:29                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:17                                                                                                                                  | 2024-11-06 02:18:29                                                                                                                                                                                                                                                                                                                                 | 31701-31731                                                                                                                                                                                      | citationNumber: 0; ccfInfo: CCF-C MTA               | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Influence-Driven Data Poisoning for Robust Recommender Systems                                                                                                  | Wu, Chenwang; Lian, Defu; Ge, Yong; Zhu, Zhihao; Chen, Enhong                                                                                                           | 2023             | journalArticle  | IEEE transactions on pattern analysis and machine intelligence                                                                                                                                      |                                                                                                                                                                                                                                                                               |                                     | 1939-3539                       | 10.1109/TPAMI.2023.3274759            | "Recent studies have shown that recommender systems are vulnerable                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | and it is easy for attackers to inject well-designed malicious profiles into the system                                                                                                 | resulting in biased recommendations. We cannot deprive these data's injection right and deny their existence's rationality                                                                                                                                                                                                                                | making it imperative to study recommendation robustness. Despite impressive emerging work                                                            | threat assessment of the bi-level poisoning problem and the imperceptibility of poisoning users remain key challenges to be solved. To this end                                                                                                                                                                                                     | we propose Infmix                                                                                                                                                                                |                                                     |                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| Denoising Explicit Social Signals for Robust Recommendation                                                                                                     | Sun, Youchen                                                                                                                                                            | 2023             | conferencePaper | Proceedings of the 17th ACM Conference on Recommender Systems                                                                                                                                       | https://doi.org/10.1145/3604915.3608884                                                                                                                                                                                                                                       | 979-8-4007-0241-9                   |                                 | 10.1145/3604915.3608884               | Social recommender system assumes that user’s preferences can be influenced by their social connections. However, social networks are inherently noisy and contain redundant signals that are not helpful or even harmful for the recommendation task. In this extended abstract, we classify the noise in the explicit social links into intrinsic noise and extrinsic noise. Intrinsic noises are those edges that are natural in the social network but do not have an influence on the user preference modeling; Extrinsic noises, on the other hand, are those social links that are introduced intentionally through malicious attacks such that the attackers can manipulate the social influence to bias the recommendation outcome. To tackle this issue, we first propose a self-supervised denoising framework that learns to filter out the noisy social edges. Specifically, we introduce the influence of key opinion leaders to hinder the diffusion of noisy signals and also function as an extra source to enhance user preference modeling and alleviate the data sparsity issue. Experiments will be conducted on the real-world datasets for the Top-K ranking evaluation as well as the model’s robustness to simulated social noises. Finally, we discuss the future plan about how to defend against extrinsic noise from the attacker’s perspective through adversarial training. 社交推荐系统假定用户的偏好会受到其社交关系的影响。然而，社交网络本身是有噪声的，并且包含对推荐任务无益甚至有害的冗余信号。在本扩展摘要中，我们将显式社交链接中的噪声分为内在噪声和外在噪声。内在噪音是指那些在社交网络中自然存在的、但对用户偏好建模没有影响的边缘；而外来噪音则是指那些通过恶意攻击有意引入的社交链接，攻击者可以操纵社交影响力，使推荐结果出现偏差。为了解决这个问题，我们首先提出了一个自监督去噪框架，通过学习来过滤掉有噪声的社交边缘。具体来说，我们引入了关键意见领袖的影响力，以阻止噪声信号的扩散，同时作为额外的来源来增强用户偏好建模，并缓解数据稀疏性问题。我们将在真实世界的数据集上进行实验，以评估 Top-K 排名以及模型对模拟社会噪声的鲁棒性。最后，我们将讨论如何通过对抗训练从攻击者的角度抵御外在噪声的未来计划。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | 2023-09-14                                                                                                                                                                              | 2024-10-29 00:50:50                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:31                                                                                                                                  | 2024-10-28                                                                                                                                                                                                                                                                                                                                          | 1344–1348                                                                                                                                                                                        | citationNumber: 0; ccfInfo: CCF-B RecSys            | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| RecAD: Towards A Unified Library for Recommender Attack and Defense                                                                                             | Wang, Changsheng; Ye, Jianbai; Wang, Wenjie; Gao, Chongming; Feng, Fuli; He, Xiangnan                                                                                   | 2023             | preprint        |                                                                                                                                                                                                     | http://arxiv.org/abs/2309.04884                                                                                                                                                                                                                                               |                                     |                                 |                                       | In recent years, recommender systems have become a ubiquitous part of our daily lives, while they suffer from a high risk of being attacked due to the growing commercial and social values. Despite significant research progress in recommender attack and defense, there is a lack of a widely-recognized benchmarking standard in the field, leading to unfair performance comparison and limited credibility of experiments. To address this, we propose RecAD, a unified library aiming at establishing an open benchmark for recommender attack and defense. RecAD takes an initial step to set up a unified benchmarking pipeline for reproducible research by integrating diverse datasets, standard source codes, hyper-parameter settings, running logs, attack knowledge, attack budget, and evaluation results. The benchmark is designed to be comprehensive and sustainable, covering both attack, defense, and evaluation tasks, enabling more researchers to easily follow and contribute to this promising field. RecAD will drive more solid and reproducible research on recommender systems attack and defense, reduce the redundant efforts of researchers, and ultimately increase the credibility and practical value of recommender attack and defense. The project is released at https://github.com/gusye1234/recad. 近年来，推荐系统已成为我们日常生活中无处不在的一部分，但由于其商业价值和社会价值日益增长，推荐系统遭受攻击的风险也很高。尽管在推荐器攻击和防御方面的研究取得了重大进展，但该领域缺乏一个得到广泛认可的基准标准，导致性能比较不公平，实验可信度有限。为了解决这个问题，我们提出了一个统一的库 RecAD，旨在建立一个开放的推荐器攻防基准。RecAD 通过整合各种数据集、标准源代码、超参数设置、运行日志、攻击知识、攻击预算和评估结果，迈出了为可重现研究建立统一基准管道的第一步。该基准旨在实现全面性和可持续性，涵盖攻击、防御和评估任务，使更多研究人员能够轻松跟踪这一前景广阔的领域并做出贡献。RecAD 将推动有关推荐系统攻防的更扎实、更可重复的研究，减少研究人员的重复劳动，并最终提高推荐系统攻防的可信度和实用价值。项目发布于 https://github.com/gusye1234/recad。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | 2023-09-09                                                                                                                                                                              | 2024-10-29 00:43:03                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:32                                                                                                                                  | 2024-10-29 00:43:03                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                  | citationNumber: 0; ccfInfo: CCF-B RecSys            | /unread; Computer Science - Information Retrieval                                                                                                                                                                                                                                                                                                                                                                                      |
| RETRACTED ARTICLE: Detection of shilling attack in recommender system for YouTube video statistics using machine learning techniques                            | Rani, Shalli; Kaur, Manpreet; Kumar, Munish; Ravi, Vinayakumar; Ghosh, Uttam; Mohanty, Jnyana Ranjan                                                                    | 2023             | journalArticle  | Soft Computing                                                                                                                                                                                      | https://link.springer.com/10.1007/s00500-021-05586-8                                                                                                                                                                                                                          |                                     | 1432-7643, 1433-7479            | 10.1007/s00500-021-05586-8            | Literature survey shows that the recommendation systems have been largely adapted and evaluated in various domains. Due to low performances from various cyber attacks, the adoption of recommender system is in the initial stage of defense systems. One of the most common attacks for recommender system is shilling attack. There are some existing techniques for identifying the shilling attacks built in the user ratings patterns. The performance of ratings on target items differs between the attack user proﬁles and actual user proﬁles. To differentiate the certain proﬁles, the affected proﬁles are known as attack proﬁles. Besides the shilling attacks, real cyber attacks are taking place in the community which are being solved by Petri Net methods. These attacks can be falsely predicted (shilling attacks) by the users which can raise security threats. For identifying various shilling attacks without a priori knowledge, Recommendation System suffers from low accuracy. Basically, recommendation attack is split into nuke and push attack that encourage and discourage the recommended target item. The strength of shilling attack is usually measured by ﬁller size and attack size. An experiment over unsupervised machine learning algorithms with ﬁller size 3% over 3%, 5%, 8% and 10% attack sizes is presented for Netﬂix dataset. Furthermore, we conducted an experiment on data of 26 K videos on the Trending YouTube Video Statistics, to predict the user preferences for a particular genre of videos using Machine Learning Algorithms. Based on the results, it observed that the Boosted Decision tree performs the best with an accuracy of 99 percent. 文献调查显示，推荐系统已在各个领域得到广泛应用和评估。由于各种网络攻击的影响较小，推荐系统的应用还处于防御系统的初级阶段。托攻击是推荐系统最常见的攻击之一。现有的一些技术可以识别建立在用户评级模式中的托儿攻击。受攻击的用户画像和实际用户画像对目标项目的评分表现是不同的。为了区分不同的用户群，受影响的用户群被称为攻击用户群。除了托攻击外，社会上还发生了真实的网络攻击，这些攻击都是通过 Petri Net 方法解决的。用户可以错误地预测这些攻击（托儿攻击），从而引发安全威胁。为了在没有先验知识的情况下识别各种托攻击，推荐系统的准确率很低。从根本上说，推荐攻击分为 “核弹 ”攻击和 “推送 ”攻击。托攻击的强度通常用阈值大小和攻击大小来衡量。我们以 Netﬂix 数据集为例，对无监督机器学习算法进行了实验，实验中的攻击大小分别为 3%、5%、8% 和 10%。此外，我们还对 YouTube 趋势视频统计中的 26 K 个视频数据进行了实验，使用机器学习算法预测用户对特定类型视频的偏好。根据实验结果，我们发现助推决策树的准确率高达 99%，表现最佳。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | 2023-01                                                                                                                                                                                 | 2024-10-28 09:58:04                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:33                                                                                                                                  | 2024-10-29 01:19:11                                                                                                                                                                                                                                                                                                                                 | 377-389                                                                                                                                                                                          | ccfInfo: Not Found; citationNumber: Not Found       | /unread; Collaborative filtering; Artificial Intelligence; Shilling attack; Cyber attacks; Defense systems; Machine learning; Recommender system; Security threats; Soft computing; YouTube video statistics                                                                                                                                                                                                                           |
| A detection method for hybrid attacks in recommender systems                                                                                                    | Hao, Yaojun; Meng, Guoyan; Wang, Jian; Zong, Chunmei                                                                                                                    | 2023             | journalArticle  | Information Systems                                                                                                                                                                                 | https://linkinghub.elsevier.com/retrieve/pii/S0306437922001326                                                                                                                                                                                                                |                                     | 03064379                        | 10.1016/j.is.2022.102154              | To defend recommender systems, some methods have been proposed to detect model-generative shilling attacks and group shilling attacks respectively. Unfortunately, these two categories of attacks are often mixed together to carry out actual attacks. Without the additional knowledge about attack categories, traditional detection methods are likely to be trapped in the poor performance under hybrids of model-generative shilling attacks and group shilling attacks. To simultaneously detect these hybrid attacks, we put forward a detector based on the graph convolutional networks (GCN). Firstly, we extract five user features from the item popularity sequence and rating values to characterize both model-generative shilling profiles and group shilling profiles. And we define the users’ distance to construct the user graph. Secondly, we develop a two-stage scheme for detecting shilling profiles based on user features and the user graph. In particular, we propose a cluster-based method to partially label user nodes, and then these labeled samples are fed into a GCN-based detector to training the model to identify the other tangled shilling profiles. In the GCN-based model, we present a weighted loss function with R-drop regularization to solve the over-fitting problem and the imbalanced classification problem for the specific detection task. Finally, we make extensive experiments on three datasets to evaluate the proposed detector. Experiential results demonstrate the efficacy of our method when detecting the hybrids of model-generative shilling attacks and group shilling attacks. 为了防御推荐系统，人们提出了一些方法来分别检测模型生成托攻击和群体托攻击。遗憾的是，这两类攻击往往混合在一起实施实际攻击。如果没有关于攻击类别的额外知识，传统的检测方法很可能会陷入模型生成托攻击和群体托攻击混合攻击下性能不佳的困境。为了同时检测这些混合攻击，我们提出了一种基于图卷积网络（GCN）的检测器。首先，我们从项目流行度序列和评分值中提取了五个用户特征，以描述模型生成型托儿和群体托儿的特征。此外，我们还定义了用户距离，从而构建了用户图。其次，我们开发了一种基于用户特征和用户图谱的两阶段方案来检测托儿档案。具体来说，我们提出了一种基于聚类的方法来对用户节点进行部分标注，然后将这些标注样本送入基于 GCN 的检测器来训练模型，以识别其他纠缠在一起的托儿档案。在基于 GCN 的模型中，我们提出了带有 R-drop 正则化的加权损失函数，以解决特定检测任务中的过拟合问题和不平衡分类问题。最后，我们在三个数据集上进行了大量实验，以评估所提出的检测器。实验结果证明了我们的方法的有效性。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | 2023-03                                                                                                                                                                                 | 2024-10-29 00:37:57                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:19:54                                                                                                                                  | 2024-10-29 00:38:29                                                                                                                                                                                                                                                                                                                                 | 102154                                                                                                                                                                                           | citationNumber: 3; ccfInfo: CCF-B IS                | /unread; Graph convolutional networks; Group shilling attacks; Hybrid attacks; Model-generative shilling attacks; Shilling attack detection                                                                                                                                                                                                                                                                                            |
| A novel classification-based shilling attack detection approach for multi-criteria recommender systems                                                          | Turkoglu, Tugba; Yalcin, Emre; Kaleli, Cihan                                                                                                                            | 2023             | journalArticle  | Computational Intelligence                                                                                                                                                                          |                                                                                                                                                                                                                                                                               |                                     |                                 | 10.1111/coin.12579                    | Recommender systems are emerging techniques guiding individuals with provided referrals by considering their past rating behaviors. By collecting multi-criteria preferences concentrating on distinguishing perspectives of the items, a new extension of traditional recommenders, multi-criteria recommender systems reveal how much a user likes an item and why user likes it; thus, they can improve predictive accuracy. However, these systems might be more vulnerable to malicious attacks than traditional ones, as they expose multiple dimensions of user opinions on items. Attackers might try to inject fake profiles into these systems to skew the recommendation results in favor of some particular items or to bring the system into discredit. Although several methods exist to defend systems against such attacks for traditional recommenders, achieving robust systems by capturing shill profiles remains elusive for multi-criteria rating-based ones. Therefore, in this study, we first consider a prominent and novel attack type, that is, the power-item attack model, and introduce its four distinct variants adapted for multi-criteria data collections. Then, we propose a classification method detecting shill profiles based on various generic and model-based user attributes, most of which are new features usually related to item popularity and distribution of rating values. The experiments conducted on three benchmark datasets conclude that the proposed method successfully detects attack profiles from genuine users even with a small selected size and attack size. The empirical outcomes also demonstrate that item popularity and user characteristics based on their rating profiles are highly beneficial features in capturing shilling attack profiles. 推荐系统是一种新兴技术，它通过考虑个人过去的评分行为，为其提供推荐。多标准推荐系统是对传统推荐系统的新扩展，它通过收集多标准偏好来集中区分物品的不同视角，从而揭示用户对物品的喜爱程度以及喜爱的原因，从而提高预测的准确性。然而，与传统推荐系统相比，这些系统可能更容易受到恶意攻击，因为它们暴露了用户对物品的多个维度的意见。攻击者可能会试图向这些系统注入虚假资料，使推荐结果偏向某些特定项目，或使系统声誉受损。虽然对于传统的推荐系统来说，有多种方法可以抵御此类攻击，但对于基于多标准评级的推荐系统来说，通过捕捉虚假用户配置文件来实现稳健的系统仍然是难以实现的。因此，在本研究中，我们首先考虑了一种突出的新型攻击类型，即权力项攻击模型，并介绍了其适用于多标准数据收集的四种不同变体。然后，我们提出了一种基于各种通用和基于模型的用户属性来检测托的分类方法，其中大部分是通常与项目流行度和评分值分布相关的新特征。在三个基准数据集上进行的实验得出结论，即使所选规模和攻击规模较小，所提出的方法也能成功地从真实用户中检测出攻击配置文件。实证结果还证明，项目流行度和基于其评分档案的用户特征是捕捉托攻击档案的非常有利的特征。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | 2023-06-01                                                                                                                                                                              | 2024-10-28 10:02:53                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:19:54                                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                     | 499-528                                                                                                                                                                                          | ccfInfo: CCF-C; citationNumber: 0                   | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Detection of Shilling Attacks on Collaborative Filtering Recommender Systems by Combining Multiple Random Forest Models                                         | Grozdanić, Vjeran; Vladimir, Klemo; Delač, Goran; Šilić, Marin                                                                                                          | 2023             | conferencePaper | 2023 46th MIPRO ICT and Electronics Convention (MIPRO)                                                                                                                                              | https://ieeexplore.ieee.org/document/10159874/?arnumber=10159874                                                                                                                                                                                                              |                                     |                                 | 10.23919/MIPRO57284.2023.10159874     | Collaborative filtering recommender systems are one of the essential recommender systems and are still widely used in combination with other algorithms to make predictions for users. However, they are vulnerable to shilling attacks, and if there isn’t any detection system to prevent those attacks, original recommendations can be heavily influenced to benefit the attackers. Designing attack-resistant recommendation systems is not an easy task, and many researchers have tried to tackle that problem. In this paper, a new approach that combines multiple random forest models is proposed. Each of the random forest models is specialized in detecting one group of shilling attacks, and then all the models are combined into an ensemble model. Experimental results show that the proposed ensemble is capable of detecting attack profiles at high rate without causing significant bias in the original recommendation system. 协作过滤推荐系统是必不可少的推荐系统之一，目前仍被广泛应用于与其他算法相结合为用户做出预测。然而，它们很容易受到托攻击，如果没有任何检测系统来防止这些攻击，原始推荐就会受到严重影响，使攻击者受益。设计抗攻击的推荐系统并非易事，许多研究人员都试图解决这一问题。本文提出了一种结合多个随机森林模型的新方法。每个随机森林模型专门检测一组托攻击，然后将所有模型组合成一个集合模型。实验结果表明，所提出的集合模型能够高速检测攻击特征，而不会对原始推荐系统造成明显偏差。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 2023-05                                                                                                                                                                                 | 2024-10-29 00:48:00                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:03                                                                                                                                  | 2024-10-29 00:48:00                                                                                                                                                                                                                                                                                                                                 | 959-963                                                                                                                                                                                          | citationNumber: 0; ccfInfo: CCF-None MIPRO          | /unread; Collaboration; Collaborative filtering; Recommender systems; shilling attacks; collaborative filtering; attack detection; classification; Forestry; Prediction algorithms; random forest; Task analysis                                                                                                                                                                                                                       |
| KC-GCN: A Semi-Supervised Detection Model against Various Group Shilling Attacks in Recommender Systems                                                         | Cai, Hongyun; Ren, Jichao; Zhao, Jing; Yuan, Shilin; Meng, Jie                                                                                                          | 2023             | journalArticle  | Wireless Communications and Mobile Computing                                                                                                                                                        | https://onlinelibrary.wiley.com/doi/abs/10.1155/2023/2854874                                                                                                                                                                                                                  |                                     | 1530-8677                       | 10.1155/2023/2854874                  | Various detection methods have been proposed for defense against group shilling attacks in recommender systems; however, these methods cannot effectively detect attack groups generated based on adversarial attacks (e.g., GOAT) or mixed attack groups. In this study, we propose a two-stage method, called KC-GCN, which is based on k-cliques and graph convolutional networks. First, we construct a user relationship graph, generate suspicious candidate groups, and extract influential users by calculating the user nearest-neighbor similarity. We construct the user relationship graph by calculating the edge weight between any two users through analyzing their similarity over suspicious time intervals on each item. Second, we combine the extracted user initial embeddings and the structural features hidden in the user relationship graph to detect attackers. On the Netflix and sampled Amazon datasets, the detection results of KC-GCN surpass those of the state-of-the-art methods under different types of group shilling attacks. The F1-measure of KC-GCN can reach above 93% and 87% on these two datasets, respectively. 为防御推荐系统中的托群攻击，人们提出了多种检测方法；然而，这些方法无法有效检测基于对抗性攻击（如 GOAT）或混合攻击群生成的攻击群。在本研究中，我们提出了一种基于 k-cliques 和图卷积网络的两阶段方法，称为 KC-GCN。首先，我们构建用户关系图，生成可疑候选群组，并通过计算用户近邻相似度提取有影响力的用户。我们通过分析任意两个用户在每个项目上的可疑时间间隔，计算他们之间的边缘权重，从而构建用户关系图。其次，我们结合提取的用户初始嵌入和隐藏在用户关系图中的结构特征来检测攻击者。在 Netflix 和抽样亚马逊数据集上，KC-GCN 在不同类型的群托攻击下的检测结果都超过了最先进的方法。在这两个数据集上，KC-GCN 的 F1 测量值分别达到 93% 和 87% 以上。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | 2023                                                                                                                                                                                    | 2024-10-29 00:46:39                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:07                                                                                                                                  | 2024-10-29 00:46:39                                                                                                                                                                                                                                                                                                                                 | 2854874                                                                                                                                                                                          | citationNumber: 1; ccfInfo: Not Found               | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| SHADEWATCHER: Recommendation-guided Cyber Threat Analysis using System Audit Records                                                                            | Zengy, Jun; Wang, Xiang; Liu, Jiahao; Chen, Yinfang; Liang, Zhenkai; Chua, Tat-Seng; Chua, Zheng Leong                                                                  | 2022             | conferencePaper | 2022 IEEE Symposium on Security and Privacy (SP)                                                                                                                                                    | https://ieeexplore.ieee.org/document/9833669/?arnumber=9833669                                                                                                                                                                                                                |                                     |                                 | 10.1109/SP46214.2022.9833669          | System auditing provides a low-level view into cyber threats by monitoring system entity interactions. In response to advanced cyber-attacks, one prevalent solution is to apply data provenance analysis on audit records to search for anomalies (anomalous behaviors) or specifications of known attacks. However, existing approaches suffer from several limitations: 1) generating high volumes of false alarms, 2) relying on expert knowledge, or 3) producing coarse-grained detection signals. In this paper, we recognize the structural similarity between threat detection in cybersecurity and recommendation in information retrieval. By mapping security concepts of system entity interactions to recommendation concepts of user-item interactions, we identify cyber threats by predicting the preferences of a system entity on its interactive entities. Furthermore, inspired by the recent advances in modeling high-order connectivity via item side information in the recommendation, we transfer the insight to cyber threat analysis and customize an automated detection system, SHADEWATCHER. It fulfills the potential of high-order information in audit records via graph neural networks to improve detection effectiveness. Besides, we equip SHADEWATCHER with dynamic updates towards better generalization to false alarms. In our evaluation against both real-life and simulated cyber-attack scenarios, SHADEWATCHER shows its advantage in identifying threats with high precision and recall rates. Moreover, SHADEWATCHER is capable of pinpointing threats from nearly a million system entity interactions within seconds. 系统审计通过监控系统实体之间的交互，提供了一个低层次的网络威胁视角。为应对高级网络攻击，一种流行的解决方案是对审计记录进行数据来源分析，以搜索异常（异常行为）或已知攻击的规格。然而，现有方法有几个局限性： 1）产生大量误报；2）依赖专家知识；或 3）产生粗粒度检测信号。在本文中，我们认识到网络安全中的威胁检测与信息检索中的推荐之间存在结构上的相似性。通过将系统实体交互的安全概念映射到用户-物品交互的推荐概念，我们通过预测系统实体对其交互实体的偏好来识别网络威胁。此外，受最近在通过推荐中的项目方信息建立高阶连接建模方面取得的进展的启发，我们将这一洞察力转移到网络威胁分析中，并定制了一个自动检测系统--SHADEWATCHER。它通过图神经网络发挥了审计记录中高阶信息的潜力，从而提高了检测效率。此外，我们还为 SHADEWATCHER 配备了动态更新功能，以便更好地泛化误报。在我们对现实生活和模拟网络攻击场景进行的评估中，SHADEWATCHER 显示出其在识别威胁方面的优势，具有较高的精确率和召回率。此外，SHADEWATCHER 还能在数秒内从近百万系统实体交互中精确定位威胁。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | 2022-05                                                                                                                                                                                 | 2024-10-29 00:37:31                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:15                                                                                                                                  | 2024-10-29 00:37:31                                                                                                                                                                                                                                                                                                                                 | 489-506                                                                                                                                                                                          | ccfInfo: CCF-A S&P; citationNumber: 94              | /unread; Analytical models; Behavioral sciences; Privacy; Computer security; Graph neural networks; Information retrieval; Monitoring                                                                                                                                                                                                                                                                                                  |
| Federated Unlearning for On-Device Recommendation                                                                                                               | Yuan, Wei; Yin, Hongzhi; Wu, Fangzhao; Zhang, Shijie; He, Tieke; Wang, Hao                                                                                              | 2022             | preprint        |                                                                                                                                                                                                     | http://arxiv.org/abs/2210.10958                                                                                                                                                                                                                                               |                                     |                                 |                                       | The increasing data privacy concerns in recommendation systems have made federated recommendations attract more and more attention. Existing federated recommendation systems mainly focus on how to effectively and securely learn personal interests and preferences from their on-device interaction data. Still, none of them considers how to efficiently erase a user’s contribution to the federated training process. We argue that such a dual setting is necessary. First, from the privacy protection perspective, “the right to be forgotten (RTBF)” requires that users have the right to withdraw their data contributions. Without the reversible ability, federated recommendation systems risk breaking data protection regulations. On the other hand, enabling a federated recommender to forget specific users can improve its robustness and resistance to malicious clients’ attacks. To support user unlearning in federated recommendation systems, we propose an efficient unlearning method FRU (Federated  Recommendation U nlearning), inspired by the log-based rollback  mechanism of transactions in database management systems. It  removes a user’s contribution by rolling back and calibrating the  historical parameter updates and then uses these updates to speed  up federated recommender reconstruction. However, storing all  historical parameter updates on resource-constrained personal devices is challenging and even infeasible. In light of this challenge,  we propose a small-sized negative sampling method to reduce the  number of item embedding updates and an importance-based update selection mechanism to store only important model updates.  To evaluate the effectiveness of FRU, we propose an attack method  to disturb federated recommenders via a group of compromised  users. Then, we use FRU to recover recommenders by eliminating  these users’ influence. Finally, we conduct extensive experiments  on two real-world recommendation datasets (i.e. MovieLens-100k and Steam-200k) with two widely used federated recommenders to  show the efficiency and effectiveness of our proposed approaches. 推荐系统中日益增长的数据隐私问题使联合推荐受到越来越多的关注。现有的联合推荐系统主要关注如何有效、安全地从设备交互数据中学习个人兴趣和偏好。但是，它们都没有考虑如何有效地消除用户对联合训练过程的贡献。我们认为这种双重设置是必要的。首先，从隐私保护的角度来看，“被遗忘权（RTBF）”要求用户有权撤回其数据贡献。如果不具备可逆能力，联合推荐系统就有可能违反数据保护法规。另一方面，使联合推荐系统能够遗忘特定用户可以提高其鲁棒性和抵御恶意客户端攻击的能力。 为了支持联合推荐系统中的用户解除学习，我们受数据库管理系统中基于日志的事务回滚机制的启发，提出了一种高效的解除学习方法 FRU（Federated Recommendation U nlearning）。它通过回滚和校准历史参数更新来消除用户的贡献，然后利用这些更新加速联合推荐系统的重建。然而，在资源有限的个人设备上存储所有历史参数更新具有挑战性，甚至是不可行的。有鉴于此，我们提出了一种小规模负抽样方法来减少项目嵌入更新的数量，并提出了一种基于重要性的更新选择机制来只存储重要的模型更新。 为了评估 FRU 的有效性，我们提出了一种攻击方法，通过一组受损用户来干扰联合推荐器。然后，我们使用 FRU 通过消除这些用户的影响来恢复推荐器。最后，我们在两个真实世界的推荐数据集（即 MovieLens-100k 和 Steam-200k）上使用两个广泛使用的联合推荐器进行了大量实验，以展示我们提出的方法的效率和有效性。                                                                                                                                                                                                                                                                                                       | 2022-12-03                                                                                                                                                                              | 2024-10-29 01:19:16                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:31                                                                                                                                  | 2024-10-29 01:19:16                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                  | ccfInfo: CCF-B WSDM; citationNumber: 24             | /unread; Computer Science - Information Retrieval                                                                                                                                                                                                                                                                                                                                                                                      |
| Detect Professional Malicious User With Metric Learning in Recommender Systems                                                                                  | Xu, Yuanbo; Yang, Yongjian; Wang, En; Zhuang, Fuzhen; Xiong, Hui                                                                                                        | 2022             | journalArticle  | IEEE Transactions on Knowledge and Data Engineering                                                                                                                                                 | https://ieeexplore.ieee.org/document/9271919/?arnumber=9271919                                                                                                                                                                                                                |                                     | 1558-2191                       | 10.1109/TKDE.2020.3040618             | In e-commerce, online retailers are usually suffering from professional malicious users (PMUs), who utilize negative reviews and low ratings to their consumed products on purpose to threaten the retailers for illegal profits. PMUs are difficult to be detected because they utilize masking strategies to disguise themselves as normal users. Specifically, there are three challenges for PMU detection: 1) professional malicious users do not conduct any abnormal or illegal interactions (they never concurrently leave too many negative reviews and low ratings at the same time), and they conduct masking strategies to disguise themselves. Therefore, conventional outlier detection methods are confused by their masking strategies. 2) the PMU detection model should take both ratings and reviews into consideration, which makes PMU detection a multi-modal problem. 3) there are no datasets with labels for professional malicious users in public, which makes PMU detection an unsupervised learning problem. To this end, we propose an unsupervised multi-modal learning model: MMD, which employs Metric learning for professional Malicious users Detection with both ratings and reviews. MMD first utilizes a modified RNN to project the informational review into a sentiment score, which jointly considers the ratings and reviews. Then professional malicious user profiling (MUP) is proposed to catch the sentiment gap between sentiment scores and ratings. MUP filters the users and builds a candidate PMU set. We apply a metric learning-based clustering to learn a proper metric matrix for PMU detection. Finally, we can utilize this metric and labeled users to detect PMUs. Specifically, we apply the attention mechanism in metric learning to improve the model’s performance. The extensive experiments in four datasets demonstrate that our proposed method can solve this unsupervised detection problem. Moreover, the performance of the state-of-the-art recommender models is enhanced by taking MMD as a preprocessing stage. 在电子商务中，在线零售商通常会受到职业恶意用户（PMU）的侵害，他们利用负面评论和低评分对所消费的产品进行评价，目的是威胁零售商以获取非法利润。PMU 很难被发现，因为他们会利用掩盖策略将自己伪装成正常用户。具体来说，PMU 检测面临三个挑战： 1）专业恶意用户不会进行任何异常或非法互动（他们不会同时留下太多负面评论和低评分），而且他们会采取掩盖策略来伪装自己。因此，传统的离群点检测方法会被他们的掩盖策略所迷惑。2）PMU 检测模型应同时考虑评分和评论，这使得 PMU 检测成为一个多模式问题。3）目前还没有公开的带有职业恶意用户标签的数据集，这使得 PMU 检测成为一个无监督学习问题。为此，我们提出了一种无监督多模态学习模型： MMD 采用度量学习来检测具有评分和评论的专业恶意用户。MMD 首先利用改进的 RNN 将信息评论投射到情感分数中，该分数同时考虑了评分和评论。然后，提出了专业恶意用户剖析（MUP）来捕捉情感分数和评分之间的情感差距。MUP 过滤用户并建立候选 PMU 集。我们应用基于度量学习的聚类来学习用于 PMU 检测的适当度量矩阵。最后，我们就可以利用这个度量矩阵和标签用户来检测 PMU。具体来说，我们在度量学习中应用了注意力机制，以提高模型的性能。在四个数据集上进行的大量实验证明，我们提出的方法可以解决这个无监督检测问题。此外，通过将 MMD 作为预处理阶段，先进推荐模型的性能也得到了提高。                                                                                                                                                                                                                                                                                                                                                                                                                 | 2022-09                                                                                                                                                                                 | 2024-10-28 09:55:05                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:26                                                                                                                                  | 2024-10-29 01:03:25                                                                                                                                                                                                                                                                                                                                 | 4133-4146                                                                                                                                                                                        | ccfInfo: CCF-A  TKDE; citationNumber: 9             | /unread; Collaboration; Recommender systems; recommender system; Anomaly detection; Measurement; metric learning; Negative feedback; Phasor measurement units; Professional malicious users; unsupervised learning; Unsupervised learning                                                                                                                                                                                              |
| Defending Substitution-Based Profile Pollution Attacks on Sequential Recommenders                                                                               | Yue, Zhenrui; Zeng, Huimin; Kou, Ziyi; Shang, Lanyu; Wang, Dong                                                                                                         | 2022             | preprint        |                                                                                                                                                                                                     | http://arxiv.org/abs/2207.11237                                                                                                                                                                                                                                               |                                     |                                 |                                       | While sequential recommender systems achieve significant improvements on capturing user dynamics, we argue that sequential recommenders are vulnerable against substitution-based profile pollution attacks. To demonstrate our hypothesis, we propose a substitution-based adversarial attack algorithm, which modifies the input sequence by selecting certain vulnerable elements and substituting them with adversarial items. In both untargeted and targeted attack scenarios, we observe significant performance deterioration using the proposed profile pollution algorithm. Motivated by such observations, we design an efficient adversarial defense method called Dirichlet neighborhood sampling. Specifically, we sample item embeddings from a convex hull constructed by multi-hop neighbors to replace the original items in input sequences. During sampling, a Dirichlet distribution is used to approximate the probability distribution in the neighborhood such that the recommender learns to combat local perturbations. Additionally, we design an adversarial training method tailored for sequential recommender systems. In particular, we represent selected items with one-hot encodings and perform gradient ascent on the encodings to search for the worst case linear combination of item embeddings in training. As such, the embedding function learns robust item representations and the trained recommender is resistant to test-time adversarial examples. Extensive experiments show the effectiveness of both our attack and defense methods, which consistently outperform baselines by a significant margin across model architectures and datasets. 虽然顺序推荐系统在捕捉用户动态方面取得了显著的改进，但我们认为顺序推荐系统很容易受到基于替换的配置文件污染攻击。为了证明我们的假设，我们提出了一种基于替换的对抗攻击算法，该算法通过选择某些易受攻击的元素并用对抗项替换它们来修改输入序列。在非目标攻击和目标攻击场景中，我们都观察到使用所提出的轮廓污染算法后性能显著下降。受这些观察结果的启发，我们设计了一种高效的对抗性防御方法，称为 Dirichlet 邻域采样。具体来说，我们从多跳邻域构建的凸壳中抽样项目嵌入，以替换输入序列中的原始项目。在采样过程中，我们使用 Dirichlet 分布来近似邻域中的概率分布，从而使推荐器学会对抗局部扰动。此外，我们还为顺序推荐系统设计了一种对抗性训练方法。具体来说，我们用单点编码表示选定的项目，并在编码上执行梯度上升，以在训练中搜索项目嵌入的最差线性组合。因此，嵌入函数可以学习稳健的项目表征，训练有素的推荐器可以抵御测试时间的对抗性示例。广泛的实验表明，我们的攻击和防御方法都很有效，在不同的模型架构和数据集上，我们的效果都明显优于基线方法。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | 2022-07-19                                                                                                                                                                              | 2024-10-29 01:01:47                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:01                                                                                                                                  | 2024-10-29 01:01:47                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                  | citationNumber: 7; ccfInfo: CCF-B RecSys            | /unread; Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security; Computer Science - Information Retrieval; Computer Science - Machine Learning                                                                                                                                                                                                                                                       |
| Probabilistic Inference and Trustworthiness Evaluation of Associative Links Toward Malicious Attack Detection for Online Recommendations                        | Yang, Zhihai; Sun, Qindong; Zhang, Yaling                                                                                                                               | 2022             | journalArticle  | IEEE Transactions on Dependable and Secure Computing                                                                                                                                                | https://ieeexplore.ieee.org/document/9194336/?arnumber=9194336                                                                                                                                                                                                                |                                     | 1941-0018                       | 10.1109/TDSC.2020.3023114             | The increasing use of recommender systems as personalization recommendation services such as Amazon, TripAdvisor, and Yelp, has stressed the demand for secure and usable abnormality detection techniques, due to fundamental vulnerabilities of recommender systems and their openness. With the emergence of new attacks, how to defend diverse malicious attacks for online recommendations is a challenging issue. Moreover, characterizing and evaluating sparse rating behaviors are a long-standing problem that still remains open, leading to an upsurge of research, as well as real application. This article investigates probabilistic inference and trustworthiness evaluation of behavioral links according to coupled association networks converted from rating behaviors, and presents a unified detection framework from a novel perspective to spot diverse malicious threats. First, an association graph is constructed from the original rating matrix based on both the inherent rating motivation of users and atomic propagation rules of coupled networks. Then, we evaluate the trustworthiness of link behaviors in the targeted network of coupled association network by exploiting a factor graph model of coupled network, and redetermine concerned links in the targeted network. Finally, suspicious users and items can be empirically inferred by comprehensively evaluating the trustworthiness of both links and nodes in the targeted network. Extensive experiments on synthetic data for profile injection attacks and co-visitation injection attacks, as well as real-world data including Amazon and TripAdvisor, demonstrate the effectiveness of the proposed detection approach compared with competing benchmarks. 亚马逊、TripAdvisor 和 Yelp 等公司越来越多地使用推荐系统作为个性化推荐服务，由于推荐系统的基本漏洞及其开放性，对安全、可用的异常检测技术的需求日益突出。随着新攻击的出现，如何防御针对在线推荐的各种恶意攻击是一个具有挑战性的问题。此外，稀疏评级行为的特征描述和评估是一个长期存在的问题，至今仍未解决，导致研究和实际应用的激增。本文根据评级行为转换而来的耦合关联网络，研究了行为链接的概率推断和可信度评估，并从一个新颖的角度提出了一个统一的检测框架，以发现各种恶意威胁。首先，基于用户固有的评分动机和耦合网络的原子传播规则，从原始评分矩阵中构建关联图。然后，利用耦合网络的因子图模型，评估耦合关联网络中目标网络中链接行为的可信度，并重新确定目标网络中的相关链接。最后，通过综合评估目标网络中链接和节点的可信度，可以根据经验推断出可疑用户和项目。在针对配置文件注入攻击和共同访问注入攻击的合成数据以及包括亚马逊和 TripAdvisor 在内的真实世界数据上进行的大量实验证明，与竞争基准相比，所提出的检测方法非常有效。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | 2022-03                                                                                                                                                                                 | 2024-10-28 09:56:01                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:32                                                                                                                                  | 2024-10-28 09:56:01                                                                                                                                                                                                                                                                                                                                 | 879-896                                                                                                                                                                                          | citationNumber: 7; ccfInfo: CCF-A TDSC              | /unread; Recommender systems; Social network services; Analytical models; recommender system; Feature extraction; malicious attack; Adaptation models; Attack detection; Cathode ray tubes; Probabilistic logic; rating behavior; trustworthiness evaluation                                                                                                                                                                           |
| Three Birds With One Stone: User Intention Understanding and Influential Neighbor Disclosure for Injection Attack Detection                                     | Yang, Zhihai; Sun, Qindong; Liu, Zhaoli                                                                                                                                 | 2022             | journalArticle  | IEEE Transactions on Information Forensics and Security                                                                                                                                             | https://ieeexplore.ieee.org/document/9693911/?arnumber=9693911                                                                                                                                                                                                                |                                     | 1556-6021                       | 10.1109/TIFS.2022.3146769             | Recommender system, as a data-driven way to help customers locate products that match their interests, is increasingly critical for providing competitive customer suggestions in many web services. However, recommender systems are highly vulnerable to malicious injection attacks due to their fundamental vulnerabilities and openness. With the endless emergence of new attacks, how to provide a feasible way for defending different malicious threats against online recommendations is still an under-explored issue. In this paper, we explore a new way to defend malicious injection attacks through user intention understanding and influential neighbour disclosure. Specifically, we propose a detection approach, termed TBOS (Three Birds with One Stone), to deal with different malicious threats. In TBOS, we first develop the discrimination of attack target by combining global influence evaluation and risk attitude estimation of users. In order to make TBOS controllable, second, we propose to incorporate an optimal denoising mechanism to remove disturbed information before detection. To enhance the representativeness and predictability of detection model, finally, we propose to leverage a behavioral label propagation mechanism based on constructed label space for the determination of malicious injection behaviors. Extensive experiments on both synthetic and real data demonstrate that TBOS outperforms all baselines in different cases. Particularly, the detection performance of TBOS can achieve an improvement of 6.08% FAR (false alarm rate) for optimal-injection attacks, an improvement of 3.83% FAR in average for co-visitation injection attacks, as well as an improvement of 2.3% for profile injection attacks over benchmarks in terms of FAR while keeping the highest DR (detection rate). Additional experiments on real-world data show that TBOS brings an improvement with the advantage of 6.5% FAR in average compared with baselines. 在许多网络服务中，推荐系统作为一种以数据为驱动的方式来帮助客户找到与其兴趣相匹配的产品，对于提供有竞争力的客户建议越来越重要。然而，推荐系统由于其基本的脆弱性和开放性，极易受到恶意注入攻击。随着新攻击的层出不穷，如何提供一种可行的方法来防御针对在线推荐的各种恶意威胁，仍然是一个尚未得到充分探讨的问题。在本文中，我们探索了一种通过用户意图理解和有影响力的邻居披露来防御恶意注入攻击的新方法。具体来说，我们提出了一种名为 TBOS（一石三鸟）的检测方法来应对不同的恶意威胁。在 TBOS 中，我们首先结合用户的全局影响力评估和风险态度估计，建立了攻击目标的判别方法。其次，为了使 TBOS 具有可控性，我们建议在检测前加入优化去噪机制以去除干扰信息。为了提高检测模型的代表性和可预测性，最后，我们提出利用基于构建的标签空间的行为标签传播机制来确定恶意注入行为。在合成数据和真实数据上进行的大量实验表明，TBOS 在不同情况下的性能均优于所有基线。特别是在最优注入攻击方面，TBOS 的检测性能比基准提高了 6.08% 的误报率（FAR）；在协同观察注入攻击方面，平均提高了 3.83% 的误报率（FAR）；在轮廓注入攻击方面，在保持最高 DR（检测率）的情况下，比基准提高了 2.3% 的误报率（FAR）。在真实世界数据上进行的其他实验表明，与基准相比，TBOS 的平均 FAR 提高了 6.5%。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | 2022                                                                                                                                                                                    | 2024-10-28 10:03:38                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:18                                                                                                                                  | 2024-10-28 10:03:38                                                                                                                                                                                                                                                                                                                                 | 531-546                                                                                                                                                                                          | citationNumber: 3; ccfInfo: CCF-A TIFS              | /unread; Recommender systems; Position measurement; attack detection; behavior representation; Birds; Estimation; Injection attack; performance analysis; Predictive models; Sun; Technological innovation                                                                                                                                                                                                                             |
| RGRecSys: A Toolkit for Robustness Evaluation of Recommender Systems                                                                                            | Ovaisi, Zohreh; Heinecke, Shelby; Li, Jia; Zhang, Yongfeng; Zheleva, Elena; Xiong, Caiming                                                                              | 2022             | preprint        |                                                                                                                                                                                                     | http://arxiv.org/abs/2201.04399                                                                                                                                                                                                                                               |                                     |                                 |                                       | Robust machine learning is an increasingly important topic that focuses on developing models resilient to various forms of imperfect data. Due to the pervasiveness of recommender systems in online technologies, researchers have carried out several robustness studies focusing on data sparsity and profile injection attacks. Instead, we propose a more holistic view of robustness for recommender systems that encompasses multiple dimensions - robustness with respect to sub-populations, transformations, distributional disparity, attack, and data sparsity. While there are several libraries that allow users to compare different recommender system models, there is no software library for comprehensive robustness evaluation of recommender system models under different scenarios. As our main contribution, we present a robustness evaluation toolkit, Robustness Gym for RecSys (RGRecSys -- https://www.github.com/salesforce/RGRecSys), that allows us to quickly and uniformly evaluate the robustness of recommender system models. 鲁棒性机器学习是一个日益重要的课题，其重点是开发能够抵御各种形式不完美数据的模型。由于推荐系统在在线技术中的普遍存在，研究人员已经开展了多项鲁棒性研究，重点关注数据稀疏性和配置文件注入攻击。相反，我们对推荐系统的鲁棒性提出了更全面的看法，它包含多个维度--与子群体、转换、分布差异、攻击和数据稀疏有关的鲁棒性。虽然有几个库可以让用户比较不同的推荐系统模型，但还没有一个软件库可以在不同场景下对推荐系统模型进行全面的鲁棒性评估。作为我们的主要贡献，我们提出了一个鲁棒性评估工具包--Robustness Gym for RecSys（RGRecSys -- https://www.github.com/salesforce/RGRecSys），它允许我们快速、统一地评估推荐系统模型的鲁棒性。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 2022-01-12                                                                                                                                                                              | 2024-10-29 01:14:26                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:14                                                                                                                                  | 2024-10-29 01:14:26                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                  | citationNumber: 10; ccfInfo: CCF-B WSDM             | /unread; Computer Science - Artificial Intelligence; Computer Science - Information Retrieval; Computer Science - Machine Learning                                                                                                                                                                                                                                                                                                     |
| Addressing Confounding Feature Issue for Causal Recommendation                                                                                                  | He, Xiangnan; Zhang, Yang; Feng, Fuli; Song, Chonggang; Yi, Lingling; Ling, Guohui; Zhang, Yongdong                                                                     | 2022             | preprint        |                                                                                                                                                                                                     | http://arxiv.org/abs/2205.06532                                                                                                                                                                                                                                               |                                     |                                 |                                       | In recommender system, some feature directly affects whether an interaction would happen, making the happened interactions not necessarily indicate user preference. For instance, short videos are objectively easier to be finished even though the user does not like the video. We term such feature as confounding feature, and video length is a confounding feature in video recommendation. If we fit a model on such interaction data, just as done by most data-driven recommender systems, the model will be biased to recommend short videos more, and deviate from user actual requirement. This work formulates and addresses the problem from the causal perspective. Assuming there are some factors affecting both the confounding feature and other item features, e.g., the video creator, we find the confounding feature opens a backdoor path behind user item matching and introduces spurious correlation. To remove the effect of backdoor path, we propose a framework named Deconfounding Causal Recommendation (DCR), which performs intervened inference with do-calculus. Nevertheless, evaluating do calculus requires to sum over the prediction on all possible values of confounding feature, significantly increasing the time cost. To address the efficiency challenge, we further propose a mixture-of experts (MoE) model architecture, modeling each value of confounding feature with a separate expert module. Through this way, we retain the model expressiveness with few additional costs. We demonstrate DCR on the backbone model of neural factorization machine (NFM), showing that DCR leads to more accurate prediction of user preference with small inference time cost. 在推荐系统中，某些特征会直接影响互动是否会发生，因此发生的互动并不一定代表用户的偏好。例如，短视频客观上更容易被看完，即使用户并不喜欢该视频。我们把这种特征称为混杂特征，视频长度就是视频推荐中的一种混杂特征。如果我们像大多数数据驱动的推荐系统那样，在这种交互数据上拟合一个模型，该模型就会偏向于推荐更多的短视频，从而偏离用户的实际需求。这项工作从因果关系的角度提出并解决了这个问题。假设混杂特征和其他项目特征（如视频创作者）都受到某些因素的影响，我们发现混杂特征在用户项目匹配背后打开了一条后门路径，并引入了虚假相关性。为了消除后门路径的影响，我们提出了一个名为 “去混淆因果推荐”（DCR）的框架，利用 do 微积分进行干预推理。然而，评估 do 微积分需要对混杂特征的所有可能值进行预测求和，这大大增加了时间成本。为了解决效率难题，我们进一步提出了专家混合（MoE）模型架构，用单独的专家模块对每个混杂特征值进行建模。通过这种方式，我们在保留模型表现力的同时，只需付出很少的额外成本。我们在神经因数分解机（NFM）的骨干模型上演示了 DCR，结果表明 DCR 能够以较小的推理时间成本更准确地预测用户偏好。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | 2022-08-26                                                                                                                                                                              | 2024-10-29 01:18:59                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:19:56                                                                                                                                  | 2024-10-29 01:18:59                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                  | ccfInfo: CCF-A  TOIS; citationNumber: 20            | /unread; Computer Science - Information Retrieval                                                                                                                                                                                                                                                                                                                                                                                      |
| Low-rank Defenses Against Adversarial Attacks in Recommender Systems                                                                                            | Entezari, Negin; Papalexakis, Evangelos E.                                                                                                                              | 2022             | conferencePaper | 2022 IEEE International Conference on Big Data (Big Data)                                                                                                                                           | https://ieeexplore.ieee.org/document/10020712/?arnumber=10020712                                                                                                                                                                                                              |                                     |                                 | 10.1109/BigData55660.2022.10020712    | Recommender systems are powerful tools which touch on numerous aspects of everyday life, from shopping to consuming content, and beyond. However, as other machine learning models, recommender system models are vulnerable to adversarial attacks and their performance could drop significantly with a slight modification of the input data. Most of the studies in the area of adversarial machine learning are focused on the image and vision domain. There are very few work that study adversarial attacks on recommender systems and even fewer work that study ways to make the recommender systems robust and reliable. In this study, we explore two state-of-the-art adversarial attack methods proposed by Tang et al. [1] and Christakopoulou et al. [2] and we report our proposed defenses and experimental evaluations against these attacks. In particular, we observe that low-rank reconstructions and/or transformation of the attacked data has a significant alleviating effect on the attack, and we present extensive experimental evidence to demonstrate the effectiveness of this approach. We also show that a simple classifier is able to learn to detect fake users from real users and can successfully discard them from the dataset. This observation elaborates the fact that the threat model does not generate fake users that mimic the same behavior of real users and can be easily distinguished from real users’ behavior. We also examine how transforming latent factors of the matrix factorization model into a low-dimensional space impacts its performance. Furthermore, we combine fake users from both attacks to examine how our proposed defense is able to defend against multiple attacks at the same time. Local low-rank reconstruction was able to reduce the hit ratio of target items from 23.54% to 15.69% while the overall performance of the recommender system was preserved.Adversarial machine learning, recommender systems, low-rank reconstruction 推荐系统是一种功能强大的工具，涉及日常生活的方方面面，从购物到内容消费等等。然而，与其他机器学习模型一样，推荐系统模型也容易受到对抗性攻击，只要对输入数据稍加修改，其性能就会大幅下降。对抗式机器学习领域的大部分研究都集中在图像和视觉领域。研究针对推荐系统的对抗性攻击的工作很少，而研究如何使推荐系统变得稳健可靠的工作就更少了。在本研究中，我们探讨了 Tang 等人[1] 和 Christakopoulou 等人[2] 提出的两种最先进的对抗性攻击方法，并报告了我们针对这些攻击提出的防御措施和实验评估。特别是，我们观察到被攻击数据的低秩重构和/或转换对攻击有显著的缓解作用，我们提出了大量实验证据来证明这种方法的有效性。我们还表明，一个简单的分类器能够学会从真实用户中检测出虚假用户，并能成功地将其从数据集中剔除。这一观察结果说明了一个事实，即威胁模型不会生成模仿真实用户相同行为的虚假用户，而且可以很容易地与真实用户的行为区分开来。我们还研究了将矩阵因式分解模型的潜在因素转换到低维空间对其性能的影响。此外，我们还将两种攻击中的虚假用户结合起来，研究我们提出的防御方法如何能够同时抵御多种攻击。局部低阶重构能够将目标项目的命中率从23.54%降低到15.69%，同时保持了推荐系统的整体性能。 对抗性机器学习、推荐系统、低阶重构                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | 2022-12                                                                                                                                                                                 | 2024-10-28 10:03:51                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:07                                                                                                                                  | 2024-10-28 10:03:51                                                                                                                                                                                                                                                                                                                                 | 5708-5714                                                                                                                                                                                        | ccfInfo: CCF-C; citationNumber: 0                   | /unread; Behavioral sciences; Data models; Reliability; Adversarial machine learning; Big Data; Resists; Threat modeling                                                                                                                                                                                                                                                                                                               |
| Towards Robust Recommender Systems via Triple Cooperative Defense                                                                                               | Wang, Qingyang; Lian, Defu; Wu, Chenwang; Chen, Enhong                                                                                                                  | 2022             | preprint        |                                                                                                                                                                                                     | http://arxiv.org/abs/2210.13762                                                                                                                                                                                                                                               |                                     |                                 |                                       | Recommender systems are often susceptible to well-crafted fake profiles, leading to biased recommendations. The wide application of recommender systems makes studying the defense against attack necessary. Among existing defense methods, data-processing-based methods inevitably exclude normal samples, while model-based methods struggle to enjoy both generalization and robustness. Considering the above limitations, we suggest integrating data processing and robust model and propose a general framework, Triple Cooperative Defense (TCD), which cooperates to improve model robustness through the co-training of three models. Specifically, in each round of training, we sequentially use the high-confidence prediction ratings (consistent ratings) of any two models as auxiliary training data for the remaining model, and the three models cooperatively improve recommendation robustness. Notably, TCD adds pseudo label data instead of deleting abnormal data, which avoids the cleaning of normal data, and the cooperative training of the three models is also beneficial to model generalization. Through extensive experiments with five poisoning attacks on three real-world datasets, the results show that the robustness improvement of TCD significantly outperforms baselines. It is worth mentioning that TCD is also beneficial for model generalizations. 推荐系统往往容易受到精心制作的虚假资料的影响，从而导致有偏见的推荐。推荐系统的广泛应用使得研究如何防御攻击成为必要。在现有的防御方法中，基于数据处理的方法不可避免地会将正常样本排除在外，而基于模型的方法则难以兼顾泛化和鲁棒性。考虑到上述局限性，我们建议整合数据处理和鲁棒模型，并提出了一个通用框架--三重协同防御（TCD），通过三个模型的协同训练来提高模型的鲁棒性。具体来说，在每一轮训练中，我们依次使用任意两个模型的高置信度预测评级（一致评级）作为其余模型的辅助训练数据，三个模型共同提高推荐鲁棒性。值得注意的是，TCD 增加了伪标签数据而不是删除异常数据，这避免了对正常数据的清理，三个模型的协同训练也有利于模型泛化。通过在三个真实数据集上对五种中毒攻击进行大量实验，结果表明 TCD 的鲁棒性改进明显优于基线。值得一提的是，TCD 还有利于模型泛化。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | 2022-10-25                                                                                                                                                                              | 2024-10-28 10:02:32                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:18                                                                                                                                  | 2024-10-28 10:02:32                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                  | citationNumber: 1; ccfInfo: CCF-C WISE              | /unread; Computer Science - Social and Information Networks; Computer Science - Cryptography and Security; Computer Science - Machine Learning                                                                                                                                                                                                                                                                                         |
| Detecting shilling groups in online recommender systems based on graph convolutional network                                                                    | Wang, Shilei; Zhang, Peng; Wang, Hui; Yu, Hongtao; Zhang, Fuzhi                                                                                                         | 2022             | journalArticle  | Information Processing & Management                                                                                                                                                                 | https://www.sciencedirect.com/science/article/pii/S0306457322001418                                                                                                                                                                                                           |                                     | 0306-4573                       | 10.1016/j.ipm.2022.103031             | Online recommender systems have been shown to be vulnerable to group shilling attacks in which attackers of a shilling group collaboratively inject fake profiles with the aim of increasing or decreasing the frequency that particular items are recommended. Existing detection methods mainly use the frequent itemset (dense subgraph) mining or clustering method to generate candidate groups and then utilize the hand-crafted features to identify shilling groups. However, such two-stage detection methods have two limitations. On the one hand, due to the sensitivity of support threshold or clustering parameters setting, it is difficult to guarantee the quality of candidate groups generated. On the other hand, they all rely on manual feature engineering to extract detection features, which is costly and time-consuming. To address these two limitations, we present a shilling group detection method based on graph convolutional network. First, we model the given dataset as a graph by treating users as nodes and co-rating relations between users as edges. By assigning edge weights and filtering normal user relations, we obtain the suspicious user relation graph. Second, we use principal component analysis to refine the rating features of users and obtain the user feature matrix. Third, we design a three-layer graph convolutional network model with a neighbor filtering mechanism and perform user classification by combining both structure and rating features of users. Finally, we detect shilling groups through identifying target items rated by the attackers according to the user classification results. Extensive experiments show that the classification accuracy and detection performance (F1-measure) of the proposed method can reach 98.92% and 99.92% on the Netflix dataset and 93.18% and 92.41% on the Amazon dataset. 在线推荐系统很容易受到托群攻击，托群攻击者会协同注入虚假资料，目的是增加或减少特定项目被推荐的频率。现有的检测方法主要使用频繁项集（密集子图）挖掘或聚类方法生成候选群组，然后利用手工创建的特征来识别托儿群组。然而，这种两阶段检测方法有两个局限性。一方面，由于支持阈值或聚类参数设置的敏感性，很难保证生成的候选群组的质量。另一方面，它们都依赖于人工特征工程来提取检测特征，成本高且耗时。针对这两个局限性，我们提出了一种基于图卷积网络的托群检测方法。首先，我们将用户视为节点，将用户之间的共同评级关系视为边，从而将给定数据集建模为一个图。通过分配边的权重和过滤正常的用户关系，我们得到了可疑用户关系图。其次，我们使用主成分分析法细化用户评级特征，得到用户特征矩阵。第三，我们设计了一个具有邻居过滤机制的三层图卷积网络模型，并结合用户的结构和评分特征进行用户分类。最后，我们根据用户分类结果，通过识别攻击者评级的目标项目来检测托群。大量实验表明，所提方法在 Netflix 数据集上的分类准确率和检测性能（F1-measure）分别达到 98.92% 和 99.92%，在亚马逊数据集上的分类准确率和检测性能（F1-measure）分别达到 93.18% 和 92.41%。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | 2022-09-01                                                                                                                                                                              | 2024-10-25 05:45:24                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:26                                                                                                                                  | 2024-10-25 05:45:24                                                                                                                                                                                                                                                                                                                                 | 103031                                                                                                                                                                                           | citationNumber: 7; ccfInfo: CCF-B IPM               | /unread; Recommender systems; Group shilling attacks; Graph convolutional network; Target item identification                                                                                                                                                                                                                                                                                                                          |
| Graph Embedding for Recommendation against Attribute Inference Attacks                                                                                          | Zhang, Shijie; Yin, Hongzhi; Chen, Tong; Huang, Zi; Cui, Lizhen; Zhang, Xiangliang                                                                                      | 2021             | preprint        |                                                                                                                                                                                                     | http://arxiv.org/abs/2101.12549                                                                                                                                                                                                                                               |                                     |                                 |                                       | In recent years, recommender systems play a pivotal role in helping users identify the most suitable items that satisfy personal preferences. As user-item interactions can be naturally modelled as graph-structured data, variants of graph convolutional networks (GCNs) have become a well-established building block in the latest recommenders. Due to the wide utilization of sensitive user profile data, existing recommendation paradigms are likely to expose users to the threat of privacy breach, and GCN-based recommenders are no exception. Apart from the leakage of raw user data, the fragility of current recommenders under inference attacks offers malicious attackers a backdoor to estimate users' private attributes via their behavioral footprints and the recommendation results. However, little attention has been paid to developing recommender systems that can defend such attribute inference attacks, and existing works achieve attack resistance by either sacrificing considerable recommendation accuracy or only covering specific attack models or protected information. In our paper, we propose GERAI, a novel differentially private graph convolutional network to address such limitations. Specifically, in GERAI, we bind the information perturbation mechanism in differential privacy with the recommendation capability of graph convolutional networks. Furthermore, based on local differential privacy and functional mechanism, we innovatively devise a dual-stage encryption paradigm to simultaneously enforce privacy guarantee on users' sensitive features and the model optimization process. Extensive experiments show the superiority of GERAI in terms of its resistance to attribute inference attacks and recommendation effectiveness. 近年来，推荐系统在帮助用户识别满足个人偏好的最合适项目方面发挥着举足轻重的作用。由于用户与物品之间的交互可以自然地建模为图结构数据，图卷积网络（GCN）的变体已成为最新推荐器的成熟构建模块。由于敏感的用户资料数据被广泛使用，现有的推荐模式很可能会使用户面临隐私泄露的威胁，基于 GCN 的推荐程序也不例外。除了原始用户数据的泄露，当前推荐器在推理攻击下的脆弱性也为恶意攻击者提供了通过用户行为足迹和推荐结果估算用户隐私属性的后门。然而，人们很少关注开发能够防御这种属性推断攻击的推荐系统，现有的作品要么牺牲了相当高的推荐准确性，要么只覆盖了特定的攻击模型或受保护的信息，从而实现了抗攻击性。在本文中，我们提出了一种新颖的差异化私有图卷积网络 GERAI 来解决这些局限性。具体来说，在 GERAI 中，我们将差分隐私中的信息扰动机制与图卷积网络的推荐能力结合起来。此外，基于局部差分隐私和函数机制，我们创新性地设计了一种双阶段加密范式，可同时对用户的敏感特征和模型优化过程实施隐私保证。广泛的实验表明，GERAI 在抵御属性推理攻击和推荐有效性方面都具有优越性。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | 2021-01-29                                                                                                                                                                              | 2024-10-28 10:01:59                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:24                                                                                                                                  | 2024-10-28 10:01:59                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                  | ccfInfo: CCF-A WWW; citationNumber: 92              | /unread; Computer Science - Information Retrieval                                                                                                                                                                                                                                                                                                                                                                                      |
| Fusing hypergraph spectral features for shilling attack detection                                                                                               | Li, Hao; Gao, Min; Zhou, Fengtao; Wang, Yueyang; Fan, Qilin; Yang, Linda                                                                                                | 2021             | journalArticle  | Journal of Information Security and Applications                                                                                                                                                    | https://linkinghub.elsevier.com/retrieve/pii/S2214212621002374                                                                                                                                                                                                                |                                     | 22142126                        | 10.1016/j.jisa.2021.103051            | Recommender systems can effectively improve user experience, but they are vulnerable to shilling attacks due to their open nature. Attackers inject fake user profiles to destroy the security and reliability of the recommender systems. Therefore, it is crucial to detect shilling attacks effectively. The primitive detection models are feasible but costly because of the dependence on plenty of hand-engineered explicit features based on statistical measures. Even though the upgraded models based on learning embeddings of the implicit features are more general, they fail to take some distinct features in distinguishing fake users into consideration. Moreover, these primitive and upgraded models are difficult to capture the high order relationships between users and items as the models usually learn the embedding from the first-order interactions. The representation and similarity information learned from the first-order interactions are not comprehensive enough, limiting the detection task. To this end, we propose a novel shilling attack detection model by fusing hypergraph spectral features (SpDetector). The proposed model combines the explicit and implicit features to balance the effectiveness and generality and deal with the high order relationships by hypergraphs-based embedding. From the implicit perspective, SpDetector constructs user hypergraphs and item hypergraphs for the high-order relationships hidden in the interaction and extracts spectral features from hypergraphs to capture high-order similarity for users and items, respectively. From the explicit perspective, it extracts two kinds of explicit features: item similarity offsets (ISO) based on item spectral features and rating prediction errors (RPE), for all users as their distinct capability of distinguishing fake users. Finally, the SpDetector learns to distinguish fake users by training a deep neural network with those features. Experiments conducted on MovieLens and Amazon datasets show that SpDetector outperforms state-of-the-art detection models. 推荐系统可以有效改善用户体验，但由于其开放性，很容易受到托攻击。攻击者会注入虚假的用户资料，破坏推荐系统的安全性和可靠性。因此，有效检测托攻击至关重要。原始的检测模型是可行的，但成本高昂，因为它依赖于大量基于统计量的手工设计的明确特征。尽管基于学习隐含特征嵌入的升级模型更为通用，但它们在区分虚假用户时未能考虑到一些明显的特征。此外，这些原始模型和升级模型很难捕捉用户与物品之间的高阶关系，因为这些模型通常是从一阶交互中学习嵌入的。从一阶交互中学到的表示和相似性信息不够全面，限制了检测任务的完成。为此，我们通过融合超图谱特征（SpDetector）提出了一种新颖的托攻击检测模型。该模型结合了显式特征和隐式特征，以平衡有效性和通用性，并通过基于超图的嵌入处理高阶关系。从隐性角度来看，SpDetector 为交互中隐藏的高阶关系构建用户超图和项目超图，并从超图中提取频谱特征，分别捕捉用户和项目的高阶相似性。从显性角度来看，它提取了两种显性特征：基于项目光谱特征的项目相似性偏移（ISO）和针对所有用户的评分预测误差（RPE），作为区分虚假用户的独特能力。最后，SpDetector 通过使用这些特征训练深度神经网络来学习如何区分虚假用户。在 MovieLens 和亚马逊数据集上进行的实验表明，SpDetector 优于最先进的检测模型。                                                                                                                                                                                                                                                                                                                                                                                                                             | 2021-12                                                                                                                                                                                 | 2024-10-28 10:01:44                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:23                                                                                                                                  | 2024-10-28 10:01:44                                                                                                                                                                                                                                                                                                                                 | 103051                                                                                                                                                                                           | citationNumber: 8; ccfInfo: CCF-C JISA              | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Semi-supervised recommendation attack detection based on Co-Forest                                                                                              | Zhou, Quanqiang; Duan, Liangliang                                                                                                                                       | 2021             | journalArticle  | Computers & Security                                                                                                                                                                                | https://linkinghub.elsevier.com/retrieve/pii/S0167404821002145                                                                                                                                                                                                                |                                     | 01674048                        | 10.1016/j.cose.2021.102390            | In recommendation attack, malicious users attempt to bias the recommendation results by injecting fake profiles into the rating database. To detect such attack, three types of methods, i.e., unsupervised, supervised and semi-supervised, have been proposed. Among these works, the advantage of semi-supervised methods is that they can use the unlabeled user profiles to improve the detection performance. However, the existing semi-supervised methods suffer from low precision. Aiming at this problem, in this paper, we propose a semisupervised detection approach named SSADR-CoF based on the Co-Forest algorithm. Being different from the existing semi-supervised methods which only use a few of features to train a single classifier for the detection, the proposed approach uses a series of features to train an ensemble of classifiers to detect the recommendation attack. We first use the window dividing and rating behavior statistical methods to extract a series of user rating behavior mode features for training the detection model. Then, we use a small number of labeled user profiles to initialize an ensemble of classifiers, and use the ensemble of classifiers to assign labels to the unlabeled user profiles. Finally, we use the labeled and the newly labeled user profiles to iteratively update the classifiers for the detection. Experiments conducted on three benchmark datasets MovieLens 10M, MovieLens 25M, and Amazon show that the proposed approach can effectively improve the precision of the semi-supervised methods under the condition of maintaining high recall and AUC. 在推荐攻击中，恶意用户试图通过向评级数据库注入虚假资料来使推荐结果产生偏差。为了检测这种攻击，人们提出了三种方法，即无监督、有监督和半监督方法。在这些方法中，半监督方法的优势在于可以利用未标记的用户资料来提高检测性能。然而，现有的半监督方法存在精度低的问题。针对这一问题，本文提出了一种基于 Co-Forest 算法的半监督检测方法，名为 SSADR-CoF。不同于现有的半监督方法只使用几个特征来训练一个分类器进行检测，本文提出的方法使用一系列特征来训练一个分类器集合来检测推荐攻击。我们首先使用窗口分割和评分行为统计方法提取一系列用户评分行为模式特征，用于训练检测模型。然后，我们使用少量有标签的用户配置文件来初始化分类器集合，并使用分类器集合为无标签的用户配置文件分配标签。最后，我们使用已标记和新标记的用户配置文件迭代更新分类器，以进行检测。在 MovieLens 10M、MovieLens 25M 和 Amazon 三个基准数据集上进行的实验表明，在保持高召回率和 AUC 的条件下，所提出的方法能有效提高半监督方法的精度。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | 2021-10                                                                                                                                                                                 | 2024-10-28 10:01:14                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:15                                                                                                                                  | 2024-10-28 10:01:14                                                                                                                                                                                                                                                                                                                                 | 102390                                                                                                                                                                                           | ccfInfo: CCF-B; citationNumber: 9                   | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Large-scale Fake Click Detection for E-commerce Recommendation Systems                                                                                          | Li, Jingdong; Li, Zhao; Huang, Jiaming; Zhang, Ji; Wang, Xiaoling; Lu, Xingjian; Zhou, Jingren                                                                          | 2021             | conferencePaper | 2021 IEEE 37th International Conference on Data Engineering (ICDE)                                                                                                                                  | https://ieeexplore.ieee.org/document/9458938/?arnumber=9458938                                                                                                                                                                                                                |                                     |                                 | 10.1109/ICDE51399.2021.00290          | "With the development of e-commerce platforms                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | e-commerce recommendation systems are playing an increasingly important role for the purpose of product recommendation. As a new attack model against e-commerce recommendation systems | the ""Ride Item's Coattails"" attack creates fake click information to establish the deceptive correlation between popular products and low-quality products in order to mislead the recommendation system of e-commerce platform to boost the sales of low-quality products. This attack is characterized by high concealment and strong destructiveness | which can cause great damage to e-commerce recommendation systems                                                                                    | and adversely affect the usability of the e-commerce platform and users' shopping experience. It is therefore of great practical significance to study how to quickly and effectively identify the false click information and the corresponding ""Ride Item's Coattails"" attack to better safeguard e-commerce recommendation systems. At present | there is no previously reported relevant research work conducted specifically for addressing the detection of the ""Ride Item's Coattails"" attack. In this work                                 | IEEE Xplore                                         |                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| Adversarial Attacks to API Recommender Systems: Time to Wake Up and Smell the Coffee?                                                                           | Nguyen, Phuong T.; Di Sipio, Claudio; Di Rocco, Juri; Di Penta, Massimiliano; Di Ruscio, Davide                                                                         | 2021             | conferencePaper | 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)                                                                                                                 | https://ieeexplore.ieee.org/document/9678946/?arnumber=9678946                                                                                                                                                                                                                |                                     |                                 | 10.1109/ASE51524.2021.9678946         | Recommender systems in software engineering provide developers with a wide range of valuable items to help them complete their tasks. Among others, API recommender systems have gained momentum in recent years as they became more successful at suggesting API calls or code snippets. While these systems have proven to be effective in terms of prediction accuracy, there has been less attention for what concerns such recommenders’ resilience against adversarial attempts. In fact, by crafting the recommenders’ learning material, e.g., data from large open-source software (OSS) repositories, hostile users may succeed in injecting malicious data, putting at risk the software clients adopting API recommender systems. In this paper, we present an empirical investigation of adversarial machine learning techniques and their possible influence on recommender systems. The evaluation performed on three state-of-the-art API recommender systems reveals a worrying outcome: all of them are not immune to malicious data. The obtained result triggers the need for effective countermeasures to protect recommender systems against hostile attacks disguised in training data. 软件工程中的推荐系统为开发人员提供了大量有价值的项目，帮助他们完成任务。其中，应用程序接口推荐系统近年来在推荐应用程序接口调用或代码片段方面越来越成功，因此获得了巨大的发展。虽然这些系统在预测准确性方面已被证明是有效的，但人们对此类推荐器抵御对抗性尝试的能力关注较少。事实上，通过精心设计推荐器的学习材料，例如来自大型开源软件（OSS）库的数据，敌对用户可能会成功注入恶意数据，从而使采用 API 推荐系统的软件客户端面临风险。在本文中，我们对对抗性机器学习技术及其对推荐系统可能产生的影响进行了实证调查。对三种最先进的应用程序接口推荐系统进行的评估揭示了一个令人担忧的结果：所有这些系统对恶意数据都无法免疫。这一结果促使人们需要采取有效的应对措施，以保护推荐系统免受伪装成训练数据的恶意攻击。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | 2021-11                                                                                                                                                                                 | 2024-10-28 09:59:43                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:19:57                                                                                                                                  | 2024-10-28 09:59:43                                                                                                                                                                                                                                                                                                                                 | 253-265                                                                                                                                                                                          | citationNumber: Not Found; ccfInfo: CCF-A ASE       | /unread; Recommender systems; Task analysis; Adversarial machine learning; Adversarial Attacks; Adversarial Machine Learning; API Mining; Codes; Open source software; Software engineering; Training data                                                                                                                                                                                                                             |
| Fight Fire with Fire: Towards Robust Recommender Systems via Adversarial Poisoning Training                                                                     | Wu, Chenwang; Lian, Defu; Ge, Yong; Zhu, Zhihao; Chen, Enhong; Yuan, Senchao                                                                                            | 2021             | conferencePaper | Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval                                                                                     | https://doi.org/10.1145/3404835.3462914                                                                                                                                                                                                                                       | 978-1-4503-8037-9                   |                                 | 10.1145/3404835.3462914               | "Recent studies have shown that recommender systems are vulnerable                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | and it is easy for attackers to inject well-designed malicious profiles into the system                                                                                                 | leading to biased recommendations. We cannot deny these data's rationality                                                                                                                                                                                                                                                                                | making it imperative to establish a robust recommender system. Adversarial training has been extensively studied for robust recommendations. However | traditional adversarial training adds small perturbations to the parameters (inputs)                                                                                                                                                                                                                                                                | which do not comply with the poisoning mechanism in the recommender system. Thus for the practical models that are very good at learning existing data                                           | New York, NY, USA                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| Hybrid convolutional neural network (CNN) and long-short term memory (LSTM) based deep learning model for detecting shilling attack in the social-aware network | Vivekanandan, K.; Praveena, N.                                                                                                                                          | 2021             | journalArticle  | Journal of Ambient Intelligence and Humanized Computing                                                                                                                                             | https://doi.org/10.1007/s12652-020-02164-y                                                                                                                                                                                                                                    |                                     | 1868-5145                       | 10.1007/s12652-020-02164-y            | In social aware network (SAN) paradigm, the fundamental activities concentrate on exploring the behavior and attributes of the users. This investigation of user characteristic aids in the design of highly efficient and suitable protocols. In particular, the shilling attack introduces a high degree of vulnerability into the recommender systems. The shilling attackers use the reviews, user ratings and forged user generated content data for the computation of recommendation rankings. The detection of shilling attack in recommender systems is considered to be essential for sustaining their fairness and reliability. In specific, the collaborative filtering strategies utilized for detecting shilling attackers through efficient user behavior mining are considered as the predominant methodologies in the literature. In this paper, a hybrid convolutional neural network (CNN) and long-short term memory (LSTM)-based deep learning model (CNN–LSTM) is proposed for detecting shilling attack in recommender systems. This deep learning model utilizes the transformed network architecture for exploiting the deep-level attributes derived from user rated profiles. It overcomes the limitations of the existing shilling attack detection methods which mostly focuses on identifying spam users by designing features artificially in order to enhance their efficiency and robustness. It is also potent in elucidating deep-level features for efficiently detecting shilling attacks by accurately elaborating the user ratings. The experimental results confirmed the significance of the proposed CNN–LSTM approach by accurately detecting most of the obfuscated attacks compared to the state-of-art algorithms used for investigation. 在社会感知网络（SAN）范例中，基本活动集中于探索用户的行为和属性。对用户特征的调查有助于设计高效、合适的协议。特别是，“托 ”攻击给推荐系统带来了高度的脆弱性。托客攻击利用评论、用户评级和伪造的用户生成内容数据来计算推荐排名。检测推荐系统中的 “托 ”攻击被认为是维持推荐系统公平性和可靠性的关键。具体来说，通过有效的用户行为挖掘来检测 “托 ”攻击的协同过滤策略被认为是文献中的主流方法。本文提出了一种基于卷积神经网络（CNN）和长短期记忆（LSTM）的混合深度学习模型（CNN-LSTM），用于检测推荐系统中的托攻击。该深度学习模型利用转换后的网络架构来利用从用户评级档案中获得的深层次属性。它克服了现有托儿攻击检测方法的局限性，这些方法大多侧重于通过人为设计特征来识别垃圾用户，以提高其效率和鲁棒性。它还通过准确阐述用户评分，有力地阐明了有效检测托儿攻击的深层次特征。实验结果证实了所提出的 CNN-LSTM 方法的重要性，与用于调查的最先进算法相比，它能准确地检测出大多数混淆攻击。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 2021-01-01                                                                                                                                                                              | 2024-10-28 10:00:35                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:25                                                                                                                                  | 2024-10-28 10:00:35                                                                                                                                                                                                                                                                                                                                 | 1197-1210                                                                                                                                                                                        | citationNumber: 22; ccfInfo: CCF-None JAIHC         | /unread; Recommender systems; Artificial Intelligence; Shilling attack; Convolutional neural network (CNN); Deep learning model; Long-short term memory (LSTM); User profiles; User ratings                                                                                                                                                                                                                                            |
| Identification of Malicious Injection Attacks in Dense Rating and Co-Visitation Behaviors                                                                       | Yang, Zhihai; Sun, Qindong; Zhang, Yaling; Wang, Wei                                                                                                                    | 2021             | journalArticle  | IEEE Transactions on Information Forensics and Security                                                                                                                                             | https://ieeexplore.ieee.org/document/9167299/?arnumber=9167299                                                                                                                                                                                                                |                                     | 1556-6021                       | 10.1109/TIFS.2020.3016827             | Personalized recommender systems are pervasive in different domains, ranging from e-commerce services, financial transaction systems to social networks. The generated ratings and reviews by users toward products are not only favourable to make targeted improvements on the products for online businesses, but also beneficial for other users to get a more insightful review of the products. In reality, recommender systems can also be deliberately manipulated by malicious users due to their fundamental vulnerabilities and openness. However, improving the detection performance for defending malicious threats including profile injection attacks and co-visitation injection attacks is constrained by the challenging issues: (1) various types of malicious attacks in real-world data coexist; (2) it is difficult to balance the commonality and speciality of rating behaviors in terms of accurate detection; and (3) rating behaviors between attackers and anchor users caused by the consistency of attack intentions are extremely similar. In this article, we develop a unified detection approach named IMIA-HCRF, to progressively discriminate malicious injection behaviors for recommender systems. First, disturbed data are empirically eliminated by implementing both the construction of association graph and enhancement of dense behaviors, which can be adapted to different attacks. Then, the smooth boundary of dense rating (or co-visitation) behaviors is further segmented using higher order potentials, which is finally leveraged to determine the concerned injection behaviors. Extensive experiments on both synthetic data and real-world data demonstrate that the proposed IMIA-HCRF outperforms all baselines on various metrics. The detection performance of IMIA-HCRF can achieve an improvement of 7.8% for mixed profile injection attacks as well as 6% for mixed co-visitation injection attacks over the baselines in terms of FAR (false alarm rate) while keeping the highest DR (detection rate). Additional experiments on real-world data show that IMIA-HCRF brings an improvement with the advantage of 11.5% FAR in average compared with the baselines. 从电子商务服务、金融交易系统到社交网络，个性化推荐系统普遍存在于不同领域。用户对产品生成的评分和评论不仅有利于在线企业对产品进行有针对性的改进，也有利于其他用户对产品进行更深入的评论。在现实中，由于推荐系统的基本漏洞和开放性，它也可能被恶意用户蓄意操纵。然而，要提高防御恶意威胁（包括配置文件注入攻击和共同访问注入攻击）的检测性能，却受到以下挑战性问题的制约：（1）现实世界数据中各种类型的恶意攻击并存；（2）在精确检测方面难以平衡评价行为的共性和特殊性；（3）攻击者和主播用户之间因攻击意图的一致性而导致的评价行为极其相似。在本文中，我们开发了一种名为 IMIA-HCRF 的统一检测方法，以逐步区分推荐系统中的恶意注入行为。首先，通过关联图的构建和密集行为的增强，根据经验剔除干扰数据，从而适应不同的攻击行为。然后，利用高阶电位进一步分割密集评级（或共同访问）行为的平滑边界，最后利用这些边界确定相关的注入行为。在合成数据和真实世界数据上进行的大量实验表明，所提出的 IMIA-HCRF 在各种指标上都优于所有基线。就误报率（FAR）而言，IMIA-HCRF 对混合轮廓注入攻击的检测性能比基线方法提高了 7.8%，对混合共视注入攻击的检测性能比基线方法提高了 6%，同时保持了最高的 DR（检测率）。在真实世界数据上进行的其他实验表明，IMIA-HCRF 与基线相比平均提高了 11.5% 的误报率。                                                                                                                                                                                                                                                                                   | 2021                                                                                                                                                                                    | 2024-10-28 09:56:33                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:25                                                                                                                                  | 2024-10-28 09:56:33                                                                                                                                                                                                                                                                                                                                 | 537-552                                                                                                                                                                                          | citationNumber: 10; ccfInfo: CCF-A TIFS             | /unread; Recommender systems; Social network services; recommender system; Feature extraction; Attack detection; Cathode ray tubes; behavior representation; Sun; Business; information security; Security                                                                                                                                                                                                                             |
| MSPLD: Shilling Attack Detection Model Based on Meta Self-Paced Learning                                                                                        | Yang, Yanjing; Gao, Min; Li, Yuerang; Wu, Fan; Wang, Jia; Zhao, Quanwu                                                                                                  | 2021             | conferencePaper | 2021 International Joint Conference on Neural Networks (IJCNN)                                                                                                                                      | https://ieeexplore.ieee.org/document/9533622/?arnumber=9533622                                                                                                                                                                                                                |                                     |                                 | 10.1109/IJCNN52387.2021.9533622       | With the dramatic rise of recommendation systems, more and more companies use them to improve users' experience. However, the openness of the recommender systems makes them vulnerable to shilling attacks, which causes a bad impact on user satisfaction. Existing shilling attack detection models usually have problems in solving the noise of samples and labels. To this end, this paper proposes a shilling attack detection model based on meta self-paced learning, named MSPLD. Meta self-paced learning can make the model select samples from easy to difficult in the learning process, which can alleviate the problem that the model parameters are difficult to optimize due to the outliers or noises in the samples. Specifically, MSPLD adopts some methods to get the extraction of potential feature embedding vectors first. Second, metadata is selected adaptively by a regression method. Then, it uses the embedding vectors of malicious users and metadata as input data. Third, it optimizes the age loss function and the loss function of the classifier itself with bilateral optimization. The relationship between age and sample loss of the classifier will determine the weight of sample selection. Finally, using the tendency of the age gradually to select training samples from easy to difficult can improve the generalization ability of the models. Compared with the state-of-the-art detection models, the experimental results on two public datasets show that MSPLD can achieve better detection performance. Besides, we illustrate the training process of MSPLD to analyze the reason for the superiority of the model. 随着推荐系统的急剧兴起，越来越多的公司使用推荐系统来改善用户体验。然而，推荐系统的开放性使其很容易受到 “托”（shilling）攻击，从而对用户满意度造成恶劣影响。现有的托攻击检测模型通常在解决样本和标签的噪声方面存在问题。为此，本文提出了一种基于元自步调学习的托攻击检测模型，命名为 MSPLD。元自步调学习可以使模型在学习过程中从易到难选择样本，从而缓解由于样本中的异常值或噪声导致模型参数难以优化的问题。具体来说，MSPLD 采用了一些方法，首先提取潜在的特征嵌入向量。其次，通过回归方法自适应地选择元数据。然后，将恶意用户和元数据的嵌入向量作为输入数据。第三，用双边优化法优化年龄损失函数和分类器本身的损失函数。年龄与分类器样本损失之间的关系将决定样本选择的权重。最后，利用年龄逐渐增大的趋势，由易到难地选择训练样本，可以提高模型的泛化能力。与最先进的检测模型相比，在两个公共数据集上的实验结果表明，MSPLD 可以获得更好的检测性能。此外，我们还对 MSPLD 的训练过程进行了说明，以分析其优越性的原因。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | 2021-07                                                                                                                                                                                 | 2024-10-25 05:43:15                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:27                                                                                                                                  | 2024-10-25 05:43:15                                                                                                                                                                                                                                                                                                                                 | 1-8                                                                                                                                                                                              | citationNumber: 0; ccfInfo: CCF-C IJCNN             | /unread; Analytical models; Neural networks; Feature extraction; Training; Adaptation models; meta self-paced learning; Metadata; Reliability theory; shilling attack detection; visual theoretical reliability analysis                                                                                                                                                                                                               |
| An unsupervised detection method for shilling attacks based on deep learning and community detection                                                            | Hao, Yaojun; Zhang, Fuzhi                                                                                                                                               | 2021             | journalArticle  | Soft Computing                                                                                                                                                                                      | https://doi.org/10.1007/s00500-020-05162-6                                                                                                                                                                                                                                    |                                     | 1433-7479                       | 10.1007/s00500-020-05162-6            | In the detection methods for shilling attacks, the supervised methods require labeled samples to train the classifiers. Due to lack of the labeled sample profiles in real scenarios, the applicability of supervised detection method is restricted. For unsupervised methods, the prior knowledge is often required to guarantee the detection accuracy. To break the traditional limitations, we present an unsupervised method to detect various shilling profiles from reconstructed user–user graph based on deep learning and community detection. Firstly, we construct the user–user graph, whose edge weight is calculated by the similarity of user’s behaviors. Secondly, the stacked denoising autoencoders are used to extract the robust graph features at different scales with different corruption rates. Based on the features at different scales, we generate multiple clustering results and reconstruct the user–user graph by evidence accumulation method. Thirdly, the community detection is carried out by using the persistence optimization algorithm. Extensive experiments on two datasets illustrate that our proposed method has better performance than some baseline detectors for detecting the simulated attacks and actual attacks. 在托攻击的检测方法中，有监督方法需要标注样本来训练分类器。由于缺乏真实场景中的标注样本资料，有监督检测方法的适用性受到了限制。对于无监督方法，通常需要先验知识来保证检测精度。为了打破传统的限制，我们提出了一种基于深度学习和社群检测的无监督方法，从重构的用户-用户图中检测各种托概况。首先，我们构建用户-用户图，根据用户行为的相似性计算图边权重。其次，利用堆叠去噪自动编码器提取不同尺度、不同损坏率的鲁棒图特征。根据不同尺度的特征，我们生成多个聚类结果，并通过证据积累法重建用户-用户图。第三，利用持久性优化算法进行社群检测。在两个数据集上进行的大量实验表明，在检测模拟攻击和实际攻击方面，我们提出的方法比一些基准检测器具有更好的性能。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | 2021-01-01                                                                                                                                                                              | 2024-10-25 05:44:18                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:21                                                                                                                                  | 2024-10-25 05:44:18                                                                                                                                                                                                                                                                                                                                 | 477-494                                                                                                                                                                                          | ccfInfo: CCF-C; citationNumber: 13                  | /unread; Artificial Intelligence; Shilling attack; Deep learning; Community detection; Unsupervised detection                                                                                                                                                                                                                                                                                                                          |
| GCN-Based User Representation Learning for Unifying Robust Recommendation and Fraudster Detection                                                               | Zhang, Shijie; Yin, Hongzhi; Chen, Tong; Hung, Quoc Viet Nguyen; Huang, Zi; Cui, Lizhen                                                                                 | 2020             | preprint        |                                                                                                                                                                                                     | http://arxiv.org/abs/2005.10150                                                                                                                                                                                                                                               |                                     |                                 |                                       | In recent years, recommender system has become an indispensable function in all e-commerce platforms. The review rating data for a recommender system typically comes from open platforms, which may attract a group of malicious users to deliberately insert fake feedback in an attempt to bias the recommender system to their favour. The presence of such attacks may violate modeling assumptions that high-quality data is always available and these data truly reflect users' interests and preferences. Therefore, it is of great practical significance to construct a robust recommender system that is able to generate stable recommendations even in the presence of shilling attacks. In this paper, we propose GraphRfi - a GCN-based user representation learning framework to perform robust recommendation and fraudster detection in a unified way. In its end-to-end learning process, the probability of a user being identified as a fraudster in the fraudster detection component automatically determines the contribution of this user's rating data in the recommendation component; while the prediction error outputted in the recommendation component acts as an important feature in the fraudster detection component. Thus, these two components can mutually enhance each other. Extensive experiments have been conducted and the experimental results show the superiority of our GraphRfi in the two tasks - robust rating prediction and fraudster detection. Furthermore, the proposed GraphRfi is validated to be more robust to the various types of shilling attacks over the state-of-the-art recommender systems. 近年来，推荐系统已成为所有电子商务平台不可或缺的功能。推荐系统的评论评级数据通常来自开放平台，这可能会吸引一批恶意用户故意插入虚假反馈，试图使推荐系统偏向他们的利益。这种攻击的存在可能会违反建模假设，即高质量的数据总是可用的，而且这些数据真实地反映了用户的兴趣和偏好。因此，构建一种即使在存在托攻击的情况下也能生成稳定推荐的鲁棒推荐系统具有重要的现实意义。在本文中，我们提出了基于 GCN 的用户表征学习框架 GraphRfi，以统一的方式执行稳健推荐和欺诈检测。在其端到端的学习过程中，欺诈检测组件中用户被识别为欺诈者的概率自动决定了该用户评级数据在推荐组件中的贡献；而推荐组件中输出的预测误差则成为欺诈检测组件中的重要特征。因此，这两个组件可以相互促进。我们进行了广泛的实验，实验结果表明，我们的 GraphRfi 在稳健评级预测和欺诈检测这两项任务中都具有优势。此外，经过验证，与最先进的推荐系统相比，所提出的 GraphRfi 对各种类型的托攻击具有更强的鲁棒性。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 2020-05-20                                                                                                                                                                              | 2024-10-29 01:01:06                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:23                                                                                                                                  | 2024-10-29 01:01:06                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                  | ccfInfo: CCF-A SIGIR; citationNumber: 197           | /unread; Computer Science - Social and Information Networks; Computer Science - Information Retrieval                                                                                                                                                                                                                                                                                                                                  |
| Sequential Attack Detection in Recommender Systems                                                                                                              | Aktukmak, Mehmet; Yilmaz, Yasin; Uysal, Ismail                                                                                                                          | 2021             | journalArticle  | IEEE Transactions on Information Forensics and Security                                                                                                                                             | https://ieeexplore.ieee.org/document/9417209/                                                                                                                                                                                                                                 |                                     | 1556-6013, 1556-6021            | 10.1109/TIFS.2021.3076295             | Recommender systems are widely used in electronic commerce, social media and online streaming services to provide personalized recommendations to the users by exploiting past ratings and interactions. This paper considers the security aspect with quick and accurate detection of attacks by observing the newly created proﬁles sequentially to prevent the damage which may be incurred by the injection of new proﬁles with dishonest ratings. The proposed framework consists of a latent variable model, which is trained by a variational EM algorithm, followed by a sequential detection algorithm. The latent variable model generates homogeneous representations of the users given their rating history and mixed data-type attributes such as age and gender. The representations are then exploited to generate univariate statistics to be efﬁciently used in a CUSUM-like sequential detection algorithm that can quickly detect persistent attacks while maintaining low false alarm rates. We apply our proposed framework to three different real-world datasets and exhibit superior performance in comparison to the existing baseline algorithms for both attack proﬁle and sequential detection. Furthermore, we demonstrate robustness to different attack strategies and conﬁgurations. 推荐系统广泛应用于电子商务、社交媒体和在线流媒体服务，通过利用用户过去的评分和互动，为用户提供个性化推荐。本文从安全方面考虑，通过依次观察新创建的商品来快速准确地检测攻击，以防止因注入带有不诚实评分的新商品而造成的损害。建议的框架由一个潜变量模型和一个顺序检测算法组成，前者通过变异 EM 算法进行训练。潜变量模型根据用户的评分历史和混合数据类型属性（如年龄和性别）生成用户的同质表征。然后，利用这些表征生成单变量统计数据，以便在类似 CUSUM 的顺序检测算法中高效使用，该算法可以快速检测出持续性攻击，同时保持较低的误报率。我们将所提出的框架应用于三个不同的真实数据集，与现有的基线算法相比，在攻击预测和顺序检测方面都表现出了卓越的性能。此外，我们还展示了对不同攻击策略和配置的鲁棒性。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | 2021                                                                                                                                                                                    | 2024-10-28 09:57:45                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:15                                                                                                                                  | 2024-10-28 09:57:45                                                                                                                                                                                                                                                                                                                                 | 3285-3298                                                                                                                                                                                        | citationNumber: 7; ccfInfo: CCF-A TIFS              | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| An Unsupervised Approach for Detecting Group Shilling Attacks in Recommender Systems Based on Topological Potential and Group Behaviour Features                | Cai, Hongyun; Zhang, Fuzhi; Jhaveri, Rutvij                                                                                                                             | 2021             | journalArticle  | Sec. and Commun. Netw.                                                                                                                                                                              | https://doi.org/10.1155/2021/2907691                                                                                                                                                                                                                                          |                                     | 1939-0114                       | 10.1155/2021/2907691                  | To protect recommender systems against shilling attacks, a variety of detection methods have been proposed over the past decade. However, these methods focus mainly on individual features and rarely consider the lockstep behaviours among attack users, which suffer from low precision in detecting group shilling attacks. In this work, we propose a three-stage detection method based on strong lockstep behaviours among group members and group behaviour features for detecting group shilling attacks. First, we construct a weighted user relationship graph by combining direct and indirect collusive degrees between users. Second, we find all dense subgraphs in the user relationship graph to generate a set of suspicious groups by introducing a topological potential method. Finally, we use a clustering method to detect shilling groups by extracting group behaviour features. Extensive experiments on the Netflix and sampled Amazon review datasets show that the proposed approach is effective for detecting group shilling attacks in recommender systems, and the F1-measure on two datasets can reach over 99 percent and 76 percent, respectively. 为了保护推荐系统免受托攻击，过去十年间提出了多种检测方法。然而，这些方法主要关注个体特征，很少考虑攻击用户之间的锁步行为，在检测群体托儿攻击时存在精度低的问题。在这项工作中，我们提出了一种基于群成员之间强锁定行为和群行为特征的三阶段检测方法，用于检测群托儿攻击。首先，我们结合用户之间的直接和间接串通程度构建加权用户关系图。其次，我们在用户关系图中找到所有密集子图，通过引入拓扑势方法生成一组可疑群组。最后，我们使用聚类方法，通过提取群体行为特征来检测托群。在 Netflix 和抽样亚马逊评论数据集上的大量实验表明，所提出的方法可以有效地检测推荐系统中的托群攻击，在两个数据集上的 F1-measure 分别可以达到 99% 和 76% 以上。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | 2021-01-01                                                                                                                                                                              | 2024-10-28 09:58:43                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:19:58                                                                                                                                  | 2024-10-28 09:58:43                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                  | citationNumber: 5; ccfInfo: CCF-C SCN               | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Recommendation attack detection based on deep learning                                                                                                          | Zhou, Quanqiang; Wu, Jinxia; Duan, Liangliang                                                                                                                           | 2020             | journalArticle  | Journal of Information Security and Applications                                                                                                                                                    | https://linkinghub.elsevier.com/retrieve/pii/S2214212619306131                                                                                                                                                                                                                |                                     | 22142126                        | 10.1016/j.jisa.2020.102493            | Collaborative recommender systems are vulnerable to recommendation attack, in which malicious users insert fake proﬁles into the rating database in order to bias the systems output. To reduce this risk, many methods have been proposed to detect such attack. Despite their effectiveness, a lot of these methods are built based on the hand-designed features which are usually diﬃcult to extract even for domain experts. In order to build the detection method without resorting to hand-designed features, in this paper, we propose a deep learning-based approach for detecting recommendation attack (called DL-DRA). The proposed approach can learn directly from the low-level rating data for the training of classiﬁer. Therefore, it does not have the problem of how to extract hand-designed features. We ﬁrst propose a framework to show the basic structure of the proposed detection approach. Then, we propose a rating matrix generation method to transform the rating vector into rating matrix for each user. After that, we use the bicubic interpolation algorithm to resize the rating matrix in order to reduce the sparsity of the rating matrix. Finally, on the basis of convolutional neural network (CNN), we construct a deep learning network which can learn directly from the resized rating matrix. On this basis, we propose an algorithm for detecting the recommendation attack. We conduct a large number of comparative experiments with the state-of-theart methods for recommendation attack on different scale MovieLens datasets. The experimental results show that the proposed approach can detect the recommendation attack, effectively and steadily. 协作推荐系统很容易受到推荐攻击，恶意用户会在评级数据库中插入虚假信息，从而使系统的输出产生偏差。为了降低这种风险，人们提出了许多方法来检测这种攻击。尽管这些方法很有效，但其中很多方法都是基于手工设计的特征而建立的，即使是领域专家通常也很难提取这些特征。为了建立不依赖手工设计特征的检测方法，我们在本文中提出了一种基于深度学习的推荐攻击检测方法（称为 DL-DRA）。本文提出的方法可以直接从底层评级数据中学习，用于训练分类器。因此，它不存在如何提取人工设计特征的问题。我们首先提出了一个框架来展示所提议的检测方法的基本结构。然后，我们提出一种评分矩阵生成方法，将评分向量转换为每个用户的评分矩阵。然后，我们使用双三次插值算法调整评分矩阵的大小，以减少评分矩阵的稀疏性。最后，在卷积神经网络（CNN）的基础上，我们构建了一个深度学习网络，它可以直接从调整后的评分矩阵中学习。在此基础上，我们提出了一种检测推荐攻击的算法。我们在不同规模的 MovieLens 数据集上进行了大量的对比实验，并与目前最先进的推荐攻击检测方法进行了比较。实验结果表明，所提出的方法能有效、稳定地检测出推荐攻击。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 2020-06                                                                                                                                                                                 | 2024-10-28 09:53:22                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:33                                                                                                                                  | 2024-10-28 09:53:22                                                                                                                                                                                                                                                                                                                                 | 102493                                                                                                                                                                                           | ccfInfo: CCF-C JISA; citationNumber: 25             | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Graph embedding-based approach for detecting group shilling attacks in collaborative recommender systems                                                        | Zhang, Fuzhi; Qu, Yueqi; Xu, Yishu; Wang, Shilei                                                                                                                        | 2020             | journalArticle  | Knowledge-Based Systems                                                                                                                                                                             | https://linkinghub.elsevier.com/retrieve/pii/S0950705120302951                                                                                                                                                                                                                |                                     | 09507051                        | 10.1016/j.knosys.2020.105984          | Over the past decade, many approaches have been presented to detect shilling attacks in collaborative recommender systems. However, these approaches focus mainly on detecting individual attackers and rarely consider the collusive shilling behaviors among attackers, i.e., a group of attackers working together to bias the output of collaborative recommender systems by injecting fake profiles. Such shilling behaviors are generally termed group shilling attacks, which are more harmful to collaborative recommender systems than traditional shilling attacks. In this paper, we propose a graph embeddingbased method to detect group shilling attacks in collaborative recommender systems. First, we construct a user relationship graph by analyzing the user rating behaviors and use a graph embedding method to obtain the low-dimensional vector representation of each node in the user relationship graph. Second, we employ the k-means++ clustering algorithm to obtain candidate groups based on the generated user feature vectors. Finally, we calculate the suspicious degree of each candidate group according to the attack group detection indicators and use the Ward’s hierarchical clustering method to cluster the candidate groups according to their suspicious degrees and obtain the attack groups. The experimental results on the Amazon and Netflix datasets show that the proposed method outperforms the baseline methods in detection performance. 在过去十年中，人们提出了许多方法来检测协同推荐系统中的托客攻击。然而，这些方法主要侧重于检测单个攻击者，而很少考虑攻击者之间的串通托客行为，即一组攻击者通过注入虚假资料，共同使协作推荐系统的输出产生偏差。这种托行为一般被称为群体托攻击，与传统的托攻击相比，它对协同推荐系统的危害更大。本文提出了一种基于图嵌入的方法来检测协同推荐系统中的群体托客攻击。首先，我们通过分析用户评分行为构建用户关系图，并使用图嵌入方法获得用户关系图中每个节点的低维向量表示。其次，我们采用 k-means++ 聚类算法，根据生成的用户特征向量获得候选群组。最后，根据攻击群组检测指标计算每个候选群组的可疑度，并使用 Ward 分层聚类方法根据可疑度对候选群组进行聚类，得到攻击群组。在亚马逊和 Netflix 数据集上的实验结果表明，所提出的方法在检测性能上优于基线方法。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | 2020-07                                                                                                                                                                                 | 2024-10-28 09:52:49                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:24                                                                                                                                  | 2024-10-28 09:52:49                                                                                                                                                                                                                                                                                                                                 | 105984                                                                                                                                                                                           | ccfInfo: CCF-C KBS; citationNumber: 25              | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Detecting Group Shilling Attacks in Online Recommender Systems Based on Bisecting K-Means Clustering                                                            | Zhang, Fuzhi; Wang, Shilei                                                                                                                                              | 2020             | journalArticle  | IEEE Transactions on Computational Social Systems                                                                                                                                                   | https://ieeexplore.ieee.org/document/9169810/?arnumber=9169810                                                                                                                                                                                                                |                                     | 2329-924X                       | 10.1109/TCSS.2020.3013878             | Existing shilling attack detection approaches focus mainly on identifying individual attackers in online recommender systems and rarely address the detection of group shilling attacks in which a group of attackers colludes to bias the output of an online recommender system by injecting fake profiles. In this article, we propose a group shilling attack detection method based on the bisecting K-means clustering algorithm. First, we extract the rating track of each item and divide the rating tracks to generate candidate groups according to a fixed time interval. Second, we propose item attention degree and user activity to calculate the suspicious degrees of candidate groups. Finally, we employ the bisecting K-means algorithm to cluster the candidate groups according to their suspicious degrees and obtain the attack groups. The results of experiments on the Netflix and Amazon data sets indicate that the proposed method outperforms the baseline methods. 现有的托客攻击检测方法主要侧重于识别在线推荐系统中的单个攻击者，而很少涉及群体托客攻击的检测，在群体托客攻击中，一群攻击者通过注入虚假资料串通一气，使在线推荐系统的输出出现偏差。本文提出了一种基于分叉 K-means 聚类算法的群托攻击检测方法。首先，我们提取每个项目的评分轨迹，并按照固定的时间间隔划分评分轨迹，生成候选组。其次，我们提出项目关注度和用户活跃度来计算候选组的可疑度。最后，我们采用 K-means 分叉算法，根据可疑度对候选群组进行聚类，得到攻击群组。在 Netflix 和亚马逊数据集上的实验结果表明，所提出的方法优于基线方法。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 2020-10                                                                                                                                                                                 | 2024-10-28 09:52:01                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:01                                                                                                                                  | 2024-10-28 09:52:01                                                                                                                                                                                                                                                                                                                                 | 1189-1199                                                                                                                                                                                        | citationNumber: 24; ccfInfo: CCF-C TCSS             | /unread; Recommender systems; Feature extraction; recommender systems; Bisecting K-means clustering; Clustering algorithms; group shilling attack detection; group shilling attacks; Principal component analysis; Support vector machines; Target tracking                                                                                                                                                                            |
| BS-SC: An Unsupervised Approach for Detecting Shilling Profiles in Collaborative Recommender Systems                                                            | Cai, Hongyun; Zhang, Fuzhi                                                                                                                                              | 2021             | journalArticle  | IEEE Transactions on Knowledge and Data Engineering                                                                                                                                                 | https://ieeexplore.ieee.org/document/8862947/?arnumber=8862947                                                                                                                                                                                                                |                                     | 1558-2191                       | 10.1109/TKDE.2019.2946247             | Collaborative recommender systems are vulnerable to shilling attacks. To address this issue, many methods including supervised and unsupervised have been proposed. However, supervised detection methods require training classifiers and they only apply to detect known types of attacks. The existing unsupervised detection methods need to know the prior knowledge of attacks, otherwise they suffer from low detection precision. In this paper, we present BS-SC, an unsupervised approach for detecting shilling profiles, which does not need to know the attack size or to label the candidate spammers. BS-SC starts from an in-depth analysis of user behaviors and uses two key mechanisms (i.e., behavior features extraction and behavior similarity matrix clustering) to distinguish shilling profiles from genuine ones. The behavior features reflect the behavior difference between genuine and shilling profiles, and the behavior similarity matrix clustering is to cluster shilling profiles based on their highly similar behaviors. Experimental results on the MovieLens and the sampled Amazon review datasets indicate that BS-SC outperforms the baseline unsupervised approaches, even when the prior knowledge is given for them. 协同推荐系统容易受到托攻击。为了解决这个问题，人们提出了许多方法，包括有监督和无监督方法。然而，监督检测方法需要训练分类器，而且只适用于检测已知类型的攻击。现有的非监督检测方法需要知道攻击的先验知识，否则检测精度低。在本文中，我们提出了一种用于检测托概况的无监督方法 BS-SC，它不需要知道攻击规模，也不需要标记候选垃圾邮件发送者。BS-SC 从对用户行为的深入分析入手，利用两个关键机制（即行为特征提取和行为相似性矩阵聚类）来区分真假托。行为特征提取反映了真托儿与假托儿的行为差异，行为相似性矩阵聚类则是根据假托儿的高度相似行为对其进行聚类。在 MovieLens 和抽样亚马逊评论数据集上的实验结果表明，BS-SC 的性能优于基线无监督方法，即使为它们提供了先验知识。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 2021-04                                                                                                                                                                                 | 2024-10-28 09:42:40                                                                                                                                                                                                                                                                                                                                       | 2024-12-06 22:57:11                                                                                                                                  | 2024-10-28 09:42:40                                                                                                                                                                                                                                                                                                                                 | 1375-1388                                                                                                                                                                                        | ccfInfo: CCF-A  TKDE; citationNumber: 14            | /unread; behavior analysis; Clustering algorithms; Collaboration; Collaborative recommender systems; Correlation; Detection algorithms; Feature extraction; Recommender systems; shilling attacks detection; spectral clustering; Training                                                                                                                                                                                             |
| A Robust Bayesian Probabilistic Matrix Factorization Model for Collaborative Filtering Recommender Systems Based on User Anomaly Rating Behavior Detection      | Yu, Hongtao; Sun, Lijun; Zhang, Fuzhi                                                                                                                                   | 2019             | journalArticle  | KSII Transactions on Internet and Information Systems                                                                                                                                               | https://itiis.org/digital-library/22218                                                                                                                                                                                                                                       |                                     |                                 |                                       | Collaborative filtering recommender systems are vulnerable to shilling attacks in which malicious users may inject biased profiles to promote or demote a particular item being recommended. To tackle this problem, many robust collaborative recommendation methods have been presented. Unfortunately, the robustness of most methods is improved at the expense of prediction accuracy. In this paper, we construct a robust Bayesian probabilistic matrix factorization model for collaborative filtering recommender systems by incorporating the detection of user anomaly rating behaviors. We first detect the anomaly rating behaviors of users by the modified K-means algorithm and target item identification method to generate an indicator matrix of attack users. Then we incorporate the indicator matrix of attack users to construct a robust Bayesian probabilistic matrix factorization model and based on which a robust collaborative recommendation algorithm is devised. The experimental results on the MovieLens and Netflix datasets show that our model can significantly improve the robustness and recommendation accuracy compared with three baseline methods. 协同过滤推荐系统很容易受到托攻击，恶意用户可能会注入有偏见的资料来提升或降低被推荐的特定项目。为解决这一问题，人们提出了许多稳健的协作推荐方法。遗憾的是，大多数方法的稳健性都是以牺牲预测准确性为代价的。在本文中，我们为协同过滤推荐系统构建了一个稳健的贝叶斯概率矩阵因式分解模型，其中包含了对用户异常评分行为的检测。我们首先通过改进的 K-means 算法和目标项识别方法检测用户的异常评分行为，生成攻击用户的指标矩阵。然后，我们结合攻击用户的指标矩阵构建稳健的贝叶斯概率矩阵因式分解模型，并在此基础上设计出稳健的协同推荐算法。在 MovieLens 和 Netflix 数据集上的实验结果表明，与三种基线方法相比，我们的模型能显著提高鲁棒性和推荐准确性。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | 2019-09-29                                                                                                                                                                              | 2024-10-28 09:45:38                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:21                                                                                                                                  | 2024-10-28 09:45:38                                                                                                                                                                                                                                                                                                                                 | 4684-4705                                                                                                                                                                                        | citationNumber: 2; ccfInfo: CCF-None ITIIS          | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Slanderous user detection with modified recurrent neural networks in recommender system                                                                         | Xu, Yuanbo; Yang, Yongjian; Han, Jiayu; Wang, En; Ming, Jingci; Xiong, Hui                                                                                              | 2019             | journalArticle  | Information Sciences                                                                                                                                                                                | https://linkinghub.elsevier.com/retrieve/pii/S0020025519307017                                                                                                                                                                                                                |                                     | 00200255                        | 10.1016/j.ins.2019.07.081             | We focus on how to tackle a unique multi-view unsupervised issue: slanderous user detection, with recurrent neural networks to beneﬁt recommender systems. In real-world recommender systems, some consumers always give fake reviews and low ratings to the items they bought on purpose. In order to ensure their proﬁts, these slanderous users make a semantic gap between their ratings and reviews to avoid detection, which makes slanderous user detection a more diﬃcult problem. On some occasions, they give a false low rating with a positive review which confuse recommender systems, and vice versa. To address the above problem, in this paper, we propose a novel recommendation framework: Slanderous user Detection Recommender System (SDRS). In SDRS, we design a Hierarchical Dual-Attention recurrent Neural network (HDAN) with a modiﬁed GRU (mGRU) to compute an opinion level for reviews. Then a joint ﬁltering method is proposed to catch the gap between ratings and reviews. With joint ﬁltering, slanderous users can be detected and omitted. Finally, a modiﬁed non-negative matrix factorization is proposed to make recommendations. Extensive experiments are conducted in four datasets: Amazon, Yelp, Taobao, and Jingdong, in which the results demonstrate that our proposed method can detect slanderous users and make accurate recommendations in a uniform framework. Also, with slanderous user detection, some state-of-the-art recommendation systems can be beneﬁted. 我们的研究重点是如何利用递归神经网络解决一个独特的多视角无监督问题：诽谤用户检测，从而使推荐系统受益。在现实世界的推荐系统中，一些消费者总是故意给自己购买的商品提供虚假评论和低评分。为了确保自己的利益，这些诽谤用户会在他们的评分和评论之间制造语义空白，以避免被发现，这使得诽谤用户检测成为一个更困难的问题。在某些情况下，他们会给出虚假的低评分和正面评论，从而混淆推荐系统，反之亦然。针对上述问题，本文提出了一种新颖的推荐框架： 诽谤用户检测推荐系统（SDRS）。在 SDRS 中，我们设计了一个分层双注意递归神经网络（HDAN）和一个改进的 GRU（mGRU）来计算评论的意见等级。然后，我们提出了一种联合过滤方法来捕捉评分和评论之间的差距。通过联合过滤，可以检测并忽略诽谤用户。最后，提出了一种改进的非负矩阵因式分解方法来进行推荐。我们在四个数据集上进行了广泛的实验： 实验结果表明，我们提出的方法可以在统一的框架下检测出诽谤用户并做出准确的推荐。此外，通过诽谤用户检测，一些最先进的推荐系统也能从中受益。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | 2019-12                                                                                                                                                                                 | 2024-10-28 09:44:12                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:16                                                                                                                                  | 2024-10-28 09:44:12                                                                                                                                                                                                                                                                                                                                 | 265-281                                                                                                                                                                                          | ccfInfo: CCF-B; citationNumber: 28                  | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Detecting shilling attacks in social recommender systems based on time series analysis and trust features                                                       | Xu, Yishu; Zhang, Fuzhi                                                                                                                                                 | 2019             | journalArticle  | Knowledge-Based Systems                                                                                                                                                                             | https://linkinghub.elsevier.com/retrieve/pii/S0950705119301777                                                                                                                                                                                                                |                                     | 09507051                        | 10.1016/j.knosys.2019.04.012          | In social recommender systems or trust-based recommender systems, malicious users can bias the recommendations by injecting a large number of fake profiles and by building bogus trust relationships. The existing shilling attack detection methods suffer from low precision when detecting attacks in social recommender systems because they focus mainly on the rating pattern differences between attack profiles and genuine ones and ignore the trust relationships between users. In this paper, we propose an approach for detecting shilling attacks in social recommender systems based on time series analysis and trust features (TSA–TF). Firstly, we construct rating distribution time series for items and propose a dynamic rating distribution prediction model to detect suspicious items by using a single exponential smoothing method. Then, we filter out a part of genuine user profiles by analyzing suspicious items and obtain the set of suspicious user profiles. Secondly, we propose four features by combining rating patterns and trust relationships and train a support vector machine (SVM) classifier to discriminate attack profiles in the set of suspicious user profiles. Experiments on the CiaoDVD dataset and Epinions dataset show that the proposed approach can improve the detection precision while maintaining a high recall. 在社交推荐系统或基于信任的推荐系统中，恶意用户可以通过注入大量虚假资料和建立虚假信任关系来使推荐出现偏差。现有的托儿攻击检测方法在检测社交推荐系统中的攻击时存在精度低的问题，因为它们主要关注攻击档案与真实档案之间的评分模式差异，而忽略了用户之间的信任关系。本文提出了一种基于时间序列分析和信任特征（TSA-TF）的社交推荐系统中托攻击检测方法。首先，我们构建了项目的评分分布时间序列，并提出了一种动态评分分布预测模型，利用单指数平滑法检测可疑项目。然后，通过分析可疑条目过滤掉一部分真实用户资料，得到可疑用户资料集。其次，我们结合评分模式和信任关系提出了四个特征，并训练支持向量机（SVM）分类器来区分可疑用户配置文件集中的攻击配置文件。在 CiaoDVD 数据集和 Epinions 数据集上的实验表明，所提出的方法可以提高检测精度，同时保持较高的召回率。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 2019-08                                                                                                                                                                                 | 2024-10-25 05:29:15                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:02                                                                                                                                  | 2024-10-25 05:29:15                                                                                                                                                                                                                                                                                                                                 | 25-47                                                                                                                                                                                            | citationNumber: 22; ccfInfo: CCF-C KBS              | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Evaluating Recommender System Stability with Influence-Guided Fuzzing                                                                                           | Shriver, David; Elbaum, Sebastian; Dwyer, Matthew B.; Rosenblum, David S.                                                                                               | 2019             | journalArticle  | Proceedings of the AAAI Conference on Artificial Intelligence                                                                                                                                       | https://ojs.aaai.org/index.php/AAAI/article/view/4423                                                                                                                                                                                                                         |                                     | 2374-3468, 2159-5399            | 10.1609/aaai.v33i01.33014934          | Recommender systems help users to find products or services they may like when lacking personal experience or facing an overwhelming set of choices. Since unstable recommendations can lead to distrust, loss of profits, and a poor user experience, it is important to test recommender system stability. In this work, we present an approach based on inferred models of influence that underlie recommender systems to guide the generation of dataset modifications to assess a recommender’s stability. We implement our approach and evaluate it on several recommender algorithms using the MovieLens dataset. We find that influence-guided fuzzing can effectively find small sets of modifications that cause significantly more instability than random approaches. 当用户缺乏个人经验或面临大量选择时，推荐系统可以帮助用户找到他们可能喜欢的产品或服务。由于不稳定的推荐会导致不信任、利润损失和糟糕的用户体验，因此测试推荐系统的稳定性非常重要。在这项工作中，我们提出了一种基于推荐系统影响推断模型的方法，用于指导数据集修改的生成，以评估推荐系统的稳定性。我们使用 MovieLens 数据集实现了我们的方法并对几种推荐算法进行了评估。我们发现，与随机方法相比，影响引导模糊法能有效地找到导致更多不稳定性的小修改集。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | 2019-07-17                                                                                                                                                                              | 2024-10-29 01:00:52                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:31                                                                                                                                  | 2024-10-29 01:00:52                                                                                                                                                                                                                                                                                                                                 | 4934-4942                                                                                                                                                                                        | ccfInfo: CCF-A AAAI; citationNumber: 27             | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| A novel shilling attack detection model based on particle filter and gravitation                                                                                | Qi, Lingtao; Huang, Haiping; Li, Feng; Malekian, Reza; Wang, Ruchuan                                                                                                    | 2019             | journalArticle  | China Communications                                                                                                                                                                                | https://ieeexplore.ieee.org/document/9070138/?arnumber=9070138                                                                                                                                                                                                                |                                     | 1673-5447                       | 10.23919/JCC.2019.10.008              | With the rapid development of e-commerce, the security issues of collaborative filtering recommender systems have been widely investigated. Malicious users can benefit from injecting a great quantities of fake profiles into recommender systems to manipulate recommendation results. As one of the most important attack methods in recommender systems, the shilling attack has been paid considerable attention, especially to its model and the way to detect it. Among them, the loose version of Group Shilling Attack Generation Algorithm (GSAGenl) has outstanding performance. It can be immune to some PCC (Pearson Correlation Coefficient)-based detectors due to the nature of anti-Pearson correlation. In order to overcome the vulnerabilities caused by GSAGenl a gravitation-based detection model (GBDM) is presented, integrated with a sophisticated gravitational detector and a decider. And meanwhile two new basic attributes and a particle filter algorithm are used for tracking prediction. And then, whether an attack occurs can be judged according to the law of universal gravitation in decision-making. The detection performances of GBDM, HHT-SVM, UnRAP, AP-UnRAP Semi-SAD, SVM-TIA and PCA-P are compared and evaluated. And simulation results show the effectiveness and availability of GBDM. 随着电子商务的迅猛发展，协同过滤推荐系统的安全问题受到广泛关注。恶意用户可以通过向推荐系统注入大量虚假资料来操纵推荐结果，从而从中获益。作为推荐系统中最重要的攻击方法之一，托攻击受到了相当的关注，尤其是其模型和检测方法。其中，松散版的组托攻击生成算法（GSAGenl）性能突出。由于反皮尔逊相关性的特性，它可以对一些基于 PCC（皮尔逊相关系数）的检测器免疫。为了克服 GSAGenl 所带来的漏洞，我们提出了一个基于引力的探测模型（GBDM），该模型集成了一个复杂的引力探测器和一个解码器。同时，两种新的基本属性和粒子滤波算法被用于跟踪预测。然后，就可以根据万有引力定律来判断攻击是否发生。比较并评估了 GBDM、HHT-SVM、UnRAP、AP-UnRAP Semi-SAD、SVM-TIA 和 PCA-P 的检测性能。仿真结果表明了 GBDM 的有效性和可用性。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 2019-10                                                                                                                                                                                 | 2024-10-28 09:46:46                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:19:55                                                                                                                                  | 2024-10-28 09:46:46                                                                                                                                                                                                                                                                                                                                 | 112-132                                                                                                                                                                                          | ccfInfo: Not Found; citationNumber: 4               | /unread; Detection algorithms; Recommender systems; Feature extraction; Detectors; Principal component analysis; Support vector machines; Classification algorithms; collaborative filtering recommender systems; gravitation-based detection model; particle filter algorithm; shilling attack detection model                                                                                                                        |
| <i>L</i>2, 1-Norm Regularized Matrix Completion for Attack Detection in Collaborative Filtering Recommender Systems                                             | Mingdan, S. I.; Qingshan, L. I.                                                                                                                                         | 2019             | journalArticle  | Chinese Journal of Electronics                                                                                                                                                                      | https://cje.ejournal.org.cn/en/article/doi/10.1049/cje.2019.06.010                                                                                                                                                                                                            |                                     |                                 | 10.1049/cje.2019.06.010               | Collaborative filtering recommender systems (CFRSs) are known to be highly vulnerable to profile injection attacks, in which malicious users insert fake profiles into the rating database in order to bias the systems' output, since their openness, and attack detection is still a challenging problem in CFRSs. In order to provide more accurate recommendations, many schemas have been proposed to detect such shilling attacks. However, almost all of them are proposed to detect one or several specific attack types, and few of them can handle hybrid attack types, which usually happen in practice. With this problem in mind, we propose a novel <i>L</i>2, 1-norm regularized matrix completion incorporating prior information (LRMCPI) model to detect shilling attacks by combining matrix completion and <i>L</i>2, 1-norm. The proposed LRMCPI formalizes the attack detection problem as a missing value estimation problem, and it is appropriate because the user-item rating matrix is approximately low-rank and attack profiles could be considered as structural noise. The proposed LRMCPI model not only can better recover the rating matrix using correct rating value but also can detect the positions where the attackers are injected. We evaluate our model on three well-known data sets with different density and the experimental results show that our model outperforms baseline algorithms in both single and hybrid attack types. 众所周知，协同过滤推荐系统（CFRS）极易受到档案注入攻击，在这种攻击中，恶意用户会在评级数据库中插入虚假档案，从而使系统的输出产生偏差。为了提供更准确的推荐，人们提出了许多模式来检测这类托攻击。然而，几乎所有的模式都是为了检测一种或几种特定的攻击类型而提出的，很少有模式能处理混合攻击类型，而这种攻击类型在实际应用中经常发生。 所提出的 LRMCPI 模型将攻击检测问题形式化为缺失值估计问题，由于用户-项目评分矩阵近似为低秩矩阵，且攻击特征可被视为结构噪声，因此该模型非常合适。所提出的 LRMCPI 模型不仅能利用正确的评分值更好地恢复评分矩阵，还能检测到攻击者注入的位置。我们在三个具有不同密度的知名数据集上评估了我们的模型，实验结果表明，我们的模型在单一和混合攻击类型中都优于基准算法。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | 2019-09-10                                                                                                                                                                              | 2024-10-28 09:47:13                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:07                                                                                                                                  | 2024-10-28 09:47:13                                                                                                                                                                                                                                                                                                                                 | 906-915                                                                                                                                                                                          | citationNumber: 1; ccfInfo: Not Found               | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Detecting Shilling Attacks with Automatic Features from Multiple Views                                                                                          | Hao, Yaojun; Zhang, Fuzhi; Wang, Jian; Zhao, Qingshan; Cao, Jianfang                                                                                                    | 2019             | journalArticle  | Security and Communication Networks                                                                                                                                                                 |                                                                                                                                                                                                                                                                               |                                     |                                 | 10.1155/2019/6523183                  | Due to the openness of the recommender systems, the attackers are likely to inject a large number of fake profiles to bias the prediction of such systems. The traditional detection methods mainly rely on the artificial features, which are often extracted from one kind of user-generated information. In these methods, fine-grained interactions between users and items cannot be captured comprehensively, leading to the degradation of detection accuracy under various types of attacks. In this paper, we propose an ensemble detection method based on the automatic features extracted from multiple views. Firstly, to collaboratively discover the shilling profiles, the users’ behaviors are analyzed from multiple views including ratings, item popularity, and user-user graph. Secondly, based on the data preprocessed from multiple views, the stacked denoising autoencoders are used to automatically extract user features with different corruption rates. Moreover, the features extracted from multiple views are effectively combined based on principal component analysis. Finally, according to the features extracted with different corruption rates, the weak classifiers are generated and then integrated to detect attacks. The experimental results on the MovieLens, Netflix, and Amazon datasets indicate that the proposed method can effectively detect various attacks. 由于推荐系统的开放性，攻击者很可能会注入大量虚假资料，使此类系统的预测产生偏差。传统的检测方法主要依赖人工特征，而人工特征往往是从一种用户生成的信息中提取的。在这些方法中，无法全面捕捉用户与物品之间的细粒度交互，导致在各种类型的攻击下检测精度下降。本文提出了一种基于多视图自动特征提取的集合检测方法。首先，为了协同发现托的特征，从评分、物品人气、用户与用户图谱等多个视图分析用户的行为。其次，基于从多个视图预处理的数据，使用堆叠去噪自动编码器自动提取不同损坏率的用户特征。此外，基于主成分分析法，从多个视图中提取的特征被有效地组合在一起。最后，根据提取到的不同损坏率的特征，生成弱分类器，然后进行整合，以检测攻击。在 MovieLens、Netflix 和亚马逊数据集上的实验结果表明，所提出的方法可以有效地检测出各种攻击。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | 2019-08-05                                                                                                                                                                              | 2024-10-28 09:46:13                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:26                                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                     | 1-13                                                                                                                                                                                             | ccfInfo: CCF-C SCN; citationNumber: 19              | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Robustness analysis of multi-criteria collaborative filtering algorithms against shilling attacks                                                               | Turk, Ahmet Murat; Bilge, Alper                                                                                                                                         | 2019             | journalArticle  | Expert Systems with Applications                                                                                                                                                                    | https://linkinghub.elsevier.com/retrieve/pii/S0957417418305013                                                                                                                                                                                                                |                                     | 09574174                        | 10.1016/j.eswa.2018.08.001            | Collaborative ﬁltering is an emerging recommender system technique that aims guiding users based on other customers preferences with behavioral similarities. Such correspondences are located based on preference history of users. A relatively new extension of traditional collaborative ﬁltering schemes takes into account not only how much a user likes an item, but also why she likes the item by collecting multi-criteria preferences focusing on distinctive features of the items. These multi-criteria collaborative ﬁltering systems have the potential to improve recommender system accuracy since they reveal multiple views of users on products. However, due to providing more insightful recommendations, such systems might be subjected to malicious attacks more substantially than the traditional ones. Attackers attempt to insert fake proﬁles to bias outputs of these systems in favor of a particular product or disrepute the system itself. Since outputs of expert systems directly dependent on input signals; interventions to the inputs coherently cause failures on productions of such systems. In this study, we examine shilling attack strategies against multi-criteria preference collections, how to extend well-known attack scenarios against these systems, and propose an alternative attacking scheme. We analyze the robustness of baseline multi-criteria recommendation algorithms regarding various similarity aggregation procedures against proposed attacking schemes by the extensive experimental investigation. Empirical results on real-world data demonstrate that these systems are highly vulnerable to manipulations and proper attack detection practices are needed to ensure recommendation quality. According to our ﬁndings, manipulative attempts at such expert systems mislead decision-making process. 协作式推荐是一种新兴的推荐系统技术，旨在根据具有行为相似性的其他客户的偏好来引导用户。这种对应关系是根据用户的偏好历史记录确定的。传统协作式推荐方案的一个相对较新的扩展，不仅考虑了用户对商品的喜爱程度，还通过收集侧重于商品独特特征的多标准偏好，考虑了用户喜爱商品的原因。这些多标准协同筛选系统具有提高推荐系统准确性的潜力，因为它们揭示了用户对产品的多种看法。然而，由于能提供更具洞察力的推荐，这类系统可能比传统系统更容易受到恶意攻击。攻击者试图插入虚假信息，使这些系统的输出结果偏向于某一特定产品，或使系统本身名誉受损。由于专家系统的输出直接依赖于输入信号，对输入信号的干预会导致此类系统产品的失效。在本研究中，我们研究了针对多标准偏好集合的托攻击策略，如何扩展针对这些系统的众所周知的攻击场景，并提出了一种替代攻击方案。通过广泛的实验调查，我们分析了基线多标准推荐算法中各种相似性聚合程序对所提攻击方案的鲁棒性。真实世界数据的经验结果表明，这些系统极易受到操纵，因此需要适当的攻击检测方法来确保推荐质量。根据我们的发现，对此类专家系统的操纵企图会误导决策过程。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 2019-01                                                                                                                                                                                 | 2024-10-25 05:41:23                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:27                                                                                                                                  | 2024-10-25 05:41:23                                                                                                                                                                                                                                                                                                                                 | 386-402                                                                                                                                                                                          | ccfInfo: CCF-C ESWA; citationNumber: 32             | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Detecting unfair recommendations in trust-based pervasive environments                                                                                          | D’Angelo, Gianni; Palmieri, Francesco; Rampone, Salvatore                                                                                                               | 2019             | journalArticle  | Information Sciences                                                                                                                                                                                | https://linkinghub.elsevier.com/retrieve/pii/S0020025519301240                                                                                                                                                                                                                |                                     | 00200255                        | 10.1016/j.ins.2019.02.015             | In pervasive/ubiquitous computing environments, interacting users may evaluate their respective trustworthiness by using historical data coming from their past interactions. Nevertheless, when two users are at the ﬁrst interaction, they have no historical data involving their own activities to be analyzed, and then use information (recommender-data) provided by other users (recommenders) who, in the past, have had interactions with one of the involved parties. Although this approach has proven to be effective, it might fail if dishonest recommenders provide unfair recommender-data. Indeed, such unfair data may lead to skewed evaluations, and therefore either increase the trustworthiness of a malicious user or reduce the one of a honest user in a fraudulent way. In this work, we propose an algorithm for identifying false recommender-data. Our attention is explicitly focused on recommender-data rather than recommenders. This because some recommenders could provide recommender-data containing only a limited (but speciﬁc) set of altered information. This is used by dishonest recommenders as a tactic to avoid being discovered. The proposed algorithm uses association rules to express a conﬁdence-based measure (reputation rank), which is used as a reliability ranking of the recommender-data. The resulting approach has been compared with other existing ones in this ﬁeld, resulting more accurate in ﬁnding out unfair recommender-data sent by dishonest recommenders. 在普适/泛在计算环境中，交互用户可以通过过去的交互历史数据来评估各自的可信度。然而，当两个用户首次互动时，他们没有涉及自身活动的历史数据可供分析，因此需要使用过去与其中一方有过互动的其他用户（推荐人）提供的信息（推荐人数据）。尽管这种方法被证明是有效的，但如果不诚实的推荐人提供不公平的推荐数据，这种方法就可能失败。事实上，这种不公平的数据可能会导致评价偏差，从而增加恶意用户的可信度，或以欺诈方式降低诚实用户的可信度。在这项工作中，我们提出了一种识别虚假推荐数据的算法。我们的注意力明确地集中在推荐人数据上，而不是推荐人上。这是因为有些推荐人提供的推荐数据可能只包含有限的（但特定的）被篡改的信息。不诚实的推荐者会利用这一点来避免被发现。所提出的算法使用关联规则来表达基于可信度的衡量标准（声誉等级），并将其用作推荐人数据的可靠性排名。该算法与该领域的其他现有算法进行了比较，结果显示，该算法在找出不诚实推荐者发送的不公平推荐数据方面更为准确。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | 2019-06                                                                                                                                                                                 | 2024-10-29 00:55:01                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:26                                                                                                                                  | 2024-10-29 00:55:01                                                                                                                                                                                                                                                                                                                                 | 31-51                                                                                                                                                                                            | ccfInfo: CCF-B; citationNumber: 39                  | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| An ensemble detection method for shilling attacks based on features of automatic extraction                                                                     | Hao, Yaojun; Zhang, Fuzhi; Chao, Jinbo                                                                                                                                  | 2019             | journalArticle  | China Communications                                                                                                                                                                                | https://ieeexplore.ieee.org/document/8820766/?arnumber=8820766                                                                                                                                                                                                                |                                     | 1673-5447                       | 10.23919/JCC.2019.08.012              | Faced with the evolving attacks in recommender systems, many detection features have been proposed by human engineering and used in supervised or unsupervised detection methods. However, the detection features extracted by human engineering are usually aimed at some specific types of attacks. To further detect other new types of attacks, the traditional methods have to re-extract detection features with high knowledge cost. To address these limitations, the method for automatic extraction of robust features is proposed and then an Adaboost-based detection method is presented. Firstly, to obtain robust representation with prior knowledge, unlike uniform corruption rate in traditional mLDA (marginalized Linear Denoising Autoencoder), different corruption rates for items are calculated according to the ratings' distribution. Secondly, the ratings sparsity is used to weight the mapping matrix to extract low-dimensional representation. Moreover, the uniform corruption rate is also set to the next layer in mSLDA (marginalized Stacked Linear Denoising Autoencoder) to extract the stable and robust user features. Finally, under the robust feature space, an Adaboost-based detection method is proposed to alleviate the imbalanced classification problem. Experimental results on the Netflix and Amazon review datasets indicate that the proposed method can effectively detect various attacks. 面对推荐系统中不断发展的攻击行为，人类工程学提出了许多检测特征，并将其用于有监督或无监督的检测方法中。然而，人类工程学提取的检测特征通常是针对某些特定类型的攻击。为了进一步检测其他新类型的攻击，传统方法必须重新提取检测特征，知识成本很高。针对这些局限性，本文提出了自动提取鲁棒特征的方法，然后介绍了基于 Adaboost 的检测方法。首先，为了获得具有先验知识的鲁棒表示，与传统 mLDA（边际化线性去噪自动编码器）的统一损坏率不同，根据评分分布计算项目的不同损坏率。其次，利用评分的稀疏性对映射矩阵进行加权，以提取低维表示。此外，在 mSLDA（边际化堆叠线性去噪自动编码器）中的下一层也设置了统一的损坏率，以提取稳定和鲁棒的用户特征。最后，在鲁棒特征空间下，提出了一种基于 Adaboost 的检测方法，以缓解不平衡分类问题。在 Netflix 和亚马逊评论数据集上的实验结果表明，所提出的方法能有效检测出各种攻击。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | 2019-08                                                                                                                                                                                 | 2024-10-28 09:48:36                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:19:58                                                                                                                                  | 2024-10-28 09:48:36                                                                                                                                                                                                                                                                                                                                 | 130-146                                                                                                                                                                                          | citationNumber: 5; ccfInfo: Not Found               | /unread; Recommender systems; Computational modeling; Feature extraction; shilling attack; Classification algorithms; Data mining; ensemble detection; Ergonomics; features of automatic extraction; marginalized linear denoising autoencoder; Noise reduction                                                                                                                                                                        |
| MENTOR: The Design and Evaluation of a Protection Services Recommender System                                                                                   | Franco, Muriel Figueredo; Rodrigues, Bruno; Stiller, Burkhard                                                                                                           | 2019             | conferencePaper | 2019 15th International Conference on Network and Service Management (CNSM)                                                                                                                         | https://ieeexplore.ieee.org/document/9012686/                                                                                                                                                                                                                                 | 978-3-903176-24-9                   |                                 | 10.23919/CNSM46954.2019.9012686       | Cyberattacks are the cause of several damages on governments and companies in the last years. Such damage includes not only leaks of sensitive information, but also economic loss due to downtime of services. The security market size worth billions of dollars, which represents investments to acquire protection services and training response teams to operate such services, determines a considerable part of the investment in technologies around the world. Although a vast number of protection services are available, it is neither trivial for network operators nor endusers to choose one of them in order to prevent or mitigate an imminent attack. As the next-generation cybersecurity solutions are on the horizon, systems that simplify their adoption are still required in support of security management tasks. Thus, this paper introduces MENTOR, a support tool for cybersecurity, focusing on the recommendation of protection services. MENTOR is able to (a) to deal with different demands from the user and (b) to recommend the adequate protection service in order to provide a proper level of cybersecurity in different scenarios. Four similarity measurements are implemented in order to prove the feasibility of the MENTOR’s engine. An evaluation determines the performance and accuracy of each measurement used during the recommendation process. 在过去几年中，网络攻击给政府和公司造成了数次损失。这些损失不仅包括敏感信息的泄露，还包括因服务停机造成的经济损失。价值数十亿美元的安全市场规模代表了为获取保护服务和培训响应团队以操作此类服务所进行的投资，决定了全球技术投资的很大一部分。尽管有大量的保护服务可供选择，但网络运营商和终端用户要想预防或减轻即将发生的攻击，选择其中一种服务并非易事。由于下一代网络安全解决方案即将问世，因此仍然需要能简化其采用过程的系统来支持安全管理任务。因此，本文介绍了网络安全支持工具 MENTOR，其重点是推荐保护服务。MENTOR 能够：(a) 应对用户的不同需求；(b) 推荐适当的保护服务，以便在不同场景下提供适当的网络安全水平。为了证明 MENTOR 引擎的可行性，我们采用了四种相似性测量方法。评估确定了推荐过程中使用的每种测量方法的性能和准确性。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | 2019-10                                                                                                                                                                                 | 2024-10-28 09:47:44                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:25                                                                                                                                  | 2024-10-28 09:47:44                                                                                                                                                                                                                                                                                                                                 | 1-7                                                                                                                                                                                              | citationNumber: 24; ccfInfo: CCF-None CNSM          | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Trustworthy and profit: A new value-based neighbor selection method in recommender systems under shilling attacks                                               | Cai, Yuanfeng; Zhu, Dan                                                                                                                                                 | 2019             | journalArticle  | Decision Support Systems                                                                                                                                                                            | https://linkinghub.elsevier.com/retrieve/pii/S0167923619301411                                                                                                                                                                                                                |                                     | 01679236                        | 10.1016/j.dss.2019.113112             | User-based collaborative filtering recommender systems are widely deployed by e-retailers to facilitate customer’ decision-making and enhance e-retailers' profitability. Despite the advantages these systems provide, their recommendation effectiveness is vulnerable to attacks from malicious users who inject biased ratings. Such attacks against recommender systems are called shilling attacks. Although several shilling attack detection mechanisms have been proposed in previous studies, their detection performance is limited in various attack conditions. Furthermore, few of these mechanisms consider the value-dimension associated with recommendations, which is crucial for e-retailers. This research proposes a novel approach called Value-based Neighbor Selection (VNS) to address the above limitations. The objective of this approach is to protect recommender systems from shilling attacks while improving e-retailers' profitability. It alleviates the aforementioned problems through strategically selecting neighbors whose preferences are then used to make recommendations. We have performed a series of empirical validations in various attack conditions to compare the performance of the proposed method and three benchmark methods, in terms of both recommendation accuracy and e-retailer profitability. The results show the advantages of the proposed method in balancing customer satisfaction and e-retailer profitability. 网络零售商广泛采用基于用户的协同过滤推荐系统，以帮助客户做出决策，提高网络零售商的盈利能力。尽管这些系统具有诸多优势，但其推荐效果很容易受到恶意用户的攻击，这些恶意用户会注入带有偏见的评价。这种针对推荐系统的攻击被称为 “托”（shilling）攻击。尽管之前的研究已经提出了几种托客攻击检测机制，但它们在各种攻击条件下的检测性能都很有限。此外，这些机制很少考虑与推荐相关的价值维度，而这对电子零售商来说至关重要。本研究提出了一种名为 “基于价值的邻居选择”（VNS）的新方法来解决上述局限性。这种方法的目的是保护推荐系统免受托攻击，同时提高电子零售商的盈利能力。它通过战略性地选择邻居，然后利用邻居的偏好进行推荐，从而缓解上述问题。我们在各种攻击条件下进行了一系列经验验证，从推荐准确性和电子零售商盈利能力两方面比较了所提方法和三种基准方法的性能。结果表明，建议的方法在平衡客户满意度和电子零售商盈利能力方面具有优势。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | 2019-09                                                                                                                                                                                 | 2024-10-28 09:42:15                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:19                                                                                                                                  | 2024-10-28 09:42:15                                                                                                                                                                                                                                                                                                                                 | 113112                                                                                                                                                                                           | citationNumber: 16; ccfInfo: CCF-C DSS              | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Enhancing the Robustness of Neural Collaborative Filtering Systems Under Malicious Attacks                                                                      | Du, Yali; Fang, Meng; Yi, Jinfeng; Xu, Chang; Cheng, Jun; Tao, Dacheng                                                                                                  | 2019             | journalArticle  | IEEE Transactions on Multimedia                                                                                                                                                                     | https://ieeexplore.ieee.org/document/8576563/?arnumber=8576563                                                                                                                                                                                                                |                                     | 1941-0077                       | 10.1109/TMM.2018.2887018              | Recommendation systems have become ubiquitous in online shopping in recent decades due to their power in reducing excessive choices of customers and industries. Recent collaborative filtering methods based on the deep neural network are studied and introduce promising results due to their power in learning hidden representations for users and items. However, it has revealed its vulnerabilities under malicious user attacks. With the knowledge of a collaborative filtering algorithm and its parameters, the performance of this recommendation system can be easily downgraded. Unfortunately, this problem is not addressed well, and the study on defending recommendation systems is insufficient. In this paper, we aim to improve the robustness of recommendation systems based on two concepts - stage-wise hints training and randomness. To protect a target model, we introduce noise layers in the training of a target model to increase its resistance to adversarial perturbations. To reduce the noise layers' influence on model performance, we introduce intermediate layer outputs as hints from a teacher model to regularize the intermediate layers of a student target model. We consider white box attacks under which attackers have the knowledge of the target model. The generalizability and robustness properties of our method have been analytically inspected in experiments and discussions, and the computational cost is comparable to training a standard neural network-based collaborative filtering model. Through our investigation, the proposed defensive method can reduce the success rate of malicious user attacks and keep the prediction accuracy comparable to standard neural recommendation systems. 近几十年来，推荐系统在网上购物中变得无处不在，因为它能减少客户和行业的过多选择。最近研究了基于深度神经网络的协同过滤方法，由于其在学习用户和项目的隐藏表征方面具有强大的能力，因此取得了可喜的成果。然而，它也暴露出了在用户恶意攻击下的脆弱性。只要掌握了协同过滤算法及其参数，这种推荐系统的性能就很容易被降低。遗憾的是，这个问题没有得到很好的解决，对防御推荐系统的研究也不够充分。本文旨在基于两个概念--阶段性提示训练和随机性--来提高推荐系统的鲁棒性。为了保护目标模型，我们在目标模型的训练中引入了噪声层，以增强其对对抗性扰动的抵抗力。为了减少噪声层对模型性能的影响，我们引入中间层输出作为教师模型的提示，以规范学生目标模型的中间层。我们考虑了白盒攻击，在这种情况下，攻击者掌握了目标模型的知识。在实验和讨论中，我们分析检验了该方法的普适性和鲁棒性，其计算成本与训练基于神经网络的标准协同过滤模型相当。通过研究，我们提出的防御方法可以降低恶意用户攻击的成功率，并保持与标准神经推荐系统相当的预测精度。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | 2019-03                                                                                                                                                                                 | 2024-10-25 05:25:46                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:03                                                                                                                                  | 2024-10-25 05:25:46                                                                                                                                                                                                                                                                                                                                 | 555-565                                                                                                                                                                                          | citationNumber: 39; ccfInfo: CCF-B TMM              | /unread; Collaboration; Robustness; Neural networks; Training; adversarial learning; collaborative filtering; Information technology; malicious attacks; Perturbation methods; Recommendation systems; Standards                                                                                                                                                                                                                       |
| An Unsupervised Method for Detecting Shilling Attacks in Recommender Systems by Mining Item Relationship and Identifying Target Items                           | Cai, Hongyun; Zhang, Fuzhi; Levi, Albert                                                                                                                                | 2019             | journalArticle  | The Computer Journal                                                                                                                                                                                | https://ieeexplore.ieee.org/document/8852948                                                                                                                                                                                                                                  |                                     | 1460-2067                       | 10.1093/comjnl/bxy124                 | Collaborative filtering (CF) recommender systems have been shown to be vulnerable to shilling attacks. How to quickly and effectively detect shilling attacks is a key challenge for improving the quality and reliability of CF recommender systems. Although many recent studies have been devoted to detecting shilling attacks, there are still problems that require further discussion, especially the improvement of the detection performance on real-world unlabelled datasets. In this work, we propose an unsupervised approach that exploits item relationship and target item(s) for attack detection. We first extract behaviour features based on the item relationship. Then, we distinguish suspicious users from normal users and construct a set of suspicious users. Finally, we identify target item(s) by analysing the aggregation behaviour of suspicious users, based on which we detect attack users from the set of suspicious users. Extensive experiments on the MovieLens 100K dataset and sampled Amazon review dataset demonstrate the effectiveness of the proposed approach for detecting shilling attacks in recommender systems. 事实证明，协同过滤（CF）推荐系统很容易受到托攻击。如何快速有效地检测 “托 ”攻击是提高 CF 推荐系统质量和可靠性的关键挑战。虽然近期有很多研究致力于检测托攻击，但仍有一些问题需要进一步探讨，特别是在真实世界无标签数据集上提高检测性能。在这项工作中，我们提出了一种利用项目关系和目标项目进行攻击检测的无监督方法。我们首先根据项目关系提取行为特征。然后，我们将可疑用户与正常用户区分开来，并构建可疑用户集。最后，我们通过分析可疑用户的聚合行为确定目标项目，并据此从可疑用户集中检测攻击用户。在 MovieLens 100K 数据集和抽样亚马逊评论数据集上进行的广泛实验证明了所提出的方法在推荐系统中检测托攻击的有效性。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | 2019-04                                                                                                                                                                                 | 2024-10-28 09:41:15                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:19:58                                                                                                                                  | 2024-10-28 09:41:15                                                                                                                                                                                                                                                                                                                                 | 579-597                                                                                                                                                                                          | citationNumber: 15; ccfInfo: CCF-B                  | /unread; shilling attacks; shilling attack detection; collaborative filtering recommender systems; behaviour features; item relationship; target item identification                                                                                                                                                                                                                                                                   |
| Detecting shilling attacks in recommender systems based on analysis of user rating behavior                                                                     | Cai, Hongyun; Zhang, Fuzhi                                                                                                                                              | 2019             | journalArticle  | Knowledge-Based Systems                                                                                                                                                                             | https://linkinghub.elsevier.com/retrieve/pii/S0950705119301601                                                                                                                                                                                                                |                                     | 09507051                        | 10.1016/j.knosys.2019.04.001          | The existing unsupervised methods for detecting shilling attacks are mostly based on the rating patterns of users, ignoring the rating behavior difference between genuine users and attack users, and these methods suffer from low accuracy in detecting various shilling attacks without a priori knowledge of the attacks. To address these limitations, we propose a novel unsupervised shilling attack detection model based on an analysis of user rating behavior. First, we identify the target item(s) and the corresponding intentions of the attack users by analyzing the deviation of rating tendencies on each item, and based on this analysis, a set of suspicious users is constructed. Second, we analyze the users’ rating behaviors from an interest preference and rating preference perspective. In particular, we measure the diversity and memory of users’ interest preferences by entropy and block entropy, respectively, and we analyze the memory of user rating preferences by a self-correlation analysis. Finally, we calculate the suspicious degree and spot attack users in the set of suspicious users based on measurements of user rating behavior. Experimental results on the Netflix dataset, the MovieLens 1M dataset and the sampled Amazon review dataset demonstrate the effectiveness of the proposed detection model. 现有的无监督托攻击检测方法大多基于用户的评分模式，忽略了真实用户与攻击用户之间的评分行为差异，这些方法在没有攻击先验知识的情况下，存在检测各种托攻击准确率低的问题。针对这些局限性，我们提出了一种基于用户评分行为分析的新型无监督托客攻击检测模型。首先，我们通过分析每个项目的评分倾向偏差来确定目标项目和相应的攻击用户意图，并在此基础上构建可疑用户集。其次，我们从兴趣偏好和评分偏好的角度分析用户的评分行为。其中，我们分别通过熵和块熵来衡量用户兴趣偏好的多样性和记忆性，并通过自相关分析来分析用户评分偏好的记忆性。最后，我们基于对用户评分行为的测量，计算可疑程度并在可疑用户集中发现攻击用户。在 Netflix 数据集、MovieLens 100 万数据集和亚马逊评论抽样数据集上的实验结果证明了所提出的检测模型的有效性。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | 2019-08                                                                                                                                                                                 | 2024-10-25 05:27:33                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:23                                                                                                                                  | 2024-10-25 05:27:33                                                                                                                                                                                                                                                                                                                                 | 22-43                                                                                                                                                                                            | ccfInfo: CCF-C KBS; citationNumber: 31              | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Improving the Shilling Attack Detection in Recommender Systems Using an SVM Gaussian Mixture Model                                                              | Alostad, Jasem M.                                                                                                                                                       | 2019             | journalArticle  | Journal of Information & Knowledge Management (JIKM)                                                                                                                                                | https://ideas.repec.org//a/wsi/jikmxx/v18y2019i01ns0219649219500114.html                                                                                                                                                                                                      |                                     |                                 |                                       | With recent advances in e-commerce platforms, the information overload has grown due to increasing number of users, rapid generation of data and items in the recommender system. This tends to create serious problems in such recommender systems. The increasing features in recommender systems pose some new challenges due to poor resilience to mitigate against vulnerable attacks. In particular, the recommender systems are more prone to be attacked by shilling attacks, which creates more vulnerability. A recommender system with poor detection of attacks leads to a reduced detection rate. The performance of the recommender system is thus affected with poor detection ability. Hence, in this paper, we improve the resilience against shilling attacks using a modified Support Vector Machine (SVM) and a machine learning algorithm. The Gaussian Mixture Model is used as a machine learning algorithm to increase the detection rate and it further reduces the dimensionality of data in recommender systems. The proposed method is evaluated against several result metrics, such as the recall rate, precision rate and false positive rate between different attacks. The results of the proposed system are evaluated against probabilistic recommender approaches to demonstrate the efficacy of machine learning language in recommender systems. 随着电子商务平台的不断发展，用户数量不断增加，推荐系统中的数据和项目迅速生成，导致信息超载。这往往会给此类推荐系统带来严重问题。推荐系统不断增加的功能带来了一些新的挑战，因为它们在抵御易受攻击方面的复原能力较差。特别是，推荐器系统更容易受到托攻击，这就造成了更多的漏洞。攻击检测能力差的推荐系统会导致检测率降低。因此，检测能力差会影响推荐系统的性能。因此，在本文中，我们利用改进的支持向量机（SVM）和机器学习算法提高了对托攻击的抵御能力。高斯混合模型被用作一种机器学习算法，以提高检测率，并进一步降低推荐系统中的数据维度。所提出的方法根据几个结果指标进行了评估，如不同攻击之间的召回率、精确率和误报率。建议系统的结果与概率推荐方法进行了对比评估，以证明机器学习语言在推荐系统中的功效。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 2019                                                                                                                                                                                    | 2024-10-28 09:38:50                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:26                                                                                                                                  | 2024-10-28 09:38:50                                                                                                                                                                                                                                                                                                                                 | 1-18                                                                                                                                                                                             | citationNumber: 6; ccfInfo: CCF-None JIKM           | /unread; shilling attacks; recommender systems; attack profile; Gaussian mixture model; support vector machine                                                                                                                                                                                                                                                                                                                         |
| Uncovering anomalous rating behaviors for rating systems                                                                                                        | Yang, Zhihai; Sun, Qindong; Zhang, Yaling; Zhang, Beibei                                                                                                                | 2018             | journalArticle  | Neurocomputing                                                                                                                                                                                      | https://www.sciencedirect.com/science/article/pii/S0925231218305228                                                                                                                                                                                                           |                                     | 0925-2312                       | 10.1016/j.neucom.2018.05.001          | Personalization collaborative filtering recommendation plays a key component in online rating systems, which also suffers from profile injection attacks in reality. Although anomalous rating detection for online rating systems has attracted increasing attention in recent years, detection performance of the existing methods has not reached an end. Eliminating the impact of interfering information on anomaly detection is a crucial issue for reducing false alarm rates. Moreover, detecting anomalous ratings for unlabeled and real-world data is always a big challenge. In this paper, we investigate a two-stage detection framework to spot anomalous rating profiles. Firstly, interfering rating profiles are determined by comprehensively analyzing the distributions of user activity, item popularity and special ratings in order to eliminate sparse ratings. Based on the reserved rating profiles, combining target item analysis and non-linear structure clustering is then adopted to further determine the concerned attackers. Extensive experimental comparisons in diverse attacks demonstrate the effectiveness of the proposed method compared with competing benchmarks. Additionally, discovering interesting findings including anomalous ratings and items on two real-world datasets, Amazon and TripAdvisor, is also investigated. 个性化协同过滤推荐在在线评分系统中扮演着重要角色，而在现实中，在线评分系统也饱受个人资料注入攻击的困扰。尽管近年来在线评分系统的异常评分检测越来越受到关注，但现有方法的检测性能并不尽如人意。消除干扰信息对异常检测的影响是降低误报率的关键问题。此外，检测无标记和真实世界数据中的异常评分始终是一个巨大的挑战。在本文中，我们研究了一种两阶段检测框架来发现异常评分档案。首先，通过全面分析用户活跃度、项目流行度和特殊评分的分布，确定干扰评分档案，以消除稀疏评分。在保留评分档案的基础上，结合目标项目分析和非线性结构聚类，进一步确定相关攻击者。在各种攻击中进行的大量实验比较表明，与竞争基准相比，所提出的方法非常有效。此外，研究还发现了一些有趣的发现，包括亚马逊和 TripAdvisor 这两个真实数据集上的异常评分和项目。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | 2018-09-25                                                                                                                                                                              | 2024-10-25 06:02:46                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:28                                                                                                                                  | 2024-10-25 06:02:46                                                                                                                                                                                                                                                                                                                                 | 205-226                                                                                                                                                                                          | ccfInfo: CCF-C; citationNumber: 14                  | /unread; Shilling attack; Recommender system; Abnormality forensics; Outlier detection                                                                                                                                                                                                                                                                                                                                                 |
| UD-HMM: An unsupervised method for shilling attack detection based on hidden Markov model and hierarchical clustering                                           | Zhang, Fuzhi; Zhang, Zening; Zhang, Peng; Wang, Shilei                                                                                                                  | 2018             | journalArticle  | Knowledge-Based Systems                                                                                                                                                                             | https://www.sciencedirect.com/science/article/pii/S0950705118300959                                                                                                                                                                                                           |                                     | 0950-7051                       | 10.1016/j.knosys.2018.02.032          | The existing unsupervised methods usually require a prior knowledge to ensure the performance when detecting shilling attacks in collaborative filtering recommender systems. To address this limitation, in this paper we propose an unsupervised method to detect shilling attacks based on hidden Markov model and hierarchical clustering. We first use hidden Markov model to model user's history rating behaviors and calculate each user's suspicious degree by analyzing the user's preference sequence and the difference between genuine and attack users in rating behaviors. Then we use the hierarchical clustering method to group users according to user's suspicious degree and obtain the set of attack users. The experimental results on the MovieLens 1 M and Netflix datasets show that the proposed method outperforms the baseline methods in detection performance. 在协同过滤推荐系统中检测托攻击时，现有的无监督方法通常需要先验知识才能确保性能。针对这一局限，本文提出了一种基于隐马尔可夫模型和分层聚类的无监督方法来检测托儿攻击。我们首先使用隐马尔可夫模型对用户的历史评分行为进行建模，并通过分析用户的偏好序列以及真实用户和攻击用户在评分行为上的差异来计算每个用户的可疑程度。然后使用分层聚类方法根据用户的可疑程度对用户进行分组，得到攻击用户集。在 MovieLens 1 M 和 Netflix 数据集上的实验结果表明，所提出的方法在检测性能上优于基线方法。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | 2018-05-15                                                                                                                                                                              | 2024-10-25 05:57:51                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:28                                                                                                                                  | 2024-10-25 05:57:50                                                                                                                                                                                                                                                                                                                                 | 146-166                                                                                                                                                                                          | ccfInfo: CCF-C KBS; citationNumber: 62              | /unread; Shilling attack detection; Collaborative filtering recommender systems; Hidden Markov model; Hierarchical clustering; Shilling attacks; User rating behavior                                                                                                                                                                                                                                                                  |
| Quick and accurate attack detection in recommender systems through user attributes                                                                              | Aktukmak, Mehmet; Yilmaz, Yasin; Uysal, Ismail                                                                                                                          | 2019             | conferencePaper | Proceedings of the 13th ACM Conference on Recommender Systems                                                                                                                                       | https://dl.acm.org/doi/10.1145/3298689.3347050                                                                                                                                                                                                                                | 978-1-4503-6243-6                   |                                 | 10.1145/3298689.3347050               | Malicious profles have been a credible threat to collaborative recommender systems. Attackers provide fake item ratings to systematically manipulate the platform. Attack detection algorithms can identify and remove such users by observing rating distributions. In this study, we aim to use the user attributes as an additional information source to improve the accuracy and speed of attack detection. We propose a probabilistic factorization model which can embed mixed data type user attributes and observed ratings into a latent space to generate anomaly statistics for new users. To identify the persistent outliers in the system, we also propose a sequential attack detection algorithm to enable quick and accurate detection based on the probabilistic model learned from genuine users. The proposed model demonstrates signifcant improvements in both accuracy and speed when compared to baseline algorithms on a popular benchmark dataset. 恶意破坏一直是协作推荐系统面临的可靠威胁。攻击者提供虚假的项目评级，系统地操纵平台。攻击检测算法可以通过观察评分分布来识别和删除此类用户。在本研究中，我们旨在将用户属性作为额外的信息源，以提高攻击检测的准确性和速度。我们提出了一种概率因式分解模型，它可以将用户属性和观察到的评分混合数据嵌入一个潜在空间，从而生成新用户的异常统计数据。为了识别系统中持续存在的异常值，我们还提出了一种顺序攻击检测算法，以便根据从真实用户身上学习到的概率模型进行快速、准确的检测。在一个流行的基准数据集上，与基线算法相比，所提出的模型在准确性和速度上都有显著提高。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 2019-09-10                                                                                                                                                                              | 2024-10-25 05:53:30                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:32                                                                                                                                  | 2024-10-25 05:53:30                                                                                                                                                                                                                                                                                                                                 | 348-352                                                                                                                                                                                          | ccfInfo: CCF-B RecSys; citationNumber: 27           | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Detection of Shilling Attack Based on Bayesian Model and User Embedding                                                                                         | Yang, Fan; Gao, Min; Yu, Junliang; Song, Yuqi; Wang, Xinyi                                                                                                              | 2018             | conferencePaper | 2018 IEEE 30th International Conference on Tools with Artificial Intelligence (ICTAI)                                                                                                               | https://ieeexplore.ieee.org/document/8576100/?arnumber=8576100                                                                                                                                                                                                                |                                     |                                 | 10.1109/ICTAI.2018.00102              | The recommendation systems have been widely employed due to the effectiveness on mitigating the information overload issue. At present, the recommendation systems have made great progress, but they are under the threat of shilling attack because of their open nature. Shilling attack is the way by which the attackers can manipulate the recommendation results and cause great harm to recommendation systems. Existing shilling attack detection models are mainly based on statistical measures to extract features like the rating deviation, which are generally susceptible to attack strategies. Once the attacker changes attack strategy, the detection model which is based on the statistical method may fail. Some researchers have identified that implicit features hidden in user-user interactions and user-item interactions can be utilized to solve the problem. Their research aims to learn potential relationship between users to update features. However, the research ignores the significance of learning features by employing label information. To solve this problem, in this paper, we propose a novel detection model, named BayesDetector, which takes not only the user-user and user-item interactions but also the label information into consideration in the process of learning user implicit features. Furthermore, to take full advantage of user labels, the Bayesian model is added to the feature learning. Experiments on two datasets, Amazon and Movielens, show that BayesDetector significantly outperforms the state-of-the-art methods. 由于推荐系统能有效缓解信息过载问题，因此被广泛采用。目前，推荐系统取得了长足的进步，但由于其开放性，也面临着 “托 ”攻击的威胁。托攻击是攻击者操纵推荐结果并对推荐系统造成巨大危害的一种方式。现有的托客攻击检测模型主要基于统计量来提取评分偏差等特征，这些特征一般容易受到攻击策略的影响。一旦攻击者改变攻击策略，基于统计方法的检测模型就可能失效。一些研究人员发现，可以利用隐藏在用户-用户交互和用户-物品交互中的隐含特征来解决问题。他们的研究旨在学习用户之间的潜在关系，从而更新特征。然而，这些研究忽略了利用标签信息学习特征的意义。为了解决这个问题，我们在本文中提出了一种名为贝叶斯检测器（BayesDetector）的新型检测模型，该模型在学习用户隐含特征的过程中不仅考虑了用户与用户、用户与物品之间的交互，还考虑了标签信息。此外，为了充分利用用户标签，贝叶斯模型也被加入到特征学习中。在亚马逊和 Movielens 两个数据集上的实验表明，BayesDetector 的性能明显优于最先进的方法。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | 2018-11                                                                                                                                                                                 | 2024-10-25 05:59:15                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:31                                                                                                                                  | 2024-10-25 05:59:15                                                                                                                                                                                                                                                                                                                                 | 639-646                                                                                                                                                                                          | citationNumber: 13; ccfInfo: CCF-C ICTAI            | /unread; Computational modeling; Feature extraction; Unsupervised learning; Principal component analysis; Bayes methods; Matrix decomposition; Recommendation System, Shilling Attack Detection, Bayesian Analysis, User Embedding                                                                                                                                                                                                     |
| Abnormal Item Detection Based on Time Window Merging for Recommender Systems                                                                                    | Qi, Ling Tao; Huang, Hai Ping; Wang, Peng; Wang, Ru Chuan                                                                                                               | 2018             | conferencePaper | 2018 17th IEEE International Conference On Trust, Security And Privacy In Computing And Communications/ 12th IEEE International Conference On Big Data Science And Engineering (TrustCom/BigDataSE) | https://ieeexplore.ieee.org/document/8455915/?arnumber=8455915                                                                                                                                                                                                                |                                     |                                 | 10.1109/TrustCom/BigDataSE.2018.00047 | CFRS (Collaborative Filtering Recommendation System) is one of the most widely used individualized recommendation systems. However, CFRS is susceptible to shilling attacks based on profile injection. The current research on shilling attack mainly focuses on the recognition of false user profiles, but these methods depend on the specific attack models and the computational cost is huge. From the view of item, some abnormal item detection methods are proposed which are independent of attack models and overcome the defects of user profiles model, but its detection rate, false alarm rate and time overhead need to be further improved. In order to solve these problems, it proposes an abnormal item detection method based on time window merging. This method first uses the small window to partition rating time series, and determine whether the window is suspicious in terms of the number of abnormal ratings within it. Then, the suspicious small windows are merged to form suspicious intervals. We use the rating distribution characteristics RAR (Ratio of Abnormal Rating), ATIAR (Average Time Interval of Abnormal Rating), DAR(Deviation of Abnormal Rating) and DTIAR (Deviation of Time Interval of Abnormal Rating) in the suspicious intervals to determine whether the item is subject to attacks. Experiment results on the MovieLens 100K data set show that the method has a high detection rate and a low false alarm rate. CFRS（协同过滤推荐系统）是应用最广泛的个性化推荐系统之一。然而，CFRS 容易受到基于档案注入的托攻击。目前针对托攻击的研究主要集中在虚假用户配置文件的识别上，但这些方法依赖于特定的攻击模型，计算成本巨大。从项目的角度，提出了一些独立于攻击模型的异常项目检测方法，克服了用户配置文件模型的缺陷，但其检测率、误报率和时间开销有待进一步提高。为了解决这些问题，本文提出了一种基于时间窗口合并的异常条目检测方法。该方法首先使用小窗口分割评分时间序列，并根据窗口内异常评分的数量判断窗口是否可疑。然后，合并可疑的小窗口，形成可疑区间。我们利用可疑区间中的评分分布特征 RAR（异常评分比率）、ATIAR（异常评分平均时间间隔）、DAR（异常评分偏差）和 DTIAR（异常评分时间间隔偏差）来判断项目是否受到攻击。在 MovieLens 100K 数据集上的实验结果表明，该方法具有较高的检测率和较低的误报率。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | 2018-08                                                                                                                                                                                 | 2024-10-25 06:00:59                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:19:56                                                                                                                                  | 2024-10-25 06:00:59                                                                                                                                                                                                                                                                                                                                 | 252-259                                                                                                                                                                                          | citationNumber: 1; ccfInfo: CCF-C TrustCom          | /unread; shilling attacks; recommender systems; Big Data; Conferences; Security; abnormal item detection; Data privacy; distribution characteristics; Handheld computers; time window merging                                                                                                                                                                                                                                          |
| A shilling attack detector based on convolutional neural network for collaborative recommender system in social aware network                                   | Tong, Chao; Yin, Xiang; Li, Jun; Zhu, Tongyu; Lv, Renli; Sun, Liang; Rodrigues, Joel J P C                                                                              | 2018             | journalArticle  | The Computer Journal                                                                                                                                                                                | https://academic.oup.com/comjnl/article/61/7/949/4835634                                                                                                                                                                                                                      |                                     | 0010-4620, 1460-2067            | 10.1093/comjnl/bxy008                 | One of the most fundamental tasks in the socially aware network (SAN) paradigm is to explore the attributes and behavior of users, which helps to design more suitable and efficient protocols. Particularly, detection of shilling attackers by mining users’ behavior is a frequently discussed topic in many social scenes like recommender systems based on collaborative filtering. As the per formances of collaborative filtering are entirely based on ratings provided by users, they are vul nerable to shilling attacks which perform injection of biased profiles into rating databases to alter the systems. Current shilling attack detection methods detect spam users through artificially designed features, which are neither robust nor efficient enough. This paper illustrates a novel con volutional neural network-based method named CNN-SAD, which applies transformed network structure to exploit deep-level features from users rating profiles. Since the achieved deep-level fea tures elaborate users rating more precisely than artificially designed features, CNN-SAD can detect shilling attacks more efficiently. According to the experimental results, the proposed method is capable of detecting the vast majority of obfuscated attacks precisely and outperforms other state-of-the-art algorithms, which contributes to applications and security in SAN. 社会感知网络（SAN）范式中最基本的任务之一是探索用户的属性和行为，这有助于设计更合适、更高效的协议。特别是，通过挖掘用户行为来检测托攻击者是许多社交场景（如基于协同过滤的推荐系统）中经常讨论的话题。由于协同过滤的性能完全基于用户提供的评分，因此很容易受到 “托”（shilling）攻击的影响。“托 ”会在评分数据库中注入有偏见的资料，从而改变系统。目前的 “托 ”攻击检测方法是通过人为设计的特征来检测垃圾用户，这种方法既不可靠也不够高效。本文阐述了一种名为 CNN-SAD 的基于计算神经网络的新方法，该方法采用转换网络结构来利用用户评分档案中的深层特征。由于所获得的深层特征比人工设计的特征更精确地阐述了用户的评分，因此 CNN-SAD 可以更有效地检测托攻击。实验结果表明，所提出的方法能够精确检测绝大多数混淆攻击，其性能优于其他最先进的算法，为 SAN 的应用和安全做出了贡献。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | 2018-07-01                                                                                                                                                                              | 2024-10-25 05:54:59                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:19:55                                                                                                                                  | 2024-10-25 05:54:59                                                                                                                                                                                                                                                                                                                                 | 949-958                                                                                                                                                                                          | ccfInfo: CCF-B; citationNumber: 44                  | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Toward Adversarially Robust Recommendation From Adaptive Fraudster Detection | IEEE Transactions on Information Forensics and Security                          |                                                                                                                                                                         |                  | webpage         |                                                                                                                                                                                                     | https://dl.acm.org/doi/10.1109/TIFS.2023.3327876                                                                                                                                                                                                                              |                                     |                                 |                                       | The robustness of recommender systems under node injection attacks has garnered significant attention. Recently, GraphRfi, a Graph-Neural-Network-based (GNN-based) recommender system, was proposed and shown to effectively mitigate the impact of injected fake users. However, we demonstrate that GraphRfi remains vulnerable to attacks due to the supervised nature of its fraudster detection component, where obtaining clean labels is challenging in practice. In particular, we propose a powerful poisoning attack, MetaC, against both GNN-based and Martix-Faxtorization-based recommender systems. Furthermore, we analyze why GraphRfi fails under such an attack. Then, based on our insights obtained from vulnerability analysis, we design an adaptive fraudster detection module that explicitly considers label uncertainty. This module can serve as a plug-in for different recommender systems, resulting in a robust framework named Posterior-Detection Recommender (PDR). Comprehensive experiments show that our defense approach outperforms other benchmark methods under attacks. Overall, our research presents an effective framework for integrating fraudster detection into recommendation systems to achieve adversarial robustness. 推荐系统在节点注入攻击下的鲁棒性引起了广泛关注。最近，一种基于图形神经网络（GNN）的推荐系统 GraphRfi 被提出，并被证明能有效减轻注入的虚假用户的影响。然而，我们证明了 GraphRfi 仍然容易受到攻击，这是因为其欺诈者检测组件具有监督性质，在实践中获得干净的标签具有挑战性。我们特别针对基于 GNN 和基于 Martix-Faxtorization 的推荐系统提出了一种强大的中毒攻击 MetaC。此外，我们还分析了 GraphRfi 在这种攻击下失效的原因。然后，根据我们从漏洞分析中获得的见解，我们设计了一个自适应欺诈者检测模块，该模块明确考虑了标签的不确定性。该模块可以作为不同推荐系统的插件，形成一个名为后验检测推荐器（PDR）的稳健框架。综合实验表明，在受到攻击时，我们的防御方法优于其他基准方法。总之，我们的研究提出了一个有效的框架，可将欺诈检测集成到推荐系统中，以实现对抗鲁棒性。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |                                                                                                                                                                                         | 2024-10-29 01:15:40                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:18                                                                                                                                  | 2024-10-29 01:15:40                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                  | ccfInfo: Not Found; citationNumber: Not Found       | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| RETRACTED ARTICLE: Detection of shilling attack in recommender system for YouTube video statistics using machine learning techniques | Soft Computing           |                                                                                                                                                                         |                  | webpage         |                                                                                                                                                                                                     | https://link.springer.com/article/10.1007/s00500-021-05586-8                                                                                                                                                                                                                  |                                     |                                 |                                       | Literature survey shows that the recommendation systems have been largely adapted and evaluated in various domains. Due to low performances from various cyber attacks, the adoption of recommender system is in the initial stage of defense systems. One of the most common attacks for recommender system is shilling attack. There are some existing techniques for identifying the shilling attacks built in the user ratings patterns. The performance of ratings on target items differs between the attack user profiles and actual user profiles. To differentiate the certain profiles, the affected profiles are known as attack profiles. Besides the shilling attacks, real cyber attacks are taking place in the community which are being solved by Petri Net methods. These attacks can be falsely predicted (shilling attacks) by the users which can raise security threats. For identifying various shilling attacks without a priori knowledge, Recommendation System suffers from low accuracy. Basically, recommendation attack is split into nuke and push attack that encourage and discourage the recommended target item. The strength of shilling attack is usually measured by filler size and attack size. An experiment over unsupervised machine learning algorithms with filler size 3% over 3%, 5%, 8% and 10% attack sizes is presented for Netflix dataset. Furthermore, we conducted an experiment on data of 26 K videos on the Trending YouTube Video Statistics, to predict the user preferences for a particular genre of videos using Machine Learning Algorithms. Based on the results, it observed that the Boosted Decision tree performs the best with an accuracy of 99 percent. 文献调查显示，推荐系统已在各个领域得到广泛应用和评估。由于各种网络攻击的影响较小，推荐系统的应用还处于防御系统的初级阶段。托攻击是推荐系统最常见的攻击之一。现有的一些技术可以识别建立在用户评级模式中的托儿攻击。受攻击的用户配置文件和实际用户配置文件对目标项目的评分表现不同。为了区分某些用户配置文件，受影响的配置文件被称为攻击配置文件。除了 “托 ”攻击，社会上还发生了一些真实的网络攻击，这些攻击都是通过 Petri Net 方法解决的。用户可以错误地预测这些攻击（托儿攻击），从而引发安全威胁。为了在没有先验知识的情况下识别各种托攻击，推荐系统的准确率很低。从根本上说，推荐攻击分为 “核弹 ”攻击和 “推送 ”攻击。托攻击的强度通常用填充大小和攻击大小来衡量。我们以 Netflix 数据集为例，对填充大小为 3%、攻击大小为 3%、5%、8% 和 10%的无监督机器学习算法进行了实验。此外，我们还对 YouTube 趋势视频统计中的 26 K 个视频数据进行了实验，使用机器学习算法预测用户对特定类型视频的偏好。根据实验结果，我们发现助推决策树的准确率高达 99%，表现最佳。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |                                                                                                                                                                                         | 2024-10-29 00:53:04                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:14                                                                                                                                  | 2024-10-29 00:53:04                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                  | ccfInfo: Not Found; citationNumber: Not Found       | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Unorganized Malicious Attacks Detection                                                                                                                         | Pang, Ming; Gao, Wei; Tao, Min; Zhou, Zhi-Hua                                                                                                                           | 2018             | conferencePaper | Advances in Neural Information Processing Systems                                                                                                                                                   | https://proceedings.neurips.cc/paper_files/paper/2018/hash/322f62469c5e3c7dc3e58f5a4d1ea399-Abstract.html                                                                                                                                                                     |                                     |                                 |                                       | Recommender systems have attracted much attention during the past decade. Many attack detection algorithms have been developed for better recommendations, mostly focusing on shilling attacks, where an attack organizer produces a large number of user profiles by the same strategy to promote or demote an item. This work considers another different attack style: unorganized malicious attacks, where attackers individually utilize a small number of user profiles to attack different items without organizer. This attack style occurs in many real applications, yet relevant study remains open. We formulate the unorganized malicious attacks detection as a matrix completion problem, and propose the Unorganized Malicious Attacks detection (UMA) algorithm, based on the alternating splitting augmented Lagrangian method. We verify, both theoretically and empirically, the effectiveness of the proposed approach. 过去十年间，推荐系统备受关注。为了获得更好的推荐效果，人们开发了许多攻击检测算法，这些算法大多集中在托攻击上，即攻击组织者通过相同的策略制作大量用户配置文件，以晋升或降级某个项目。这项工作考虑了另一种不同的攻击方式：无组织恶意攻击，即攻击者在没有组织者的情况下单独利用少量用户配置文件攻击不同的项目。这种攻击方式在许多实际应用中都会出现，但相关研究仍未结束。我们将无组织恶意攻击检测表述为矩阵完成问题，并提出了基于交替分裂增强拉格朗日法的无组织恶意攻击检测（UMA）算法。我们从理论和经验两方面验证了所提方法的有效性。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | 2018                                                                                                                                                                                    | 2024-10-29 00:59:45                                                                                                                                                                                                                                                                                                                                       | 2024-12-06 23:18:40                                                                                                                                  | 2024-10-29 00:59:45                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                  | ccfInfo: CCF-A NeurIPS; citationNumber: 14          | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Unsupervised approach for detecting shilling attacks in collaborative recommender systems based on user rating behaviours | IET Information Security            |                                                                                                                                                                         |                  | webpage         |                                                                                                                                                                                                     | https://digital-library.theiet.org/doi/10.1049/iet-ifs.2018.5131                                                                                                                                                                                                              |                                     |                                 |                                       | Collaborative recommender systems have been known to be extremely vulnerable to shilling attacks. To prevent such attacks, many detection approaches including supervised and unsupervised have been proposed. However, the supervised approaches are only suitable for detecting known types of attacks and the unsupervised approaches require a priori knowledge to ensure the detection performance. To address the limitations, the authors propose an unsupervised approach for detecting shilling attacks based on user rating behaviours. They first use Gibbs latent Dirichlet allocation model to extract latent topics of user preferences from user rating item sequences, then they use mixture transition distribution model to construct the user's preference model and present several metrics to capture the diversity between genuine and attack users in rating behaviours. In the case of unknown attack size, the number of attack users is obtained by analysing the critical point of rating behaviour suspicious degrees between genuine and attack users, and based on which the attack users are identified. The experimental results on the MovieLens 1 M dataset show that the proposed approach outperforms the baseline methods in terms of recall and precision metrics. 众所周知，协同推荐系统极易受到托攻击。为了防止这种攻击，人们提出了许多检测方法，包括有监督和无监督方法。然而，有监督方法只适用于检测已知类型的攻击，而无监督方法则需要先验知识来确保检测性能。针对上述局限性，作者提出了一种基于用户评分行为的无监督方法来检测托攻击。他们首先使用 Gibbs 潜在 Dirichlet 分配模型从用户评分项目序列中提取用户偏好的潜在主题，然后使用混合转换分布模型构建用户偏好模型，并提出了几个指标来捕捉真实用户和攻击用户在评分行为上的多样性。在攻击规模未知的情况下，通过分析真实用户和攻击用户之间评分行为可疑度的临界点，得出攻击用户的数量，并据此识别攻击用户。在 MovieLens 1 M 数据集上的实验结果表明，所提出的方法在召回率和精确度指标方面优于基线方法。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                         | 2024-10-28 09:43:14                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:30                                                                                                                                  | 2024-10-28 09:43:14                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                  | ccfInfo: Not Found; citationNumber: Not Found       | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Unsupervised Shilling Attack Detection Model Based on Rated Item Correlation Analysis                                                                           | CHEN, KEKE; K. CHAN, PATRICK P.; YEUNG, DANIEL S.                                                                                                                       | 2018             | conferencePaper | 2018 International Conference on Machine Learning and Cybernetics (ICMLC)                                                                                                                           | https://ieeexplore.ieee.org/document/8526971/?arnumber=8526971                                                                                                                                                                                                                |                                     |                                 | 10.1109/ICMLC.2018.8526971            | Collaborative filtering is one of the most effective methods to deal with the information overload problem by providing a suitable recommendation to users. Recent research indicates that collaborative filtering is vulnerable to shilling attack which aims at manipulating the recommendation result through injecting malicious profiles. Our previous study suggests that applying the rated item correlation to supervised learning increases the accuracy of shilling attack detection. However, label information collection is one drawback of the supervised learning. In this study, an unsupervised detection based on the rated item correlation analysis is devised. The influence of the parameters on the detection accuracy is also discussed. The experimental results demonstrate that our proposed unsupervised detection model achieve a satisfying performance in shilling detection in MovieLens 100K dataset. 协同过滤是通过向用户提供合适的推荐来解决信息过载问题的最有效方法之一。最近的研究表明，协同过滤容易受到 “托 ”攻击的影响。“托 ”攻击的目的是通过注入恶意资料来操纵推荐结果。我们之前的研究表明，将评级项相关性应用于监督学习可提高托攻击检测的准确性。然而，标签信息收集是监督学习的一个缺点。本研究设计了一种基于评分项相关性分析的无监督检测方法。同时还讨论了参数对检测精度的影响。实验结果表明，我们提出的无监督检测模型在 MovieLens 100K 数据集中的托检测中取得了令人满意的性能。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | 2018-07                                                                                                                                                                                 | 2024-10-25 06:01:58                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:30                                                                                                                                  | 2024-10-25 06:01:58                                                                                                                                                                                                                                                                                                                                 | 667-672                                                                                                                                                                                          | citationNumber: 1; ccfInfo: CCF-None ICMLC          | /unread; Collaborative filtering; Recommender systems; Analytical models; Correlation; Feature extraction; Rated item correlation; Shilling attack; Unsupervised learning; Databases; Mathematical model                                                                                                                                                                                                                               |
| Shilling Attack Detection Using Rated Item Correlation for Collaborative Filtering                                                                              | Chen, Keke; Chan, Patrick P. K.; Yeung, Daniel S.                                                                                                                       | 2018             | conferencePaper | 2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)                                                                                                                           | https://ieeexplore.ieee.org/document/8616598/?arnumber=8616598                                                                                                                                                                                                                |                                     |                                 | 10.1109/SMC.2018.00601                | Collaborative filtering (CF) is vulnerable under shilling attack, which misleads recommendation of CF by injecting well-crafted profiles to a targeted system. Although a number of supervised learning based shilling attack detection methods are proposed, their features mainly measure rating values and items of a profile individually, but ignore the relation between items. This study aims to enhance the robustness of CF against shilling attack by considering the rated item correlation. Real users rate items based on their preferences, but rated items are randomly selected for malicious users profiles in most shilling attack. Therefore, the rated item correlation of real and malicious profiles is different. Three features are proposed to capture the information from different intervals of the distribution of rated item correlation in terms of Cosine Association (CA). A benchmark dataset, MovieLens 100K, is used to evaluate the proposed features. The discrimination ability of the proposed features is also illustrated. The experimental results suggest that the proposed features have significant contribution on shilling attack detection. 协同过滤（CF）很容易受到 “托”（shilling）攻击，这种攻击通过向目标系统注入精心制作的档案来误导协同过滤的推荐。虽然提出了一些基于监督学习的托儿攻击检测方法，但它们的特征主要是单独测量档案的评分值和项目，而忽略了项目之间的关系。本研究旨在通过考虑评分项的相关性来增强 CF 对托攻击的鲁棒性。真实用户根据自己的偏好对项目进行评分，但在大多数托攻击中，恶意用户配置文件中的评分项目都是随机选择的。因此，真实用户和恶意用户的评分项目相关性是不同的。我们提出了三种特征，以余弦关联（CA）捕捉评级项目相关性分布中不同区间的信息。基准数据集 MovieLens 100K 用于评估所提出的特征。实验还说明了所提出特征的辨别能力。实验结果表明，所提出的特征在托攻击检测方面具有显著的贡献。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 2018-10                                                                                                                                                                                 | 2024-10-25 06:01:23                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:16                                                                                                                                  | 2024-10-25 06:01:23                                                                                                                                                                                                                                                                                                                                 | 3553-3558                                                                                                                                                                                        | citationNumber: 4; ccfInfo: CCF-C SMC               | /unread; Recommender systems; Correlation; Feature extraction; Unsupervised learning; Databases; Cybernetics; Supervised learning                                                                                                                                                                                                                                                                                                      |
| Detecting shilling profiles in collaborative recommender systems via multidimensional profile temporal features                                                 | Hao, Yaojun; Zhang, Fuzhi                                                                                                                                               | 2018             | journalArticle  | IET Information Security                                                                                                                                                                            | https://onlinelibrary.wiley.com/doi/abs/10.1049/iet-ifs.2017.0012                                                                                                                                                                                                             |                                     | 1751-8717                       | 10.1049/iet-ifs.2017.0012             | To defend recommender systems, various methods have been proposed to detect shilling profiles, which can be categorised as user- and item-based detection methods. Most of the user-based methods identify shilling profiles via statistical signatures of rating values and suffer from low precision when detecting different types of attacks. Most of the item-based methods use temporal information to detect the anomaly items, but they assume that the fake ratings were injected in short periods. So they are invalid for the long duration and decentralised injection attacks. To address these limitations, the authors extract the multidimensional profile temporal features and present a shilling detection method. First, from the user profile view, user rating behaviours are characterised by corrected conditional entropy and the dissimilarity with the rest-rating model. Second, from the item profile view, the user features are extracted according to item temporal popularity. Third, the features based on weighted deviation from dynamic mean are extracted according to the fact that the items mean changes with time. Finally, support vector machine is exploited to detect shilling profiles based on the proposed features. Experimental results on the Netflix dataset indicate that the performance of the proposed method is better than that of the benchmark methods. 为了保护推荐系统，人们提出了各种检测 “托 ”档案的方法，这些方法可分为基于用户的检测方法和基于项目的检测方法。大多数基于用户的方法通过评分值的统计特征来识别托儿档案，但在检测不同类型的攻击时精度较低。大多数基于项目的方法使用时间信息来检测异常项目，但它们假定虚假评级是在短时间内注入的。因此，这些方法对于持续时间长和分散的注入攻击无效。针对这些局限性，作者提取了多维档案时间特征，提出了一种托检测方法。首先，从用户配置文件来看，用户评分行为的特征是修正的条件熵和与其他评分模型的不相似性。其次，从项目概况视图中，根据项目的时间流行度提取用户特征。第三，根据项目平均值随时间变化的事实，提取基于动态平均值加权偏差的特征。最后，根据所提出的特征，利用支持向量机来检测托资料。在 Netflix 数据集上的实验结果表明，所提方法的性能优于基准方法。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | 2018                                                                                                                                                                                    | 2024-10-25 05:56:01                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:26                                                                                                                                  | 2024-10-25 05:56:01                                                                                                                                                                                                                                                                                                                                 | 362-374                                                                                                                                                                                          | ccfInfo: CCF-C; citationNumber: 5                   | /unread; collaborative filtering; recommender systems; anomaly items; benchmark methods; collaborative recommender systems; entropy; feature extraction; information filtering; item profile view; item temporal popularity; item-based detection methods; multidimensional profile temporal features; shilling detection method; shilling profiles; support vector machines; user features; user profile view; user rating behaviours |
| 无组织恶意攻击检测问题的研究                                                                                                                                                  | 庞明; 周志华                                                                                                                                                                 | 2018             | journalArticle  | 中国科学:信息科学                                                                                                                                                                                           | https://kns.cnki.net/kcms2/article/abstract?v=64ENavj7QCBEV0wZMAzu5B6kxoNxOBRGt4w8FCzBzox7buSjharzP_wW2kZWJQ5GU9wPqYSF6sUnlMnux1fX7lKwoXPwVyQxbvv6YzM-COYcEHXTcX9wqLe8vF9yNc6TIan81orLvGNB2ZCbZ4JtEHiye389uKU6FqP4bZjF5TbynvDy7Olxyepnyom3to1o&uniplatform=NZKPT&language=CHS |                                     | 1674-7267                       |                                       | 推荐系统在我们的生活中被广泛应用,对人们的生活起着越来越重要的影响.然而,协同过滤作为一种常见的推荐技术,很容易受到伪造虚假用户评分信息的恶意攻击的影响.为了保证推荐的质量,很多恶意攻击检测的方法被提出用于检测恶意攻击.现有的攻击检测方法大多是针对有组织大规模攻击的检测,即攻击者根据同一种策略,伪造大量的虚假用户评分信息用于提升或贬低一个目标物品.本文研究了一种不同的攻击类型:无组织恶意攻击,即攻击者们在没有组织的情况下,分别伪造少量的虚假用户评分信息来提升或贬低同一个目标物品.无组织恶意攻击出现在很多真实的应用中,对推荐系统的鲁棒性造成严重影响,而针对该攻击类型的研究还很初步.实验结果表明现有攻击检测方法不能够有效地检测无组织恶意攻击.本文分析了现有的多种攻击检测方法无效的原因,进而通过分析无组织恶意攻击的特性,总结出无组织恶意攻击检测的关键.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | 2018                                                                                                                                                                                    | 2024-10-25 05:05:52                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:20                                                                                                                                  | 2024-10-25 05:05:52                                                                                                                                                                                                                                                                                                                                 | 177-186                                                                                                                                                                                          | ccfInfo: Not Found; citationNumber: 14              | /unread; 推荐系统; 攻击检测; 协同过滤; 鲁棒性; 无组织恶意攻击                                                                                                                                                                                                                                                                                                                                                                                                |
| Rating behavior evaluation and abnormality - ProQuest                                                                                                           |                                                                                                                                                                         |                  | webpage         |                                                                                                                                                                                                     | https://www.proquest.com/docview/2694794715?sourcetype=Scholarly%20Journals                                                                                                                                                                                                   |                                     |                                 |                                       | Collaborative recommender systems (CRSs) have become an essential component in a wide range of e-commerce systems. However, CRSs are also easy to suffer from malicious attacks due to the fundamental vulnerability of recommender systems. Facing with the limited representative of rating behavior and the unbalanced distribution of rating profiles, how to further improve detection performance and deal with unlabeled real-world data is a longstanding but unresolved issue. This paper develops a new detection approach to defend anomalous threats for recommender systems. First, eliminating the influence of disturbed rating profiles on abnormality detection is analyzed in order to reduce the unbalanced distribution as far as possible. Based on the remaining rating profiles, secondly, rating behaviors which belong to the same dense region using standard distance measures are further partitioned by exploiting a probability mass-based dissimilarity mechanism. To reduce the scope of determining suspicious items while keeping the advantage of target item analysis (TIA), thirdly, suspected items captured by TIA are empirically converted into an associated item-item graph according to frequent patterns of rating distributions. Finally, concerned attackers can be detected based on the determined suspicious items. Extensive experiments on synthetic data demonstrate the effectiveness of the proposed detection approach compared with benchmarks. In addition, discovering interesting findings such as suspected items or ratings on four different real-world datasets is also analyzed and discussed. 协同推荐系统（CRS）已成为各种电子商务系统的重要组成部分。然而，由于推荐系统的基本弱点，CRS 也很容易遭受恶意攻击。面对评分行为的有限代表性和评分档案的不均衡分布，如何进一步提高检测性能并处理未标记的真实世界数据是一个长期存在但尚未解决的问题。本文开发了一种新的检测方法来防御推荐系统的异常威胁。首先，分析了消除受干扰评分档案对异常检测的影响，以尽可能减少不平衡分布。其次，根据剩余的评分档案，利用基于概率质量的不相似性机制，进一步划分使用标准距离度量属于同一密集区域的评分行为。第三，为了在保持目标项目分析（TIA）优势的同时缩小可疑项目的确定范围，TIA 所捕获的可疑项目会根据评分分布的频繁模式被经验性地转换成相关的项目-项目图。最后，可以根据确定的可疑项目检测出相关的攻击者。在合成数据上进行的大量实验证明，与基准相比，所提出的检测方法非常有效。此外，还分析和讨论了在四个不同的真实数据集上发现可疑项目或评级等有趣发现的情况。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                         | 2024-10-29 00:34:20                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:32                                                                                                                                  | 2024-10-29 00:34:20                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                  | ccfInfo: Not Found; citationNumber: Not Found       | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| (PDF) A novel classification-based shilling attack detection approach for multi-criteria recommender systems                                                    |                                                                                                                                                                         |                  | webpage         |                                                                                                                                                                                                     | https://www.researchgate.net/publication/371179303_A_novel_classification-based_shilling_attack_detection_approach_for_multi-criteria_recommender_systems                                                                                                                     |                                     |                                 |                                       | Recommender systems are emerging techniques guiding individuals with provided referrals by considering their past rating behaviors. By collecting multi-criteria preferences concentrating on distinguishing perspectives of the items, a new extension of traditional recommenders, multi-criteria recommender systems reveal how much a user likes an item and why user likes it; thus, they can improve predictive accuracy. However, these systems might be more vulnerable to malicious attacks than traditional ones, as they expose multiple dimensions of user opinions on items. Attackers might try to inject fake profiles into these systems to skew the recommendation results in favor of some particular items or to bring the system into discredit. Although several methods exist to defend systems against such attacks for traditional recommenders, achieving robust systems by capturing shill profiles remains elusive for multi-criteria rating-based ones. Therefore, in this study, we first consider a prominent and novel attack type, that is, the power-item attack model, and introduce its four distinct variants adapted for multi-criteria data collections. Then, we propose a classification method detecting shill profiles based on various generic and model-based user attributes, most of which are new features usually related to item popularity and distribution of rating values. The experiments conducted on three benchmark datasets conclude that the proposed method successfully detects attack profiles from genuine users even with a small selected size and attack size. The empirical outcomes also demonstrate that item popularity and user characteristics based on their rating profiles are highly beneficial features in capturing shilling attack profiles 推荐系统是一种新兴技术，它通过考虑个人过去的评分行为，为其提供推荐。多标准推荐系统是对传统推荐系统的新扩展，它通过收集多标准偏好来集中区分物品的不同视角，从而揭示用户对物品的喜爱程度以及喜爱的原因，从而提高预测的准确性。然而，与传统推荐系统相比，这些系统可能更容易受到恶意攻击，因为它们暴露了用户对物品的多个维度的意见。攻击者可能会试图向这些系统注入虚假资料，使推荐结果偏向某些特定项目，或使系统声誉受损。虽然对于传统的推荐系统来说，有多种方法可以抵御此类攻击，但对于基于多标准评级的推荐系统来说，通过捕捉虚假用户配置文件来实现稳健的系统仍然是难以实现的。因此，在本研究中，我们首先考虑了一种突出的新型攻击类型，即幂项攻击模型，并介绍了其适用于多标准数据收集的四种不同变体。然后，我们提出了一种基于各种通用用户属性和基于模型的用户属性来检测托的分类方法，其中大部分是通常与项目流行度和评分值分布相关的新特征。在三个基准数据集上进行的实验得出结论，即使所选用户规模和攻击规模较小，所提出的方法也能成功地从真实用户中检测出攻击档案。实证结果还证明，基于用户评分档案的项目流行度和用户特征是捕捉托攻击档案的非常有利的特征                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                         | 2024-10-28 09:53:36                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:28                                                                                                                                  | 2024-10-28 09:53:36                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                  | ccfInfo: Not Found; citationNumber: Not Found       | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Enhancing Adversarial Robustness of Multi-modal Recommendation via Modality Balancing | Proceedings of the 31st ACM International Conference on Multimedia      |                                                                                                                                                                         |                  | webpage         |                                                                                                                                                                                                     | https://dl.acm.org/doi/10.1145/3581783.3612337                                                                                                                                                                                                                                |                                     |                                 |                                       | Recently multi-modal recommender systems have been widely applied in real scenarios such as e-commerce businesses. Existing multi-modal recommendation methods exploit the multi-modal content of items as auxiliary information and fuse them to boost performance. Despite the superior performance achieved by multimodal recommendation models, there’s currently no understanding of their robustness to adversarial attacks. In this work, we first identify the vulnerability of existing multi-modal recommendation models. Next, we show the key reason for such vulnerability is modality imbalance, i.e., the prediction score margin between positive and negative samples in the sensitive modality will drop dramatically facing adversarial attacks and fail to be compensated by other modalities. Finally, based on this finding we propose a novel defense method to enhance the robustness of multi-modal recommendation models through modality balancing. Specifically, we first adopt an embedding distillation to obtain a pair of contentsimilar but prediction-different item embeddings in the sensitive modality and calculate the score margin reflecting the modality vulnerability. Then we optimize the model to utilize the score margin between positive and negative samples in other modalities to compensate for the vulnerability. The proposed method can serve as a plug-and-play module and is flexible to be applied to a wide range of multi-modal recommendation models. Extensive experiments on two real-world datasets demonstrate that our method significantly improves the robustness of multi-modal recommendation models with nearly no performance degradation on clean data. 最近，多模态推荐系统被广泛应用于电子商务等实际场景中。现有的多模态推荐方法利用物品的多模态内容作为辅助信息，并对其进行融合以提高性能。尽管多模态推荐模型取得了优异的性能，但目前人们还不了解它们对对抗性攻击的鲁棒性。在这项工作中，我们首先确定了现有多模态推荐模型的脆弱性。接下来，我们说明了造成这种脆弱性的关键原因是模态不平衡，即敏感模态的正负样本之间的预测得分差值在面对对抗性攻击时会急剧下降，并且无法通过其他模态得到补偿。最后，基于这一发现，我们提出了一种新颖的防御方法，通过模态平衡来增强多模态推荐模型的鲁棒性。具体来说，我们首先采用嵌入蒸馏法，在敏感模态中获得一对内容相似但预测不同的项目嵌入，并计算出反映模态脆弱性的分数余量。然后，我们对模型进行优化，利用其他模态中正样本和负样本之间的分数差来弥补模态脆弱性。所提出的方法可以作为即插即用模块，灵活地应用于各种多模态推荐模型。在两个真实世界数据集上进行的广泛实验表明，我们的方法显著提高了多模态推荐模型的鲁棒性，在干净数据上几乎没有性能下降。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                         | 2024-10-29 01:14:44                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:03                                                                                                                                  | 2024-10-29 01:14:44                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                  | ccfInfo: Not Found; citationNumber: Not Found       | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Addressing Confounding Feature Issue for Causal Recommendation | ACM Transactions on Information Systems                                                        |                                                                                                                                                                         |                  | webpage         |                                                                                                                                                                                                     | https://dl.acm.org/doi/10.1145/3559757                                                                                                                                                                                                                                        |                                     |                                 |                                       | In recommender systems, some features directly affect whether an interaction would happen, making the happened interactions not necessarily indicate user preference. For instance, short videos are objectively easier to finish even though the user may not like the video. We term such feature as confounding feature, and video length is a confounding feature in video recommendation. If we fit a model on such interaction data, just as done by most data-driven recommender systems, the model will be biased to recommend short videos more, and deviate from user actual requirement. This work formulates and addresses the problem from the causal perspective. Assuming there are some factors affecting both the confounding feature and other item features, e.g., the video creator, we find the confounding feature opens a backdoor path behind user-item matching and introduces spurious correlation. To remove the effect of backdoor path, we propose a framework named Deconfounding Causal Recommendation (DCR), which performs intervened inference with do-calculus. Nevertheless, evaluating do-calculus requires to sum over the prediction on all possible values of confounding feature, significantly increasing the time cost. To address the efficiency challenge, we further propose a mixture-of-experts (MoE) model architecture, modeling each value of confounding feature with a separate expert module. Through this way, we retain the model expressiveness with few additional costs. We demonstrate DCR on the backbone model of neural factorization machine (NFM), showing that DCR leads to more accurate prediction of user preference with small inference time cost. We release our code at: https://github.com/zyang1580/DCR. 在推荐系统中，有些特征会直接影响互动是否会发生，因此发生的互动并不一定代表用户的偏好。例如，短视频客观上更容易看完，即使用户可能不喜欢该视频。我们把这种特征称为混杂特征，视频长度就是视频推荐中的一种混杂特征。如果我们像大多数数据驱动的推荐系统那样，在这种交互数据上拟合一个模型，该模型就会偏向于推荐更多的短视频，从而偏离用户的实际需求。这项工作从因果关系的角度提出并解决了这个问题。假设混杂特征和其他项目特征（如视频创作者）都受到某些因素的影响，我们发现混杂特征在用户-项目匹配背后打开了一条后门路径，并引入了虚假相关性。为了消除后门路径的影响，我们提出了一个名为 “去混淆因果推荐”（DCR）的框架，通过 do-calculus 执行干预推理。然而，评估 do-calculus 需要对混杂特征的所有可能值进行预测求和，这大大增加了时间成本。为了解决效率难题，我们进一步提出了专家混合（MoE）模型架构，用一个单独的专家模块对每个混杂特征值进行建模。通过这种方式，我们在保留模型表现力的同时，只需付出很少的额外成本。我们在神经因数分解机（NFM）的骨干模型上演示了 DCR，结果表明 DCR 能够以较小的推理时间成本更准确地预测用户偏好。我们在以下网址发布了我们的代码：https://github.com/zyang1580/DCR。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                                                                                                                                                                         | 2024-10-29 00:51:29                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:19:57                                                                                                                                  | 2024-10-29 00:51:29                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                  | ccfInfo: Not Found; citationNumber: Not Found       | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Anti-FakeU: Defending Shilling Attacks on Graph Neural Network based Recommender Model | Proceedings of the ACM Web Conference 2023                             |                                                                                                                                                                         |                  | webpage         |                                                                                                                                                                                                     | https://dl.acm.org/doi/10.1145/3543507.3583289                                                                                                                                                                                                                                |                                     |                                 |                                       | Graph neural network (GNN) based recommendation models are observed to be more vulnerable against carefully-designed malicious  records injected into the system, i.e., shilling attacks, which manipulate the recommendation to common users and therefore impair  user trust. In this paper, we for the frst time conduct a systematic  study on the vulnerability of GNN based recommendation model  against the shilling attack. With the aid of theoretical analysis, we  attribute the root cause of the vulnerability to its neighborhood  aggregation mechanism, which could make the negative impact of  attacks propagate rapidly in the system. To restore the robustness of  GNN based recommendation model, the key factor lies in detecting  malicious records in the system and preventing the propagation  of misinformation. To this end, we construct a user-user graph to  capture the patterns of malicious behaviors and design a novel GNN  based detector to identify fake users. Furthermore, we develop a  data augmentation strategy and a joint learning paradigm to train  the recommender model and the proposed detector. Extensive experiments on benchmark datasets validate the enhanced robustness  of the proposed method in resisting various types of shilling attacks and identifying fake users, e.g., our proposed method fully  mitigating the impact of popularity attacks on target items up to  100%, and improving the accuracy of detecting fake users on the  Gowalla dataset by 10%. 据观察，基于图神经网络（GNN）的推荐模型更容易受到注入系统的精心设计的恶意记录（即托攻击）的影响，这些恶意记录会操纵向普通用户的推荐，从而损害用户的信任。在本文中，我们首次对基于 GNN 的推荐模型面对托攻击的脆弱性进行了系统研究。借助理论分析，我们将漏洞的根本原因归结于其邻域聚合机制，这种机制会使攻击的负面影响在系统中迅速传播。要恢复基于 GNN 的推荐模型的鲁棒性，关键在于检测系统中的恶意记录并防止错误信息的传播。为此，我们构建了用户用户图来捕捉恶意行为模式，并设计了一种基于 GNN 的新型检测器来识别虚假用户。此外，我们还开发了一种数据增强策略和联合学习范式，用于训练推荐模型和所提出的检测器。在基准数据集上进行的大量实验验证了所提出的方法在抵御各种类型的托攻击和识别虚假用户方面增强的鲁棒性，例如，我们所提出的方法完全缓解了人气攻击对目标项目的影响，最高可达 100%，并将 Gowalla 数据集上的虚假用户检测准确率提高了 10%。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                                                                                         | 2024-10-29 00:49:45                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:22                                                                                                                                  | 2024-10-29 00:49:45                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                  | ccfInfo: Not Found; citationNumber: Not Found       | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| A Shilling Group Detection Framework Based on Deep Learning Techniques | Security and Communication Networks                                                    |                                                                                                                                                                         |                  | webpage         |                                                                                                                                                                                                     | https://dl.acm.org/doi/10.1155/2022/2323132                                                                                                                                                                                                                                   |                                     |                                 |                                       | Research studies have shown that online recommender systems are under the threat of group shilling attacks, in which attackers attempt to distort the recommendation results of particular items by cooperatively injecting fake profiles. Existing detection methods usually divide candidate groups at first and then use the hand-crafted features to recognize shilling groups. However, the detection performance of existing methods depends highly on the quality of candidate groups obtained. Moreover, extracting features manually is time-consuming. To overcome these limitations, we propose a shilling group detection framework based on the sparse autoencoder and modified GraphSAGE model. First, we use the sparse autoencoder to obtain rating features of users from the rating dataset. Second, we analyse user collusive degrees to calculate user transition probabilities. Third, we build a user relation graph and utilize the modified GraphSAGE model to perform user classification. Finally, shilling groups are gathered according to the neighbour relations. Extensive experiment demonstrates that the proposed framework performs better on different datasets than baseline methods 研究表明，在线推荐系统正受到托群攻击的威胁，攻击者试图通过合作注入虚假资料来扭曲特定项目的推荐结果。现有的检测方法通常是先划分候选群组，然后利用手工创建的特征来识别托群。然而，现有方法的检测性能在很大程度上取决于获得的候选组的质量。此外，手工提取特征非常耗时。为了克服这些局限性，我们提出了一种基于稀疏自动编码器和改进的 GraphSAGE 模型的先令组检测框架。首先，我们使用稀疏自动编码器从评分数据集中获取用户的评分特征。其次，我们分析用户串通程度，计算用户过渡概率。第三，我们建立用户关系图，并利用修改后的 GraphSAGE 模型进行用户分类。最后，根据邻接关系收集托组。广泛的实验表明，在不同的数据集上，建议的框架比基准方法表现得更好                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |                                                                                                                                                                                         | 2024-10-29 00:36:10                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:21                                                                                                                                  | 2024-10-29 00:36:10                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                  | ccfInfo: Not Found; citationNumber: Not Found       | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| A security detection approach based on autonomy-oriented user sensor in social recommendation network - Shanshan Wan, Ying Liu, 2022                            |                                                                                                                                                                         |                  | webpage         |                                                                                                                                                                                                     | https://journals.sagepub.com/doi/full/10.1177/15501329221082415                                                                                                                                                                                                               |                                     |                                 |                                       | User social network-based recommender system has achieved significant performance in current recommendation fields. However, the characteristic of openness brings great hidden dangers to the security of recommender systems. Shilling attackers can change the recommendations by foraging user relationships. Most shilling attack detection approaches depend on the explicit user historical data to locate shilling attackers. Some important features such as information propagation and social feedback of users in social networks have not been noticed. We propose a security detection method based on autonomy-oriented user sensor (AOUSD) to identify shilling attackers. Specifically, (1) the user is simulated as a social sensor with autonomous capabilities, (2) the user interaction model is built based on information propagation, information feedback and information disappearance mechanisms of social sensors, and a user dynamic knowledge graph is formed by considering the variable time function, (3) hierarchical clustering method is used to generate preliminary suspicious candidate groups and graph community detection clustering method is applied on the dynamic knowledge graph to detect the attackers. Then, AOUSD is first simulated on NetLogo and it is compared with other algorithms based on the Amazon data. The results prove the advantages of AOUSD in the efficiency and accuracy on shilling attack detection. 基于用户社交网络的推荐系统在当前的推荐领域取得了显著的成绩。然而，开放性的特点给推荐系统的安全性带来了极大的隐患。托攻击者可以通过觅取用户关系来改变推荐结果。大多数托客攻击检测方法都依赖于明确的用户历史数据来定位托客攻击者。社交网络中用户的信息传播和社交反馈等一些重要特征尚未引起人们的注意。我们提出了一种基于自主导向用户传感器（AOUSD）的安全检测方法来识别托儿攻击者。具体来说，(1) 将用户模拟为具有自主能力的社交传感器；(2) 基于社交传感器的信息传播、信息反馈和信息消失机制建立用户交互模型，并考虑时间函数的可变性形成用户动态知识图谱；(3) 利用层次聚类方法生成初步的可疑候选群组，并在动态知识图谱上应用图社区检测聚类方法检测攻击者。然后，首先在 NetLogo 上对 AOUSD 进行了仿真，并基于亚马逊数据与其他算法进行了比较。结果证明了 AOUSD 在托攻击检测的效率和准确性上的优势。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |                                                                                                                                                                                         | 2024-10-29 00:35:12                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:21                                                                                                                                  | 2024-10-29 00:35:12                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                  | ccfInfo: Not Found; citationNumber: Not Found       | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Improving Deep Learning-Based Recommendation Attack Detection Using Harris Hawks Optimization                                                                   |                                                                                                                                                                         |                  | webpage         |                                                                                                                                                                                                     | https://www.mdpi.com/2076-3417/12/19/10135                                                                                                                                                                                                                                    |                                     |                                 |                                       | Recommendation attack attempts to bias the recommendation results of collaborative recommender systems by injecting malicious ratings into the rating database. A lot of methods have been proposed for detecting such attacks. Among these works, the deep learning-based detection methods get rid of the dependence on hand-designed features of recommendation attack besides having excellent detection performance. However, most of them optimize the key hyperparameters by manual analysis which relies too much on domain experts and their experience. To address this issue, in this paper we propose an approach based on the Harris Hawks Optimization (HHO) algorithm to improve the deep learning-based detection methods. Being different from the original detection methods which optimize the key hyperparameters manually, the improved deep learning-based detection methods can optimize the key hyperparameters automatically. We first convert the key hyperparameters of discrete type to continuous type according to the uniform distribution theory to expand the application scope of HHO algorithm. Then, we use the detection stability as an early stop condition to reduce the optimization iterations to improve the HHO algorithm. After that, we use the improved HHO algorithm to automatically optimize the key hyperparameters for the deep learning-based detection methods. Finally, we use the optimized key hyperparameters to train the deep learning-based detection methods to generate classifiers for detecting the recommendation attack. The experiments conducted on two benchmark datasets illustrate that the improved deep learning-based detection methods have effective performance. 推荐攻击试图通过向评级数据库中注入恶意评级，使协作推荐系统的推荐结果产生偏差。目前已经提出了很多检测此类攻击的方法。其中，基于深度学习的检测方法除了具有出色的检测性能外，还摆脱了对人工设计的推荐攻击特征的依赖。然而，大多数方法都是通过人工分析来优化关键超参数的，这过于依赖领域专家及其经验。针对这一问题，本文提出了一种基于哈里斯鹰优化（HHO）算法的方法，以改进基于深度学习的检测方法。与原始检测方法手动优化关键超参数不同，改进后的基于深度学习的检测方法可以自动优化关键超参数。我们首先根据均匀分布理论将离散型关键超参数转换为连续型关键超参数，以扩大 HHO 算法的应用范围。然后，以检测稳定性作为早期停止条件，减少优化迭代次数，从而改进 HHO 算法。之后，我们利用改进后的 HHO 算法为基于深度学习的检测方法自动优化关键超参数。最后，我们使用优化后的关键超参数来训练基于深度学习的检测方法，生成用于检测推荐攻击的分类器。在两个基准数据集上进行的实验表明，改进后的基于深度学习的检测方法具有有效的性能。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                         | 2024-10-29 00:34:46                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:24                                                                                                                                  | 2024-10-29 00:34:46                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                  | citationNumber: 0; ccfInfo: Not Found               | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Detecting Shilling Attacks Using Hybrid Deep Learning Models                                                                                                    |                                                                                                                                                                         |                  | webpage         |                                                                                                                                                                                                     | https://www.mdpi.com/2073-8994/12/11/1805                                                                                                                                                                                                                                     |                                     |                                 |                                       | Recommendation systems play a significant role in alleviating information overload in the digital world. They provide suggestions to users based on past symmetric activities or behaviors. Being heavily dependent on users’ behavior, they tend to be vulnerable to shilling attacks. Therefore, protecting them from attacks’ effects is highly important. As shilling attacks have features of a large number of ratings and increasing complexity in attack models, deep learning methods become proper alternatives for more accurate attack detections. This paper proposes a hybrid model of two different neural networks, convolutional and recurrent neural networks, to detect shilling attacks efficiently. The proposed deep learning model utilizes the transformed network architecture for undertaking the attributes derived from user-rated profiles. This architecture enables modeling of the temporal and spatial information in the recommendation system’s ratings. The hybrid model overcomes the limitations of the existing shilling attack deep-learning methods to enhance the recommendation systems’ efficiency and robustness. Experimental results show that the hybrid model results in better predictions on the Movie-Lens 100 K and Netflix datasets by accurately detecting most of the obfuscated attacks compared to the state-of-art deep learning algorithms used for investigation. 推荐系统在缓解数字世界信息过载方面发挥着重要作用。它们根据用户过去的对称活动或行为向用户提供建议。由于严重依赖用户的行为，它们往往容易受到托攻击。因此，保护它们免受攻击影响非常重要。由于 “托 ”攻击具有评分数量多、攻击模型复杂度高的特点，深度学习方法成为了更准确地检测攻击的合适选择。本文提出了一种由卷积神经网络和递归神经网络两种不同神经网络组成的混合模型，以高效地检测 “托 ”攻击。所提出的深度学习模型利用转换网络架构来承接从用户评级配置文件中提取的属性。这种架构可以对推荐系统评分中的时间和空间信息进行建模。该混合模型克服了现有托攻击深度学习方法的局限性，从而提高了推荐系统的效率和鲁棒性。实验结果表明，与用于研究的最先进的深度学习算法相比，混合模型能在 Movie-Lens 100 K 和 Netflix 数据集上准确检测出大部分混淆攻击，从而获得更好的预测结果。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                         | 2024-10-28 09:55:21                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:26                                                                                                                                  | 2024-10-28 09:55:21                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                  | citationNumber: 20; ccfInfo: CCF-None SYMMETRY      | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| A Fuzzy Linguistic Approach-Based Non-malicious Noise Detection Algorithm for Recommendation System | International Journal of Fuzzy Systems                    |                                                                                                                                                                         |                  | webpage         |                                                                                                                                                                                                     | https://link.springer.com/article/10.1007/s40815-018-0508-1                                                                                                                                                                                                                   |                                     |                                 |                                       | In the world of web, recommendation system plays a vital role in predicting the user desirable item from a plethora of items. Since the recommendation system generates recommendations by processing the user ratings, the presence of irrelevant or noisy ratings will affect the accuracy of the recommendation system. Malicious users induce noise into the system in order to popularize or to de-promote a particular item which might result in reduced accuracy of recommendations. In addition to malicious users, genuine users offer noisy ratings to the items unknowingly. Despite several of the existing methods to detect and resolve the malicious noise, only few address the existence of natural or non-malicious noise in the recommendation system. Detecting natural noise is not as precise as dealing with the malicious noise. Since natural noise is caused by genuine user, the system needs to classify each rating from the perspective of user rating. In this paper, a fuzzy linguistic approach-based natural noise detection algorithm is proposed to address the uncertainty in detecting the non-malicious noise. The proposed detection algorithm is highly efficient as indicated by our experiments that evaluate noise-induced real-world data sets. 在网络世界中，推荐系统在从众多项目中预测用户所需的项目方面发挥着至关重要的作用。由于推荐系统是通过处理用户评分来生成推荐结果的，因此不相关或有噪声的评分会影响推荐系统的准确性。恶意用户为了推广或取消推广某一特定项目，会在系统中产生噪音，这可能会降低推荐的准确性。除了恶意用户，真正的用户也会在不知情的情况下对项目进行噪声评级。尽管现有的一些方法可以检测和解决恶意噪音，但只有少数方法可以解决推荐系统中存在的自然或非恶意噪音。检测自然噪音不如处理恶意噪音精确。由于自然噪音是由真实用户造成的，因此系统需要从用户评分的角度对每个评分进行分类。本文提出了一种基于模糊语言方法的自然噪声检测算法，以解决非恶意噪声检测的不确定性问题。我们对噪声引起的真实世界数据集进行了评估，结果表明所提出的检测算法非常高效。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |                                                                                                                                                                                         | 2024-10-29 00:54:21                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:19:54                                                                                                                                  | 2024-10-29 00:54:21                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                  | ccfInfo: Not Found; citationNumber: Not Found       | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| PORE: Provably Robust Recommender Systems against Data Poisoning Attacks                                                                                        | Jia, Jinyuan; Liu, Yupei; Hu, Yuepeng; Gong, Neil Zhenqiang                                                                                                             |                  | journalArticle  |                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                               |                                     |                                 |                                       | Data poisoning attacks spoof a recommender system to make arbitrary, attacker-desired recommendations via injecting fake users with carefully crafted rating scores into the recommender system. We envision a cat-and-mouse game for such data poisoning attacks and their defenses, i.e., new defenses are designed to defend against existing attacks and new attacks are designed to break them. To prevent such cat-and-mouse game, we propose PORE, the first framework to build provably robust recommender systems in this work. PORE can transform any existing recommender system to be provably robust against any untargeted data poisoning attacks, which aim to reduce the overall performance of a recommender system. Suppose PORE recommends top-N items to a user when there is no attack. We prove that PORE still recommends at least r of the N items to the user under any data poisoning attack, where r is a function of the number of fake users in the attack. Moreover, we design an efficient algorithm to compute r for each user. We empirically evaluate PORE on popular benchmark datasets. 数据中毒攻击通过向推荐系统中注入虚假用户和精心制作的评级分数，欺骗推荐系统，从而做出攻击者所希望的任意推荐。我们设想此类数据中毒攻击及其防御系统会上演一场 “猫捉老鼠 ”的游戏，即设计新的防御系统来抵御现有的攻击，设计新的攻击来破解现有的防御系统。为了防止这种 “猫鼠游戏”，我们提出了 PORE，这是本作品中第一个用于构建可证明鲁棒性推荐系统的框架。PORE 可以改造任何现有的推荐系统，使其具有可证明的鲁棒性，以抵御任何旨在降低推荐系统整体性能的无目标数据中毒攻击。假设在没有攻击的情况下，PORE会向用户推荐前N个条目。我们证明，在任何数据中毒攻击下，PORE 仍会向用户推荐 N 项中的至少 r 项，其中 r 是攻击中虚假用户数量的函数。此外，我们还设计了一种高效算法来计算每个用户的 r。我们在流行的基准数据集上对 PORE 进行了实证评估。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                         | 2024-10-29 01:16:00                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:12                                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                  | citationNumber: 1; ccfInfo: CCF-A USENIX Security   | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Shilling attacks against recommender systems: a comprehensive survey                                                                                            | Gunes, Ihsan; Kaleli, Cihan; Bilge, Alper; Polat, Huseyin                                                                                                               | 2014             | journalArticle  | Artificial Intelligence Review                                                                                                                                                                      | http://link.springer.com/10.1007/s10462-012-9364-9                                                                                                                                                                                                                            |                                     | 0269-2821, 1573-7462            | 10.1007/s10462-012-9364-9             |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | 2014-12                                                                                                                                                                                 | 2024-10-23 06:26:22                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:28                                                                                                                                  | 2024-10-23 06:26:22                                                                                                                                                                                                                                                                                                                                 | 767-799                                                                                                                                                                                          | ccfInfo: CCF-None AIR; citationNumber: 220          | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Uplift Modeling for Target User Attacks on Recommender Systems                                                                                                  | Wang, Wenjie; Wang, Changsheng; Feng, Fuli; Shi, Wentao; Ding, Daizong; Chua, Tat-Seng                                                                                  | 2024             | preprint        |                                                                                                                                                                                                     | http://arxiv.org/abs/2403.02692                                                                                                                                                                                                                                               |                                     |                                 |                                       | Recommender systems are vulnerable to injective attacks, which inject limited fake users into the platforms to manipulate the exposure of target items to all users. In this work, we identify that conventional injective attackers overlook the fact that each item has its unique potential audience, and meanwhile, the attack difficulty across different users varies. Blindly attacking all users will result in a waste of fake user budgets and inferior attack performance. To address these issues, we focus on an under-explored attack task called target user attacks, aiming at promoting target items to a particular user group. In addition, we formulate the varying attack difficulty as heterogeneous treatment effects through a causal lens and propose an Uplift-guided Budget Allocation (UBA) framework. UBA estimates the treatment effect on each target user and optimizes the allocation of fake user budgets to maximize the attack performance. Theoretical and empirical analysis demonstrates the rationality of treatment effect estimation methods of UBA. By instantiating UBA on multiple attackers, we conduct extensive experiments on three datasets under various settings with different target items, target users, fake user budgets, victim models, and defense models, validating the effectiveness and robustness of UBA. 推荐系统很容易受到注入式攻击，这种攻击将有限的虚假用户注入平台，以操纵目标项目对所有用户的曝光率。在这项工作中，我们发现传统的注入式攻击者忽视了这样一个事实，即每个项目都有其独特的潜在受众，同时不同用户的攻击难度也各不相同。盲目攻击所有用户会造成虚假用户预算的浪费和攻击性能的下降。为了解决这些问题，我们重点研究了一种尚未被充分开发的攻击任务，即目标用户攻击，旨在向特定用户群推广目标项目。此外，我们还通过因果视角将不同的攻击难度表述为异质处理效果，并提出了上行引导预算分配（UBA）框架。UBA 估算了对每个目标用户的处理效果，并优化了假用户预算的分配，使攻击性能最大化。理论和实证分析证明了 UBA 治疗效果估算方法的合理性。通过在多个攻击者身上实例化 UBA，我们在不同目标项目、目标用户、虚假用户预算、受害者模型和防御模型的各种设置下对三个数据集进行了广泛的实验，验证了 UBA 的有效性和鲁棒性。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | 2024-03-05                                                                                                                                                                              | 2024-10-25 02:59:51                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:30                                                                                                                                  | 2024-10-25 02:59:51                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                  | citationNumber: 0; ccfInfo: CCF-A WWW               | /unread; Computer Science - Information Retrieval                                                                                                                                                                                                                                                                                                                                                                                      |
| Stealthy attack on graph recommendation system                                                                                                                  | Ma, Hao; Gao, Min; Wei, Feng; Wang, Zongwei; Jiang, Feng; Zhao, Zehua; Yang, Zhengyi                                                                                    | 2024             | journalArticle  | Expert Systems with Applications                                                                                                                                                                    | https://linkinghub.elsevier.com/retrieve/pii/S0957417424013423                                                                                                                                                                                                                |                                     | 09574174                        | 10.1016/j.eswa.2024.124476            | The graph-based recommendation systems achieve significant success, yet they are accompanied by malicious attacks. In most scenes, attackers will inject crafted fake profiles into the recommendation system to boost the ranking of their target items in the recommendation lists. For an exploration of potential attacks hidden in real life, researchers have proposed various attacks with severe threats. However, current efforts in exploring attack strategies neglect the stealthiness aspect of the attack, i.e., the perturbation of recommendation performance after an attack can be quite noticeable, potentially alerting the defenders. To fill this research gap, this paper introduces a novel attack framework named InfoAtk, designed to conduct attacks while ensuring stealthiness. Specifically, given the dependency of precise recommendation predominantly on representations, the framework employs contrastive learning techniques to align representations before and after the attack, thereby augmenting stealthiness. Additionally, we optimize the representation of target items to outrank the last items in users’ recommendation lists, thereby promoting the visibility of the target item to increase the attack’s effectiveness. Extensive experiments on four public datasets validate the stealthiness and effectiveness of our proposed attack framework. 基于图的推荐系统取得了巨大成功，但也伴随着恶意攻击。在大多数情况下，攻击者会在推荐系统中注入精心制作的虚假资料，以提高其目标项目在推荐列表中的排名。为了探索隐藏在现实生活中的潜在攻击，研究人员提出了各种具有严重威胁的攻击。然而，目前在探索攻击策略方面的努力忽略了攻击的隐蔽性，即攻击后对推荐性能的扰动会非常明显，有可能引起防御者的警觉。为了填补这一研究空白，本文介绍了一种名为 InfoAtk 的新型攻击框架，旨在实施攻击的同时确保隐蔽性。具体来说，鉴于精确推荐主要依赖于表征，该框架采用了对比学习技术来调整攻击前后的表征，从而增强了隐蔽性。此外，我们还优化了目标项目的表征，使其在用户推荐列表中的排名高于最后一个项目，从而提高目标项目的可见度，增强攻击的有效性。在四个公开数据集上进行的广泛实验验证了我们提出的攻击框架的隐蔽性和有效性。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | 2024-12                                                                                                                                                                                 | 2024-10-25 07:15:49                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:17                                                                                                                                  | 2024-10-25 07:15:49                                                                                                                                                                                                                                                                                                                                 | 124476                                                                                                                                                                                           | citationNumber: 0; ccfInfo: CCF-C ESWA              | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| ToDA: Target-oriented Diffusion Attacker against Recommendation System                                                                                          | Liu, Xiaohao; Tao, Zhulin; Jiang, Ting; Chang, He; Ma, Yunshan; Wei, Yinwei; Wang, Xiang                                                                                | 2024             | preprint        |                                                                                                                                                                                                     | http://arxiv.org/abs/2401.12578                                                                                                                                                                                                                                               |                                     |                                 |                                       | Recommendation systems (RS) have become indispensable tools for web services to address information overload, thus enhancing user experiences and bolstering platforms' revenues. However, with their increasing ubiquity, security concerns have also emerged. As the public accessibility of RS, they are susceptible to specific malicious attacks where adversaries can manipulate user profiles, leading to biased recommendations. Recent research often integrates additional modules using generative models to craft these deceptive user profiles, ensuring them are imperceptible while causing the intended harm. Albeit their efficacy, these models face challenges of unstable training and the exploration-exploitation dilemma, which can lead to suboptimal results. In this paper, we pioneer to investigate the potential of diffusion models (DMs), for shilling attacks. Specifically, we propose a novel Target-oriented Diffusion Attack model (ToDA). It incorporates a pre-trained autoencoder that transforms user profiles into a high dimensional space, paired with a Latent Diffusion Attacker (LDA)-the core component of ToDA. LDA introduces noise into the profiles within this latent space, adeptly steering the approximation towards targeted items through cross-attention mechanisms. The global horizon, implemented by a bipartite graph, is involved in LDA and derived from the encoded user profile feature. This makes LDA possible to extend the generation outwards the on-processing user feature itself, and bridges the gap between diffused user features and target item features. Extensive experiments compared to several SOTA baselines demonstrate ToDA's effectiveness. Specific studies exploit the elaborative design of ToDA and underscore the potency of advanced generative models in such contexts. 推荐系统（RS）已成为网络服务解决信息过载问题不可或缺的工具，从而提升了用户体验，增加了平台收入。然而，随着其日益普及，安全问题也随之出现。由于 RS 具有公共可访问性，因此很容易受到特定的恶意攻击，对手可以操纵用户配置文件，从而导致有偏见的推荐。近期的研究通常会集成使用生成模型的附加模块来制作这些欺骗性的用户配置文件，确保它们在造成预期伤害的同时不被察觉。尽管这些模型很有效，但也面临着训练不稳定和探索-开发两难的挑战，这可能会导致次优结果。在本文中，我们率先研究了扩散模型（DMs）在抖动攻击方面的潜力。具体来说，我们提出了一种新颖的面向目标的扩散攻击模型（ToDA）。它结合了一个预先训练好的自动编码器，可将用户配置文件转换到一个高维空间，并与潜在扩散攻击器（LDA）--ToDA 的核心组件--搭配使用。LDA 将噪声引入该潜在空间的用户配置文件，通过交叉关注机制，巧妙地将近似值导向目标项目。全局水平线由一个双方图实现，参与到 LDA 中，并从编码的用户配置文件特征中衍生出来。这使得 LDA 能够将生成范围扩展到在处理过程中的用户特征本身，并在扩散的用户特征和目标项目特征之间架起了一座桥梁。与几种 SOTA 基线相比，广泛的实验证明了 ToDA 的有效性。具体研究利用了 ToDA 的精心设计，并强调了高级生成模型在这种情况下的功效。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 2024-07-18                                                                                                                                                                              | 2024-10-25 03:00:19                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:18                                                                                                                                  | 2024-10-25 03:00:19                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                  | citationNumber: 0; ccfInfo: CCF-None CORR           | /unread; Computer Science - Cryptography and Security                                                                                                                                                                                                                                                                                                                                                                                  |
| Attacking Click-through Rate Predictors via Generating Realistic Fake Samples                                                                                   | Duan, Mingxing; Li, Kenli; Zhang, Weinan; Qin, Jiarui; Xiao, Bin                                                                                                        | 2024             | journalArticle  | ACM Transactions on Knowledge Discovery from Data                                                                                                                                                   | https://dl.acm.org/doi/10.1145/3643685                                                                                                                                                                                                                                        |                                     | 1556-4681, 1556-472X            | 10.1145/3643685                       | How to construct imperceptible (realistic) fake samples is critical in adversarial attacks. Due to the sample feature diversity of a recommender system (containing both discrete and continuous features), traditional gradient-based adversarial attack methods may fail to construct realistic fake samples. Meanwhile, most recommendation models adopt click-through rate (CTR) predictors, which usually utilize black-box deep models with discrete features as input. Thus, how to efficiently construct realistic fake samples for black-box recommender systems is still full of challenges. In this article, we propose a hierarchical adversarial attack method against black-box CTR models via generating realistic fake samples, named CTRAttack. To better train the generation network, the weights of its embedding layer are shared with those of the substitute model, with both the similarity loss and classification loss used to update the generation network. To ensure that the discrete features of the generated fake samples are all real, we first adopt the similarity loss to ensure that the distribution of the generated perturbed samples is sufficiently close to the distribution of the real features, and then the nearest neighbor algorithm is used to retrieve the most appropriate features for non-existent discrete features from the candidate instance set. Extensive experiments demonstrate that CTRAttack can not only effectively attack the black-box recommender systems but also improve the robustness of these models while maintaining prediction accuracy. 如何构建不易察觉（逼真）的虚假样本在对抗攻击中至关重要。由于推荐系统的样本特征多样性（包含离散和连续特征），传统的基于梯度的对抗攻击方法可能无法构建逼真的假样本。同时，大多数推荐模型都采用点击率（CTR）预测器，这种预测器通常利用黑盒深度模型，以离散特征作为输入。因此，如何高效地为黑盒推荐系统构建逼真的虚假样本仍然充满挑战。在本文中，我们提出了一种通过生成真实假样本来攻击黑盒 CTR 模型的分层对抗攻击方法，命名为 CTRAttack。为了更好地训练生成网络，其嵌入层的权重与替代模型的权重共享，相似性损失和分类损失都用于更新生成网络。为了确保生成的假样本的离散特征都是真实的，我们首先采用相似性损失来确保生成的扰动样本的分布与真实特征的分布足够接近，然后使用最近邻算法从候选实例集中为不存在的离散特征检索最合适的特征。广泛的实验证明，CTRAttack 不仅能有效攻击黑盒推荐系统，还能提高推荐系统的鲁棒性。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 2024-06-30                                                                                                                                                                              | 2024-10-24 05:13:30                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:19:59                                                                                                                                  | 2024-10-24 05:13:30                                                                                                                                                                                                                                                                                                                                 | 1-24                                                                                                                                                                                             | citationNumber: 0; ccfInfo: CCF-B TKDD              | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Poisoning QoS-aware cloud API recommender system with generative adversarial network attack                                                                     | Chen, Zhen; Bao, Taiyu; Qi, Wenchao; You, Dianlong; Liu, Linlin; Shen, Limin                                                                                            | 2024             | journalArticle  | Expert Systems with Applications                                                                                                                                                                    | https://linkinghub.elsevier.com/retrieve/pii/S0957417423021322                                                                                                                                                                                                                |                                     | 09574174                        | 10.1016/j.eswa.2023.121630            | With the proliferation and deepening of service-oriented architecture, more and more enterprises and organizations are exposing their computing functions and big data to the Internet in the form of cloud APIs to support service-oriented software development. This has resulted in a plethora of cloud APIs with similar functionality appearing on the Web, drowning users in a sea of cloud API choices. To solve this problem, quality of service (QoS)-aware recommender system is then widely applied to the selection of cloud APIs. Due to the dynamic and open network environment, the QoS-aware cloud API recommender systems are vulnerable to data poisoning attacks, where attackers inject poisoned data to skew the recommender system and make the recommendation direction follow the attacker’s will. Given the lack of data poisoning attack methods and robustness analysis for existing QoS-aware cloud API recommender systems, in this work, we first built a general poisoning attack framework for QoS-aware cloud API recommender systems to elucidate and standardize the attack process. Then, we proposed a deep learning-based poison attack approach, which uses generative adversarial network (GAN) to learn the cloud API QoS data distribution of real users in an adversarial way, so as to generate high-quality fake user attack vectors. We conducted extensive experiments on real-world QoS datasets, and the experimental results show that our proposed GAN-based poisoning attack is effective and can better hide itself from being detected. In addition, we analyzed the data poisoning attack mechanism and the robustness of the cloud API recommender system based on four categories of twelve recommendation methods, thereby raising awareness about the security of cloud API recommendation and helping the recommender system defenders to develop more targeted defense strategies. 随着面向服务架构的普及和深化，越来越多的企业和组织将其计算功能和大数据以云 API 的形式暴露在互联网上，以支持面向服务的软件开发。这导致网络上出现了大量功能相似的云 API，用户被淹没在云 API 的选择海洋中。为解决这一问题，服务质量（QoS）感知推荐系统被广泛应用于云应用程序接口的选择。由于网络环境的动态性和开放性，服务质量感知云应用程序接口推荐系统很容易受到数据中毒攻击，即攻击者注入中毒数据以歪曲推荐系统，使推荐方向遵循攻击者的意愿。鉴于现有的QoS感知云API推荐系统缺乏数据中毒攻击方法和鲁棒性分析，在这项工作中，我们首先构建了QoS感知云API推荐系统的通用中毒攻击框架，以阐明和规范攻击过程。然后，我们提出了一种基于深度学习的中毒攻击方法，利用生成式对抗网络（GAN）以对抗的方式学习真实用户的云 API QoS 数据分布，从而生成高质量的虚假用户攻击向量。我们在真实世界的 QoS 数据集上进行了大量实验，实验结果表明，我们提出的基于 GAN 的中毒攻击是有效的，能够更好地隐藏自己不被检测到。此外，我们还基于四类十二种推荐方法分析了云 API 推荐系统的数据中毒攻击机制和鲁棒性，从而提高了云 API 推荐的安全意识，有助于推荐系统防御者制定更有针对性的防御策略。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | 2024-03                                                                                                                                                                                 | 2024-10-24 02:26:58                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:11                                                                                                                                  | 2024-10-24 02:26:58                                                                                                                                                                                                                                                                                                                                 | 121630                                                                                                                                                                                           | citationNumber: 0; ccfInfo: CCF-C ESWA              | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| LOKI: A Practical Data Poisoning Attack Framework Against Next Item Recommendations                                                                             | Zhang, Hengtong; Li, Yaliang; Ding, Bolin; Gao, Jing                                                                                                                    | 2023             | journalArticle  | IEEE Transactions on Knowledge and Data Engineering                                                                                                                                                 | https://ieeexplore.ieee.org/document/9806383/?arnumber=9806383                                                                                                                                                                                                                |                                     | 1558-2191                       | 10.1109/TKDE.2022.3181270             | Due to the openness of the online platform, recommendation systems are vulnerable to data poisoning attacks, where malicious samples are injected into the training set of the recommendation system to manipulate its recommendation results. Existing attack approaches are either based on heuristic rules or designed against specific recommendation approaches. The former suffers unsatisfactory performance, while the latter requires strong knowledge of the target system. In this paper, we propose a practical poisoning attack approach named LOKI against blackbox recommendation systems. The proposed LOKI utilizes the reinforcement learning algorithm to train the attack agent, which can be used to generate user behavior samples for data poisoning. In real-world recommendation systems, the cost of retraining recommendation models is high, and the interaction frequency between users and a recommendation system is restricted. Thus, we propose to let the agent interact with a recommender simulator instead of the target recommendation system and leverage the transferability of the generated adversarial samples to poison the target system. We also use the influence function to efficiently estimate the influence of injected samples on recommendation results, without re-training the models. Extensive experiments on multiple datasets against four representative recommendation models show that the proposed LOKI outperformances existing method. We also discuss the characteristics of vulnerable users/items, and evaluate whether anomaly detection methods can be used to mitigate the impact of data poisoning attacks. 由于在线平台的开放性，推荐系统很容易受到数据中毒攻击，即向推荐系统的训练集中注入恶意样本，以操纵其推荐结果。现有的攻击方法要么基于启发式规则，要么针对特定的推荐方法而设计。前者的性能不尽如人意，而后者则需要对目标系统有很强的了解。在本文中，我们针对黑盒推荐系统提出了一种名为 LOKI 的实用中毒攻击方法。所提出的 LOKI 利用强化学习算法训练攻击代理，可用于生成用户行为样本以进行数据投毒。在现实世界的推荐系统中，重新训练推荐模型的成本很高，而且用户与推荐系统之间的交互频率受到限制。因此，我们建议让代理与推荐模拟器而不是目标推荐系统进行交互，并利用生成的对抗样本的可转移性来毒害目标系统。我们还利用影响函数来有效估计注入样本对推荐结果的影响，而无需重新训练模型。针对四个代表性推荐模型在多个数据集上进行的大量实验表明，所提出的 LOKI 优于现有方法。我们还讨论了易受攻击用户/项目的特征，并评估了异常检测方法是否可用于减轻数据中毒攻击的影响。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 2023-05                                                                                                                                                                                 | 2024-10-24 05:15:03                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:25                                                                                                                                  | 2024-10-24 05:15:03                                                                                                                                                                                                                                                                                                                                 | 5047-5059                                                                                                                                                                                        | citationNumber: 0; ccfInfo: CCF-A  TKDE             | /unread; Collaborative filtering; Behavioral sciences; Reinforcement learning; Training; Detectors; Task analysis; Adversarial learning; data poisoning; recommendation system; Recurrent neural networks                                                                                                                                                                                                                              |
| Interaction-level Membership Inference Attack Against Federated Recommender Systems                                                                             | Yuan, Wei; Yang, Chaoqun; Nguyen, Quoc Viet Hung; Cui, Lizhen; He, Tieke; Yin, Hongzhi                                                                                  | 2023             | conferencePaper | Proceedings of the ACM Web Conference 2023                                                                                                                                                          | https://dl.acm.org/doi/10.1145/3543507.3583359                                                                                                                                                                                                                                | 978-1-4503-9416-1                   |                                 | 10.1145/3543507.3583359               | The marriage of federated learning and recommender system (FedRec) has been widely used to address the growing data privacy  concerns in personalized recommendation services. In FedRecs,  users’ attribute information and behavior data (i.e., user-item interaction data) are kept locally on their personal devices, therefore, it  is considered a fairly secure approach to protect user privacy. As  a result, the privacy issue of FedRecs is rarely explored. Unfortunately, several recent studies reveal that FedRecs are vulnerable to  user attribute inference attacks, highlighting the privacy concerns  of FedRecs. In this paper, we further investigate the privacy problem of user behavior data (i.e., user-item interactions) in FedRecs.  Specifcally, we perform the frst systematic study on interactionlevel membership inference attacks on FedRecs. An interactionlevel membership inference attacker is frst designed, and then  the classical privacy protection mechanism, Local Diferential Privacy (LDP), is adopted to defend against the membership inference  attack. Unfortunately, the empirical analysis shows that LDP is  not efective against such new attacks unless the recommendation  performance is largely compromised. To mitigate the interactionlevel membership attack threats, we design a simple yet efective  defense method to signifcantly reduce the attacker’s inference  accuracy without losing recommendation performance. Extensive  experiments are conducted with two widely used FedRecs (Fed-NCF  and Fed-LightGCN) on three real-world recommendation datasets  (MovieLens-100K, Steam-200K, and Amazon Cell Phone), and the  experimental results show the efectiveness of our solutions. 联盟学习与推荐系统（FedRec）的结合已被广泛用于解决个性化推荐服务中日益增长的数据隐私问题。在联邦推荐系统中，用户的属性信息和行为数据（即用户与项目的交互数据）都保存在用户的个人设备上，因此被认为是一种相当安全的保护用户隐私的方法。因此，人们很少探讨联邦记录的隐私问题。不幸的是，最近的一些研究表明，联邦记录很容易受到用户属性推断攻击，这凸显了联邦记录的隐私问题。在本文中，我们将进一步研究 FedRecs 中用户行为数据（即用户-物品交互）的隐私问题。 具体来说，我们首次系统地研究了对联邦记录的交互级成员推断攻击。我们首先设计了一种交互级成员推理攻击器，然后采用经典的隐私保护机制--局部差分隐私（LDP）来防御成员推理攻击。不幸的是，实证分析表明，除非推荐性能在很大程度上受到影响，否则 LDP 无法有效抵御这种新的攻击。为了减轻交互级成员攻击的威胁，我们设计了一种简单而有效的防御方法，在不损失推荐性能的情况下显著降低攻击者的推理准确性。我们使用两种广泛使用的 FedRecs（Fed-NCF 和 Fed-LightGCN）在三个真实推荐数据集（MovieLens-100K、Steam-200K 和 Amazon Cell Phone）上进行了广泛的实验，实验结果表明了我们的解决方案的有效性。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | 2023-04-30                                                                                                                                                                              | 2024-10-25 02:51:53                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:06                                                                                                                                  | 2024-10-25 02:51:53                                                                                                                                                                                                                                                                                                                                 | 1053-1062                                                                                                                                                                                        | ccfInfo: CCF-A WWW; citationNumber: 24              | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Detecting and Analyzing Collusive Entities on YouTube                                                                                                           | Dutta, Hridoy Sankar; Jobanputra, Mayank; Negi, Himani; Chakraborty, Tanmoy                                                                                             | 2021             | journalArticle  | ACM Transactions on Intelligent Systems and Technology                                                                                                                                              | https://dl.acm.org/doi/10.1145/3477300                                                                                                                                                                                                                                        |                                     | 2157-6904, 2157-6912            | 10.1145/3477300                       | YouTube sells advertisements on the posted videos, which in turn enables the content creators to monetize  their videos. As an unintended consequence, this has proliferated various illegal activities such as artificially  boosting of views, likes, comments, and subscriptions. We refer to such videos (gaining likes and comments  artificially) and channels (gaining subscriptions artificially) as “collusive entities”. Detecting such collusive  entities is an important yet challenging task. Existing solutions mostly deal with the problem of spotting fake  views, spam comments, fake content, etc., and oftentimes ignore how such fake activities emerge via collusion.  Here, we collect a large dataset consisting of two types of collusive entities on YouTube – videos submitted to  gain collusive likes and comment requests, and channels submitted to gain collusive subscriptions.  We begin by providing an in-depth analysis of collusive entities on YouTube fostered by various blackmarket  services. Following this, we propose models to detect three types of collusive YouTube entities – videos seeking  collusive likes, channels seeking collusive subscriptions, and videos seeking collusive comments. The third  type of entity is associated with temporal information. To detect videos and channels for collusive likes and  subscriptions respectively, we utilize one-class classifiers trained on our curated collusive entities and a set of  novel features. The SVM-based model shows significant performance with a true positive rate of 0.911 and 0.910  for detecting collusive videos and collusive channels respectively. To detect videos seeking collusive comments,  we propose CollATe, a novel end-to-end neural architecture that leverages time-series information of posted  comments along with static metadata of videos. CollATe is composed of three components – metadata feature  extractor (which derives metadata-based features from videos), anomaly feature extractor (which utilizes  the time-series data to detect sudden changes in the commenting activity), and comment feature extractor  (which utilizes the text of the comments posted during collusion and computes a similarity score between the  comments). Extensive experiments show the effectiveness of CollATe (with a true positive rate of 0.905) over  the baselines. YouTube 在发布的视频上出售广告，这反过来又使内容创作者能够通过视频赚钱。一个意想不到的后果是，这导致了各种非法活动的泛滥，例如人为地提高浏览量、点赞数、评论数和订阅数。我们将此类视频（人为获得点赞和评论）和频道（人为获得订阅）称为 “串通实体”。检测此类串通实体是一项重要而又具有挑战性的任务。现有的解决方案大多处理的是发现虚假浏览、垃圾评论、虚假内容等问题，往往忽略了这些虚假活动是如何通过串通出现的。 在这里，我们收集了一个大型数据集，其中包括 YouTube 上的两类合谋实体--为获得合谋点赞和评论请求而提交的视频，以及为获得合谋订阅而提交的频道。 我们首先深入分析了 YouTube 上由各种黑市服务促成的合谋实体。随后，我们提出了检测三类合谋 YouTube 实体的模型--寻求合谋点赞的视频、寻求合谋订阅的频道和寻求合谋评论的视频。第三类实体与时间信息相关。为了分别检测串通点赞和订阅的视频和频道，我们使用了根据我们策划的串通实体和一组新特征训练的单类分类器。基于 SVM 的模型在检测串通视频和串通频道方面表现出色，真阳性率分别为 0.911 和 0.910。为了检测寻求串通评论的视频，我们提出了一种新颖的端到端神经架构 CollATe，它利用了发布评论的时间序列信息和视频的静态元数据。CollATe 由三个部分组成：元数据特征提取器（从视频中提取基于元数据的特征）、异常特征提取器（利用时间序列数据检测评论活动的突然变化）和评论特征提取器（利用串通期间发布的评论文本并计算评论之间的相似度得分）。大量实验表明，CollATe 比基线更有效（真阳性率为 0.905）。 | 2021-10-31                                                                                                                                                                              | 2024-10-17 12:22:33                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:01                                                                                                                                  | 2024-10-17 12:22:33                                                                                                                                                                                                                                                                                                                                 | 1-28                                                                                                                                                                                             | ccfInfo: CCF-None TIST; citationNumber: 11          | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Addon Item                                                                                                                                                      |                                                                                                                                                                         |                  | computerProgram |                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                               |                                     |                                 |                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                                                                                         | 2024-09-07 03:19:31                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:19:56                                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                  | ccfInfo: Not Found; citationNumber: Not Found       | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Stealthy Attack on Large Language Model based Recommendation                                                                                                    | Zhang, Jinghao; Liu, Yuting; Liu, Qiang; Wu, Shu; Guo, Guibing; Wang, Liang                                                                                             | 2024             | preprint        |                                                                                                                                                                                                     | http://arxiv.org/abs/2402.14836                                                                                                                                                                                                                                               |                                     |                                 |                                       | Recently, the powerful large language models (LLMs) have been instrumental in propelling the progress of recommender systems (RS). However, while these systems have flourished, their susceptibility to security threats has been largely overlooked. In this work, we reveal that the introduction of LLMs into recommendation models presents new security vulnerabilities due to their emphasis on the textual content of items. We demonstrate that attackers can significantly boost an item's exposure by merely altering its textual content during the testing phase, without requiring direct interference with the model's training process. Additionally, the attack is notably stealthy, as it does not affect the overall recommendation performance and the modifications to the text are subtle, making it difficult for users and platforms to detect. Our comprehensive experiments across four mainstream LLM-based recommendation models demonstrate the superior efficacy and stealthiness of our approach. Our work unveils a significant security gap in LLM-based recommendation systems and paves the way for future research on protecting these systems.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | 2024-06-05                                                                                                                                                                              | 2024-09-13 13:44:53                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:17                                                                                                                                  | 2024-09-13 13:44:53                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                  | citationNumber: 0; ccfInfo: CCF-A ACL               | /unread; Computer Science - Artificial Intelligence; Computer Science - Information Retrieval; Computer Science - Computation and Language                                                                                                                                                                                                                                                                                             |
| PRADA: Practical Black-box Adversarial Attacks against Neural Ranking Models                                                                                    | Wu, Chen; Zhang, Ruqing; Guo, Jiafeng; De Rijke, Maarten; Fan, Yixing; Cheng, Xueqi                                                                                     | 2023             | journalArticle  | ACM Transactions on Information Systems                                                                                                                                                             | https://dl.acm.org/doi/10.1145/3576923                                                                                                                                                                                                                                        |                                     | 1046-8188, 1558-2868            | 10.1145/3576923                       | Neural ranking models (NRMs) have shown remarkable success in recent years, especially with pre-trained language models. However, deep neural models are notorious for their vulnerability to adversarial examples. Adversarial attacks may become a new type of web spamming technique given our increased reliance on neural information retrieval models. Therefore, it is important to study potential adversarial attacks to identify vulnerabilities of NRMs before they are deployed.             In this article, we introduce the Word Substitution Ranking Attack (WSRA) task against NRMs, which aims at promoting a target document in rankings by adding adversarial perturbations to its text. We focus on the decision-based black-box attack setting, where the attackers cannot directly get access to the model information, but can only query the target model to obtain the rank positions of the partial retrieved list. This attack setting is realistic in real-world search engines. We propose a novel Pseudo Relevance-based ADversarial ranking Attack method (PRADA) that learns a surrogate model based on Pseudo Relevance Feedback (PRF) to generate gradients for finding the adversarial perturbations.             Experiments on two web search benchmark datasets show that PRADA can outperform existing attack strategies and successfully fool the NRM with small indiscernible perturbations of text. 近年来，神经排序模型（NRMs）取得了令人瞩目的成就，尤其是在预训练语言模型方面。然而，深度神经模型因容易受到对抗性示例的影响而臭名昭著。鉴于我们越来越依赖神经信息检索模型，对抗性攻击可能会成为一种新型的网络垃圾技术。因此，在部署神经信息检索模型之前，研究潜在的对抗性攻击以识别其漏洞非常重要。 在本文中，我们介绍了针对 NRM 的单词替换排名攻击（WSRA）任务，其目的是通过在目标文档的文本中添加对抗性扰动来提升该文档的排名。我们将重点放在基于决策的黑盒攻击设置上，即攻击者无法直接获取模型信息，只能通过查询目标模型来获取部分检索列表的排名位置。这种攻击设置在现实世界的搜索引擎中是真实存在的。我们提出了一种新颖的基于伪相关性的对抗性排名攻击方法（PRADA），该方法基于伪相关性反馈（PRF）学习代理模型，以产生梯度来寻找对抗性扰动。 在两个网络搜索基准数据集上进行的实验表明，PRADA 的性能优于现有的攻击策略，并能成功地利用文本中难以辨别的微小扰动来欺骗 NRM。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | 2023-10-31                                                                                                                                                                              | 2024-10-09 13:13:50                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:12                                                                                                                                  | 2024-10-09 13:13:50                                                                                                                                                                                                                                                                                                                                 | 1-27                                                                                                                                                                                             | ccfInfo: CCF-A  TOIS; citationNumber: 19            | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Revisiting Injective Attacks on Recommender Systems                                                                                                             | Li, Haoyang; Di, Shimin; Chen, Lei                                                                                                                                      |                  | journalArticle  |                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                               |                                     |                                 |                                       | Recent studies have demonstrated that recommender systems (RecSys) are vulnerable to injective attacks. Given a limited fake user budget, attackers can inject fake users with carefully designed behaviors into the open platforms, making RecSys recommend a target item to more real users for proﬁts. In this paper, we ﬁrst revisit existing attackers and reveal that they suffer from the difﬁculty-agnostic and diversity-deﬁcit issues. Existing attackers concentrate their efforts on difﬁcult users who have low tendencies toward the target item, thus reducing their effectiveness. Moreover, they are incapable of affecting the target RecSys to recommend the target item to real users in a diverse manner, because their generated fake user behaviors are dominated by large communities. To alleviate these two issues, we propose a difﬁculty and diversity aware attacker, namely DADA. We design the difﬁculty-aware and diversity-aware objectives to enable easy users from various communities to contribute more weights when optimizing attackers. By incorporating these two objectives, the proposed attacker DADA can concentrate on easy users while also affecting a broader range of real users simultaneously, thereby boosting the effectiveness. Extensive experiments on three real-world datasets demonstrate the effectiveness of our proposed attacker. 最近的研究表明，推荐系统（RecSys）很容易受到注入式攻击。在虚假用户预算有限的情况下，攻击者可以向开放平台注入精心设计行为的虚假用户，从而使 RecSys 向更多真实用户推荐目标项目，以获取利益。在本文中，我们首先对现有的攻击者进行了重新审视，发现他们都存在能力差异和多样性缺陷问题。现有的攻击者集中攻击那些对目标项目倾向性较低的高难度用户，从而降低了攻击的有效性。此外，由于它们生成的虚假用户行为由大型社区主导，因此无法影响目标 RecSys 向真实用户推荐目标商品。为了缓解这两个问题，我们提出了一种差异和多样性感知攻击者，即 DADA。我们设计了差异感知目标和多样性感知目标，以使来自不同社区的用户在优化攻击者时能够贡献更多权重。通过结合这两个目标，所提出的攻击者 DADA 可以集中攻击易受攻击的用户，同时也能影响更多的真实用户，从而提高效率。在三个真实世界数据集上的广泛实验证明了我们提出的攻击者的有效性。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                                                                                                                                                                         | 2024-10-09 13:31:20                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:14                                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                  | ccfInfo: CCF-A NeurIPS; citationNumber: 4           | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Poisoning Attacks to Graph-Based Recommender Systems                                                                                                            | Fang, Minghong; Yang, Guolei; Gong, Neil Zhenqiang; Liu, Jia                                                                                                            | 2018             | conferencePaper | Proceedings of the 34th Annual Computer Security Applications Conference                                                                                                                            | https://dl.acm.org/doi/10.1145/3274694.3274706                                                                                                                                                                                                                                | 978-1-4503-6569-7                   |                                 | 10.1145/3274694.3274706               | Recommender system is an important component of many web services to help users locate items that match their interests. Several studies showed that recommender systems are vulnerable to poisoning a acks, in which an a acker injects fake data to a recommender system such that the system makes recommendations as the a acker desires. However, these poisoning a acks are either agnostic to recommendation algorithms or optimized to recommender systems (e.g., association-rule-based or matrix-factorization-based recommender systems) that are not graph-based. Like associationrule-based and matrix-factorization-based recommender systems, graph-based recommender system is also deployed in practice, e.g., eBay, Huawei App Store (a big app store in China). However, how to design optimized poisoning a acks for graph-based recommender systems is still an open problem. In this work, we perform a systematic study on poisoning a acks to graph-based recommender systems. We consider an a acker’s goal is to promote a target item to be recommended to as many users as possible. To achieve this goal, our a acks inject fake users with carefully crafted rating scores to the recommender system. Due to limited resources and to avoid detection, we assume the number of fake users that can be injected into the system is bounded. The key challenge is how to assign rating scores to the fake users such that the target item is recommended to as many normal users as possible. To address the challenge, we formulate the poisoning a acks as an optimization problem, solving which determines the rating scores for the fake users. We also propose techniques to solve the optimization problem. We evaluate our a acks and compare them with existing a acks under white-box (recommendation algorithm and its parameters are known), gray-box (recommendation algorithm is known but its parameters are unknown), and blackbox (recommendation algorithm is unknown) se ings using two real-world datasets. Our results show that our a ack is effective and outperforms existing a acks for graph-based recommender systems. For instance, when 1% of users are injected fake users, our a ack can make a target item recommended to 580 times more normal users in certain scenarios. 推荐系统是许多网络服务的重要组成部分，可帮助用户找到符合其兴趣的项目。一些研究表明，推荐系统很容易受到 “中毒 ”攻击的影响。“中毒 ”攻击者会向推荐系统注入虚假数据，使推荐系统按照 “中毒 ”攻击者的意愿进行推荐。然而，这些 “投毒 ”行为要么与推荐算法无关，要么针对非基于图的推荐系统（如基于关联规则或矩阵因子的推荐系统）进行了优化。与基于关联规则和矩阵因式分解的推荐系统一样，基于图的推荐系统也在实践中得到了应用，例如易趣、华为应用商店（中国一家大型应用商店）等。然而，如何为基于图的推荐系统设计优化的中毒机制仍是一个未决问题。在这项工作中，我们对基于图的推荐系统的中毒策略进行了系统研究。我们认为中毒者的目标是将目标项目推荐给尽可能多的用户。为了实现这一目标，我们的 “中毒者 ”会向推荐系统注入经过精心设计的虚假用户评分。由于资源有限，为了避免被发现，我们假定注入系统的虚假用户数量是有界限的。关键的挑战在于如何为虚假用户分配评级分数，从而向尽可能多的正常用户推荐目标项目。为了应对这一挑战，我们将 “中毒 ”问题表述为一个优化问题，通过求解优化问题来确定虚假用户的评级分数。我们还提出了解决优化问题的技术。我们使用两个真实世界的数据集对我们的算法进行了评估，并与白盒（推荐算法及其参数已知）、灰盒（推荐算法已知，但其参数未知）和黑盒（推荐算法未知）条件下的现有算法进行了比较。结果表明，对于基于图的推荐系统来说，我们的方法是有效的，而且优于现有的方法。例如，当 1%的用户被注入虚假用户时，在某些情况下，我们的拦截器能使目标项目被推荐给多 580 倍的正常用户。                                                                                                | 2018-12-03                                                                                                                                                                              | 2024-10-09 12:43:08                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:30                                                                                                                                  | 2024-10-09 12:43:08                                                                                                                                                                                                                                                                                                                                 | 381-392                                                                                                                                                                                          | ccfInfo: CCF-B ACSAC; citationNumber: 107           | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Adversarial attacks on an oblivious recommender                                                                                                                 | Christakopoulou, Konstantina; Banerjee, Arindam                                                                                                                         | 2019             | conferencePaper | Proceedings of the 13th ACM Conference on Recommender Systems                                                                                                                                       | https://dl.acm.org/doi/10.1145/3298689.3347031                                                                                                                                                                                                                                | 978-1-4503-6243-6                   |                                 | 10.1145/3298689.3347031               | Can machine learning models be easily fooled? Despite the recent surge of interest in learned adversarial attacks in other domains, in the context of recommendation systems this question has mainly been answered using hand-engineered fake user profles. This paper attempts to reduce this gap. We provide a formulation for learning to attack a recommender as a repeated general-sum game between two players, i.e., an adversary and a recommender oblivious to the adversary’s existence. We consider the challenging case of poisoning attacks, which focus on the training phase of the recommender model. We generate adversarial user profles targeting subsets of users or items, or generally the top-K recommendation quality. Moreover, we ensure that the adversarial user profles remain unnoticeable by preserving proximity of the real user rating/interaction distribution to the adversarial fake user distribution. To cope with the challenge of the adversary not having access to the gradient of the recommender’s objective with respect to the fake user profles, we provide a non-trivial algorithm building upon zero-order optimization techniques. We ofer a wide range of experiments, instantiating the proposed method for the case of the classic popular approach of a low-rank recommender, and illustrating the extent of the recommender’s vulnerability to a variety of adversarial intents. These results can serve as a motivating point for more research into recommender defense strategies against machine learned attacks. 机器学习模型容易被愚弄吗？尽管最近人们对其他领域的学习型对抗攻击兴趣大增，但在推荐系统中，这个问题主要是通过人工设计的虚假用户资料来回答的。本文试图缩小这一差距。我们将学习攻击推荐器表述为两个玩家之间的重复泛和博弈，即一个对手和一个不知道对手存在的推荐器之间的博弈。我们考虑了具有挑战性的中毒攻击案例，其重点是推荐模型的训练阶段。我们针对用户或项目的子集，或者一般来说针对 Top-K 推荐质量，生成对抗性的用户漏洞。此外，我们还通过保持真实用户评分/交互分布与对抗性假用户分布的接近性，确保对抗性用户亵渎行为不被察觉。为了应对对手无法获取推荐者目标相对于虚假用户质量的梯度这一挑战，我们提供了一种基于零阶优化技术的非难算法。我们提供了大量实验，针对低等级推荐器这一经典的流行方法实例化了所提出的方法，并说明了推荐器在各种敌意面前的脆弱程度。这些结果可以作为更多研究推荐器针对机器的防御策略的动力点。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | 2019-09-10                                                                                                                                                                              | 2024-10-09 12:43:43                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:19:57                                                                                                                                  | 2024-10-09 12:43:43                                                                                                                                                                                                                                                                                                                                 | 322-330                                                                                                                                                                                          | citationNumber: 91; ccfInfo: CCF-B RecSys           | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Targeted poisoning attacks on social recommender systems                                                                                                        | Hu, Rui; Guo, Yuanxiong; Pan, Miao; Gong, Yanmin                                                                                                                        | 2019             | conferencePaper | 2019 IEEE Global Communications Conference (GLOBECOM)                                                                                                                                               | https://ieeexplore.ieee.org/abstract/document/9013539/                                                                                                                                                                                                                        |                                     |                                 |                                       | With the popularity of online social networks, social recommendations that rely on ones social connections to make personalized recommendations have become possible. This introduces vulnerabilities for an adversarial party to compromise the recommendations for users by utilizing their social connections. In this paper, we propose the targeted poisoning attack on the factorization-based social recommender system in which the attacker aims to promote an item to a group of target users by injecting fake ratings and social connections. We formulate the optimal poisoning attack as a bi-level program and develop an efficient algorithm to find the optimal attacking strategy. We then evaluate the proposed attacking strategy on real-world dataset and demonstrate that the social recommender system is sensitive to the targeted poisoning attack. We find that users in the social recommender system can be attacked even if they do not have direct social connections with the attacker. 随着在线社交网络的普及，依靠社交关系进行个性化推荐成为可能。这就为敌对者利用用户的社交关系破坏用户推荐带来了漏洞。在本文中，我们提出了对基于因子化的社交推荐系统的定向投毒攻击，攻击者的目的是通过注入虚假的评分和社交关系向目标用户群推广某个项目。我们将最优中毒攻击表述为一个双层程序，并开发了一种高效算法来找到最优攻击策略。然后，我们在真实世界的数据集上评估了所提出的攻击策略，并证明社交推荐系统对有针对性的中毒攻击很敏感。我们发现，社交推荐系统中的用户即使与攻击者没有直接的社交关系，也会受到攻击。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | 2019                                                                                                                                                                                    | 2024-10-09 12:44:08                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:17                                                                                                                                  | 2024-10-09 12:44:08                                                                                                                                                                                                                                                                                                                                 | 1–6                                                                                                                                                                                              | citationNumber: 19; ccfInfo: CCF-C GLOBECOM         | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Data poisoning attacks on stochastic bandits                                                                                                                    | Liu, Fang; Shroff, Ness                                                                                                                                                 | 2019             | conferencePaper | International Conference on Machine Learning                                                                                                                                                        | http://proceedings.mlr.press/v97/liu19e.html                                                                                                                                                                                                                                  |                                     |                                 |                                       | Stochastic multi-armed bandits form a class of online learning problems that have important applications in online recommendation systems, adaptive medical treatment, and many others. Even though potential attacks against these learning algorithms may hijack their behavior, causing catastrophic loss in real-world applications, little is known about adversarial attacks on bandit algorithms. In this paper, we propose a framework of offline attacks on bandit algorithms and study convex optimization based attacks on several popular bandit algorithms. We show that the attacker can force the bandit algorithm to pull a target arm with high probability by a slight manipulation of the rewards in the data. Then we study a form of online attacks on bandit algorithms and propose an adaptive attack strategy against any bandit algorithm without the knowledge of the bandit algorithm. Our adaptive attack strategy can hijack the behavior of the bandit algorithm to suffer a linear regret with only a logarithmic cost to the attacker. Our results demonstrate a significant security threat to stochastic bandits. 随机多臂匪帮是一类在线学习问题，在在线推荐系统、自适应医疗等领域有着重要的应用。尽管针对这些学习算法的潜在攻击可能会劫持它们的行为，从而在实际应用中造成灾难性损失，但人们对强盗算法的对抗性攻击却知之甚少。在本文中，我们提出了对强盗算法的离线攻击框架，并研究了对几种流行强盗算法的基于凸优化的攻击。我们证明，攻击者可以通过对数据中奖励的轻微操作，迫使强盗算法高概率地拉动目标臂。然后，我们研究了一种针对强盗算法的在线攻击形式，并提出了一种在不了解强盗算法的情况下针对任何强盗算法的自适应攻击策略。我们的自适应攻击策略可以劫持强盗算法的行为，使其遭受线性遗憾，而攻击者只需付出对数的代价。我们的研究结果表明，随机强盗算法面临着巨大的安全威胁。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | 2019                                                                                                                                                                                    | 2024-10-09 12:44:24                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:23                                                                                                                                  | 2024-10-09 12:44:24                                                                                                                                                                                                                                                                                                                                 | 4042–4050                                                                                                                                                                                        | ccfInfo: CCF-A ICML; citationNumber: 103            | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| SAShA: Semantic-Aware Shilling Attacks on Recommender Systems Exploiting Knowledge Graphs                                                                       | Anelli, Vito Walter; Deldjoo, Yashar; Di Noia, Tommaso; Di Sciascio, Eugenio; Merra, Felice Antonio                                                                     | 2020             | bookSection     | The Semantic Web                                                                                                                                                                                    | http://link.springer.com/10.1007/978-3-030-49461-2_18                                                                                                                                                                                                                         | 978-3-030-49460-5 978-3-030-49461-2 |                                 |                                       | Recommender systems (RS) play a focal position in modern user-centric online services. Among them, collaborative filtering (CF) approaches have shown leading accuracy performance compared to content-based filtering (CBF) methods. Their success is due to an effective exploitation of similarities/correlations encoded in user interaction patterns, which is computed by considering common items users rated in the past. However, their strength is also their weakness. Indeed, a malicious agent can alter recommendations by adding fake user profiles into the platform thereby altering the actual similarity values in an engineered way. The spread of well-curated information available in knowledge graphs (KG) has opened the door to several new possibilities in compromising the security of a recommender system. In fact, KG are a wealthy source of information that can dramatically increase the attacker’s (and the defender’s) knowledge of the underlying system. In this paper, we introduce SAShA, a new attack strategy that leverages semantic features extracted from a knowledge graph in order to strengthen the efficacy of the attack to standard CF models. We performed an extensive experimental evaluation in order to investigate whether SAShA is more effective than baseline attacks against CF models by taking into account the impact of various semantic features. Experimental results on two realworld datasets show the usefulness of our strategy in favor of attacker’s capacity in attacking CF models 推荐系统（RS）在以用户为中心的现代在线服务中占据着重要地位。其中，与基于内容的过滤（CBF）方法相比，协同过滤（CF）方法显示出领先的准确性。它们的成功是由于有效利用了用户交互模式中的相似性/相关性，而这种相似性/相关性是通过考虑用户过去评价的共同项目计算出来的。然而，它们的优势也是弱点。事实上，恶意代理可以通过在平台中添加虚假的用户配置文件来改变推荐结果，从而以设计的方式改变实际的相似性值。知识图谱（KG）中经过精心整理的信息的传播为破坏推荐系统的安全性打开了一扇新的大门。事实上，知识图谱是一种丰富的信息来源，可以显著增加攻击者（和防御者）对底层系统的了解。在本文中，我们介绍了一种新的攻击策略 SAShA，它利用从知识图谱中提取的语义特征来加强对标准 CF 模型的攻击效果。考虑到各种语义特征的影响，我们进行了广泛的实验评估，以研究 SAShA 是否比针对 CF 模型的基线攻击更有效。在两个真实世界数据集上的实验结果表明，我们的策略有利于提高攻击者攻击 CF 模型的能力。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | 2020                                                                                                                                                                                    | 2024-10-09 12:46:05                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:25                                                                                                                                  | 2024-10-09 12:46:05                                                                                                                                                                                                                                                                                                                                 | 307-323                                                                                                                                                                                          | ccfInfo: CCF-C ESWC; citationNumber: 37             | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Influence Function based Data Poisoning Attacks to Top-N Recommender Systems                                                                                    | Fang, Minghong; Gong, Neil Zhenqiang; Liu, Jia                                                                                                                          | 2020             | conferencePaper | Proceedings of The Web Conference 2020                                                                                                                                                              | https://dl.acm.org/doi/10.1145/3366423.3380072                                                                                                                                                                                                                                | 978-1-4503-7023-3                   |                                 | 10.1145/3366423.3380072               | Recommender system is an essential component of web services  to engage users. Popular recommender systems model user preferences and item properties using a large amount of crowdsourced  user-item interaction data, e.g., rating scores; then top-N items  that match the best with a user’s preference are recommended to  the user. In this work, we show that an attacker can launch a data  poisoning attack to a recommender system to make recommendations as the attacker desires via injecting fake users with carefully  crafted user-item interaction data. Specifically, an attacker can trick  a recommender system to recommend a target item to as many  normal users as possible. We focus on matrix factorization based  recommender systems because they have been widely deployed in  industry. Given the number of fake users the attacker can inject,  we formulate the crafting of rating scores for the fake users as an  optimization problem. However, this optimization problem is challenging to solve as it is a non-convex integer programming problem.  To address the challenge, we develop several techniques to approximately solve the optimization problem. For instance, we leverage  influence function to select a subset of normal users who are influential to the recommendations and solve our formulated optimization  problem based on these influential users. Our results show that our  attacks are effective and outperform existing methods. 推荐系统是网络服务吸引用户的重要组成部分。流行的推荐系统利用大量众包的用户-物品交互数据（如评分分数）对用户偏好和物品属性进行建模，然后向用户推荐与用户偏好最匹配的前 N 个物品。在这项工作中，我们展示了攻击者可以向推荐系统发起数据中毒攻击，通过向虚假用户注入精心制作的用户-物品交互数据，按照攻击者的意愿进行推荐。具体来说，攻击者可以诱骗推荐系统向尽可能多的正常用户推荐目标项目。我们将重点放在基于矩阵因式分解的推荐系统上，因为它们已被广泛应用于工业领域。考虑到攻击者可以注入的虚假用户数量，我们将虚假用户评级分数的制作制定为一个优化问题。然而，由于这是一个非凸整数编程问题，因此解决这个优化问题具有挑战性。 为了应对这一挑战，我们开发了几种近似解决优化问题的技术。例如，我们利用影响力函数来选择对推荐有影响力的普通用户子集，并根据这些有影响力的用户来解决我们提出的优化问题。结果表明，我们的攻击是有效的，并且优于现有方法。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 2020-04-20                                                                                                                                                                              | 2024-10-09 12:45:35                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:06                                                                                                                                  | 2024-10-09 12:45:35                                                                                                                                                                                                                                                                                                                                 | 3019-3025                                                                                                                                                                                        | ccfInfo: CCF-A WWW; citationNumber: 155             | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Attacking Recommender Systems with Augmented User Profiles                                                                                                      | Lin, Chen; Chen, Si; Li, Hui; Xiao, Yanghua; Li, Lianyun; Yang, Qian                                                                                                    | 2020             | conferencePaper | Proceedings of the 29th ACM International Conference on Information & Knowledge Management                                                                                                          | https://dl.acm.org/doi/10.1145/3340531.3411884                                                                                                                                                                                                                                | 978-1-4503-6859-9                   |                                 | 10.1145/3340531.3411884               | Recommendation Systems (RS) have become an essential part of many online services. Due to its pivotal role in guiding customers towards purchasing, there is a natural motivation for unscrupulous parties to spoof RS for profits. In this paper, we study the shilling attack: a subsistent and profitable attack where an adversarial party injects a number of user profiles to promote or demote a target item. Conventional shilling attack models are based on simple heuristics that can be easily detected, or directly adopt adversarial attack methods without a special design for RS. Moreover, the study on the attack impact on deep learning based RS is missing in the literature, making the effects of shilling attack against real RS doubtful. We present a novel Augmented Shilling Attack framework (AUSH) and implement it with the idea of Generative Adversarial Network. AUSH is capable of tailoring attacks against RS according to budget and complex attack goals, such as targeting a specific user group. We experimentally show that the attack impact of AUSH is noticeable on a wide range of RS including both classic and modern deep learning based RS, while it is virtually undetectable by the state-ofthe-art attack detection model. 推荐系统（RS）已成为许多在线服务的重要组成部分。由于推荐系统在引导客户购买方面发挥着关键作用，不法分子自然有动机欺骗推荐系统以牟利。在本文中，我们研究了 “shilling ”攻击：这是一种持续且有利可图的攻击，敌对方通过注入大量用户配置文件来提升或降低目标商品的等级。传统的起价攻击模型基于简单的启发式方法，很容易被检测到，或者直接采用对抗式攻击方法，而不对 RS 进行特殊设计。此外，文献中缺乏对基于深度学习的 RS 攻击影响的研究，这使人们对 Shilling 攻击对真实 RS 的影响产生怀疑。我们提出了一种新颖的增强型先令攻击框架（Augmented Shilling Attack，AUSH），并利用生成式对抗网络（Generative Adversarial Network）的思想加以实现。AUSH 能够根据预算和复杂的攻击目标（如针对特定用户群）定制针对 RS 的攻击。我们的实验表明，AUSH 的攻击影响在各种 RS（包括基于深度学习的经典和现代 RS）上都很明显，而最先进的攻击检测模型几乎检测不到它。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | 2020-10-19                                                                                                                                                                              | 2024-10-09 12:44:38                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:22                                                                                                                                  | 2024-10-09 12:44:38                                                                                                                                                                                                                                                                                                                                 | 855-864                                                                                                                                                                                          | ccfInfo: CCF-B CIKM; citationNumber: 73             | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| PoisonRec: An Adaptive Data Poisoning Framework for Attacking Black-box Recommender Systems                                                                     | Song, Junshuai; Li, Zhao; Hu, Zehong; Wu, Yucheng; Li, Zhenpeng; Li, Jian; Gao, Jun                                                                                     | 2020             | conferencePaper | 2020 IEEE 36th International Conference on Data Engineering (ICDE)                                                                                                                                  | https://ieeexplore.ieee.org/abstract/document/9101655                                                                                                                                                                                                                         |                                     |                                 | 10.1109/ICDE48307.2020.00021          | Data-driven recommender systems that can help to predict users' preferences are deployed in many real online service platforms. Several studies show that they are vulnerable to data poisoning attacks, and attackers have the ability to mislead the system to perform as their desires. Considering the realistic scenario, where the recommender system is usually a black-box for attackers and complex algorithms may be deployed in them, how to learn effective attack strategies on such recommender systems is still an under-explored problem. In this paper, we propose an adaptive data poisoning framework, PoisonRec, which can automatically learn effective attack strategies on various recommender systems with very limited knowledge. PoisonRec leverages the reinforcement learning architecture, in which an attack agent actively injects fake data (user behaviors) into the recommender system, and then can improve its attack strategies through reward signals that are available under the strict black-box setting. Specifically, we model the attack behavior trajectory as the Markov Decision Process (MDP) in reinforcement learning. We also design a Biased Complete Binary Tree (BCBT) to reformulate the action space for better attack performance. We adopt 8 widely-used representative recommendation algorithms as our testbeds, and make extensive experiments on 4 different real-world datasets. The results show that PoisonRec has the ability to achieve good attack performance on various recommender systems with limited knowledge. 数据驱动的推荐系统可以帮助预测用户的偏好，已被部署在许多真实的在线服务平台上。一些研究表明，它们很容易受到数据中毒攻击，攻击者有能力误导系统按照自己的意愿运行。考虑到现实场景中，推荐系统通常是攻击者的黑盒子，其中可能部署了复杂的算法，因此如何学习针对此类推荐系统的有效攻击策略仍是一个未充分探索的问题。在本文中，我们提出了一种自适应数据中毒框架 PoisonRec，它可以在知识非常有限的情况下自动学习对各种推荐系统的有效攻击策略。PoisonRec 利用强化学习架构，即攻击代理主动向推荐系统注入虚假数据（用户行为），然后通过严格黑箱设置下的奖励信号改进其攻击策略。具体来说，我们将攻击行为轨迹建模为强化学习中的马尔可夫决策过程（MDP）。我们还设计了有偏差的完全二叉树（BCBT）来重构行动空间，以获得更好的攻击性能。我们采用了 8 种广泛使用的代表性推荐算法作为测试平台，并在 4 个不同的真实数据集上进行了大量实验。结果表明，PoisonRec 有能力在各种推荐系统上实现良好的攻击性能。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | 2020-04                                                                                                                                                                                 | 2024-10-09 13:24:40                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:12                                                                                                                                  | 2024-10-09 13:24:40                                                                                                                                                                                                                                                                                                                                 | 157-168                                                                                                                                                                                          | ccfInfo: CCF-A ICDE; citationNumber: 38             | /unread; Recommender systems; Data models; Reinforcement learning; Adaptive systems; Data Poisoning Attack; Learning (artificial intelligence); Markov processes; Recommender System; Reinforcement Learning                                                                                                                                                                                                                           |
| Revisiting Adversarially Learned Injection Attacks Against Recommender Systems                                                                                  | Tang, Jiaxi; Wen, Hongyi; Wang, Ke                                                                                                                                      | 2020             | conferencePaper | Fourteenth ACM Conference on Recommender Systems                                                                                                                                                    | https://dl.acm.org/doi/10.1145/3383313.3412243                                                                                                                                                                                                                                | 978-1-4503-7583-2                   |                                 | 10.1145/3383313.3412243               | Recommender systems play an important role in modern information and e-commerce applications. While increasing research is dedicated to improving the relevance and diversity of the recommendations, the potential risks of state-of-the-art recommendation models are under-explored, that is, these models could be subject to attacks from malicious third parties, through injecting fake user interactions to achieve their purposes. This paper revisits the adversarially-learned injection attack problem, where the injected fake user ‘behaviors’ are learned locally by the attackers with their own model – one that is potentially different from the model under attack, but shares similar properties to allow attack transfer. We found that most existing works in literature suffer from two major limitations: (1) they do not solve the optimization problem precisely, making the attack less harmful than it could be, (2) they assume perfect knowledge for the attack, causing the lack of understanding for realistic attack capabilities. We demonstrate that the exact solution for generating fake users as an optimization problem could lead to a much larger impact. Our experiments on a real-world dataset reveal important properties of the attack, including attack transferability and its limitations. These findings can inspire useful defensive methods against this possible existing attack. 推荐系统在现代信息和电子商务应用中发挥着重要作用。虽然越来越多的研究致力于提高推荐的相关性和多样性，但对最先进推荐模型的潜在风险却探索不足，即这些模型可能会受到恶意第三方的攻击，通过注入虚假用户交互来达到目的。本文重新探讨了逆向学习注入攻击问题，即注入的虚假用户 “行为 ”是由攻击者用自己的模型在本地学习的--这个模型可能与被攻击的模型不同，但具有类似的属性，允许攻击转移。我们发现，现有文献中的大多数作品都存在两大局限性：（1）没有精确解决优化问题，使攻击的危害性大打折扣；（2）假定攻击者对攻击一无所知，导致对现实攻击能力缺乏了解。我们证明，将生成虚假用户作为优化问题的精确解可能会导致更大的影响。我们在真实世界数据集上的实验揭示了攻击的重要特性，包括攻击的可转移性及其局限性。这些发现可以激发有用的防御方法来对抗这种可能存在的攻击。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | 2020-09-22                                                                                                                                                                              | 2024-10-09 13:25:17                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:14                                                                                                                                  | 2024-10-09 13:25:17                                                                                                                                                                                                                                                                                                                                 | 318-327                                                                                                                                                                                          | ccfInfo: CCF-B RecSys; citationNumber: 66           | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| A Black-Box Attack Model for Visually-Aware Recommender Systems                                                                                                 | Cohen, Rami; Sar Shalom, Oren; Jannach, Dietmar; Amir, Amihood                                                                                                          | 2021             | conferencePaper | Proceedings of the 14th ACM International Conference on Web Search and Data Mining                                                                                                                  | https://dl.acm.org/doi/10.1145/3437963.3441757                                                                                                                                                                                                                                | 978-1-4503-8297-7                   |                                 | 10.1145/3437963.3441757               | Due to the advances in deep learning, visually-aware recommender systems (RS) have recently attracted increased research interest. Such systems combine collaborative signals with images, usually represented as feature vectors outputted by pre-trained image models. Since item catalogs can be huge, recommendation service providers often rely on images that are supplied by the item providers. In this work, we show that relying on such external sources can make an RS vulnerable to attacks, where the goal of the attacker is to unfairly promote certain pushed items. Specifically, we demonstrate how a new visual attack model can effectively influence the item scores and rankings in a black-box approach, i.e., without knowing the parameters of the model. The main underlying idea is to systematically create small human-imperceptible perturbations of the pushed item image and to devise appropriate gradient approximation methods to incrementally raise the pushed item’s score. Experimental evaluations on two datasets show that the novel attack model is effective even when the contribution of the visual features to the overall performance of the recommender system is modest. 由于深度学习的进步，视觉感知推荐系统（RS）最近引起了越来越多的研究兴趣。这类系统将协作信号与图像结合起来，图像通常表示为由预先训练的图像模型输出的特征向量。由于商品目录可能非常庞大，推荐服务提供商通常依赖于商品提供商提供的图像。在这项工作中，我们展示了依赖这种外部来源会使 RS 容易受到攻击，攻击者的目的是不公平地推广某些被推送的项目。具体来说，我们展示了一种新的可视化攻击模型如何通过黑盒方法（即不知道模型参数）有效地影响项目得分和排名。主要的基本思想是系统地对被推送项目的图像进行人类无法感知的微小扰动，并设计适当的梯度逼近方法来逐步提高被推送项目的分数。在两个数据集上进行的实验评估表明，即使视觉特征对推荐系统整体性能的贡献不大，新的攻击模型也是有效的。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | 2021-03-08                                                                                                                                                                              | 2024-10-09 12:52:48                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:19:53                                                                                                                                  | 2024-10-09 12:52:48                                                                                                                                                                                                                                                                                                                                 | 94-102                                                                                                                                                                                           | ccfInfo: CCF-B WSDM; citationNumber: 27             | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Attacking black-box recommendations via copying cross-domain user profiles                                                                                      | Fan, Wenqi; Derr, Tyler; Zhao, Xiangyu; Ma, Yao; Liu, Hui; Wang, Jianping; Tang, Jiliang; Li, Qing                                                                      | 2021             | conferencePaper | 2021 IEEE 37th international conference on data engineering (ICDE)                                                                                                                                  | https://ieeexplore.ieee.org/abstract/document/9458627/                                                                                                                                                                                                                        |                                     |                                 |                                       | Recently, recommender systems that aim to suggest personalized lists of items for users to interact with online have drawn a lot of attention. In fact, many of these state-of-the-art techniques have been deep learning based. Recent studies have shown that these deep learning models (in particular for recommendation systems) are vulnerable to attacks, such as data poisoning, which generates users to promote a selected set of items. However, more recently, defense strategies have been developed to detect these generated users with fake profiles. Thus, advanced injection attacks of creating more ‘realistic’ user profiles to promote a set of items is still a key challenge in the domain of deep learning based recommender systems. In this work, we present our framework CopyAttack, which is a reinforcement learning based black-box attack method that harnesses real users from a source domain by copying their profiles into the target domain with the goal of promoting a subset of items. CopyAttack is constructed to both efficiently and effectively learn policy gradient networks that first select, and then further refine/craft, user profiles from the source domain to ultimately copy into the target domain. CopyAttack’s goal is to maximize the hit ratio of the targeted items in the Top-k recommendation list of the users in the target domain. We have conducted experiments on two real-world datasets and have empirically verified the effectiveness of our proposed framework and furthermore performed a thorough model analysis. 最近，旨在向用户推荐个性化在线互动项目列表的推荐系统引起了广泛关注。事实上，这些最先进的技术很多都是基于深度学习的。最近的研究表明，这些深度学习模型（尤其是用于推荐系统的模型）很容易受到攻击，例如数据中毒，这种攻击会让用户推广一组选定的项目。不过，最近已经开发出了防御策略，可以检测到这些生成用户的虚假配置文件。因此，创建更 “真实 ”的用户配置文件以推广一组项目的高级注入攻击仍是基于深度学习的推荐系统领域的一个关键挑战。在这项工作中，我们介绍了我们的框架 CopyAttack，这是一种基于强化学习的黑盒攻击方法，它通过将源域中的真实用户复制到目标域中来利用他们的配置文件，从而达到推广项目子集的目的。CopyAttack 的构建是为了高效学习策略梯度网络，首先从源域中选择用户配置文件，然后进一步完善/制作，最终复制到目标域中。CopyAttack 的目标是最大限度地提高目标项在目标域用户 Top-k 推荐列表中的命中率。我们在两个真实世界的数据集上进行了实验，从经验上验证了我们所提出的框架的有效性，并进一步证明了我们所提出的方法的可行性。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | 2021                                                                                                                                                                                    | 2024-10-09 12:54:02                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:19:59                                                                                                                                  | 2024-10-09 12:54:02                                                                                                                                                                                                                                                                                                                                 | 1583–1594                                                                                                                                                                                        | citationNumber: 0; ccfInfo: CCF-A ICDE              | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Data poisoning attacks to deep learning based recommender systems                                                                                               | Huang, Hai; Mu, Jiaming; Gong, Neil Zhenqiang; Li, Qi; Liu, Bin; Xu, Mingwei                                                                                            | 2021             | journalArticle  | arXiv preprint arXiv:2101.02644                                                                                                                                                                     | https://arxiv.org/abs/2101.02644                                                                                                                                                                                                                                              |                                     |                                 |                                       | Recommender systems play a crucial role in helping users to find their interested information in various web services such as Amazon, YouTube, and Google News. Various recommender systems, ranging from neighborhood-based, associationrule-based, matrix-factorization-based, to deep learning based, have been developed and deployed in industry. Among them, deep learning based recommender systems become increasingly popular due to their superior performance.  In this work, we conduct the first systematic study on data poisoning attacks to deep learning based recommender systems. An attacker’s goal is to manipulate a recommender system such that the attacker-chosen target items are recommended to many users. To achieve this goal, our attack injects fake users with carefully crafted ratings to a recommender system. Specifically, we formulate our attack as an optimization problem, such that the injected ratings would maximize the number of normal users to whom the target items are recommended. However, it is challenging to solve the optimization problem because it is a non-convex integer programming problem. To address the challenge, we develop multiple techniques to approximately solve the optimization problem. Our experimental results on three realworld datasets, including small and large datasets, show that our attack is effective and outperforms existing attacks. Moreover, we attempt to detect fake users via statistical analysis of the rating patterns of normal and fake users. Our results show that our attack is still effective and outperforms existing attacks even if such a detector is deployed. 推荐系统在帮助用户在亚马逊、YouTube 和谷歌新闻等各种网络服务中找到自己感兴趣的信息方面发挥着至关重要的作用。从基于邻域的推荐系统、基于关联规则的推荐系统、基于矩阵因子的推荐系统到基于深度学习的推荐系统，各种推荐系统已被开发并应用于工业领域。其中，基于深度学习的推荐系统因其卓越的性能而越来越受欢迎。 在这项工作中，我们首次系统地研究了对基于深度学习的推荐系统的数据中毒攻击。攻击者的目标是操纵推荐系统，使攻击者选择的目标项目被推荐给许多用户。为了实现这一目标，我们的攻击会向推荐系统注入经过精心设计的评分的虚假用户。具体来说，我们将攻击表述为一个优化问题，即注入的评分将使目标项目被推荐给的正常用户数量最大化。然而，由于这是一个非凸整数编程问题，因此解决这个优化问题具有挑战性。我们在三个真实世界数据集（包括小型和大型数据集）上的实验结果表明，我们的攻击是有效的，并且优于现有的攻击。此外，我们还尝试通过对正常用户和虚假用户的评分模式进行统计分析来检测虚假用户。我们的结果表明，即使部署了这样的检测器，我们的攻击仍然有效，而且效果优于现有攻击。s                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | 2021                                                                                                                                                                                    | 2024-10-09 12:51:08                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:01                                                                                                                                  | 2024-10-09 12:51:08                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                  | ccfInfo: CCF-A NDSS; citationNumber: 131            | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Adversarial Item Promotion: Vulnerabilities at the Core of Top-N Recommenders that Use Images to Address Cold Start                                             | Liu, Zhuoran; Larson, Martha                                                                                                                                            | 2021             | conferencePaper | Proceedings of the Web Conference 2021                                                                                                                                                              | https://dl.acm.org/doi/10.1145/3442381.3449891                                                                                                                                                                                                                                | 978-1-4503-8312-7                   |                                 | 10.1145/3442381.3449891               | E-commerce platforms provide their customers with ranked lists of recommended items matching the customers’ preferences. Merchants on e-commerce platforms would like their items to appear as high as possible in the top-N of these ranked lists. In this paper, we demonstrate how unscrupulous merchants can create item images that artificially promote their products, improving their rankings. Recommender systems that use images to address the cold start problem are vulnerable to this security risk. We describe a new type of attack, Adversarial Item Promotion (AIP), that strikes directly at the core of Top-N recommenders: the ranking mechanism itself. Existing work on adversarial images in recommender systems investigates the implications of conventional attacks, which target deep learning classifiers. In contrast, our AIP attacks are embedding attacks that seek to push features representations in a way that fools the ranker (not a classifier) and directly leads to item promotion. We introduce three AIP attacks insider attack, expert attack, and semantic attack, which are defined with respect to three successively more realistic attack models. Our experiments evaluate the danger of these attacks when mounted against three representative visually-aware recommender algorithms in a framework that uses images to address cold start. We also evaluate potential defenses, including adversarial training and find that common, currentlyexisting, techniques do not eliminate the danger of AIP attacks. In sum, we show that using images to address cold start opens recommender systems to potential threats with clear practical implications. 电子商务平台向客户提供符合客户偏好的推荐商品排名表。电子商务平台上的商家都希望自己的商品尽可能地出现在这些排名列表的前 N 位。在本文中，我们演示了不法商家如何制作商品图片，人为地宣传自己的产品，从而提高排名。使用图片解决冷启动问题的推荐系统很容易受到这种安全风险的影响。我们描述了一种新型攻击--对抗性商品促销（AIP），它直接攻击 Top-N 推荐器的核心：排名机制本身。现有关于推荐系统中对抗性图像的研究主要调查传统攻击的影响，这些攻击以深度学习分类器为目标。与此相反，我们的 AIP 攻击是嵌入式攻击，旨在以愚弄排名器（而非分类器）的方式推动特征表征，并直接导致项目晋升。我们引入了三种 AIP 攻击：内部攻击、专家攻击和语义攻击，这三种攻击是根据三个更逼真的攻击模型先后定义的。我们的实验评估了这些攻击在使用图像解决冷启动问题的框架中对三种具有代表性的视觉感知推荐算法发起攻击时的危险性。我们还评估了潜在的防御措施，包括对抗训练，并发现现有的普通技术并不能消除 AIP 攻击的危险。总之，我们的研究表明，使用图像来解决冷启动问题会使推荐系统面临潜在威胁，具有明显的实际意义。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | 2021-04-19                                                                                                                                                                              | 2024-10-09 12:55:10                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:19:57                                                                                                                                  | 2024-10-09 12:55:10                                                                                                                                                                                                                                                                                                                                 | 3590-3602                                                                                                                                                                                        | citationNumber: 26; ccfInfo: CCF-A WWW              | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Ready for emerging threats to recommender systems? A graph convolution-based generative shilling attack                                                         | Wu, Fan; Gao, Min; Yu, Junliang; Wang, Zongwei; Liu, Kecheng; Wang, Xu                                                                                                  | 2021             | journalArticle  | Information Sciences                                                                                                                                                                                | https://www.sciencedirect.com/science/article/pii/S0020025521007313                                                                                                                                                                                                           |                                     |                                 |                                       | To explore the robustness of recommender systems, researchers have proposed various shilling attack models and analyzed their adverse effects. Primitive attacks are highly feasible but less effective due to simplistic handcrafted rules, while upgraded attacks are more powerful but costly and difficult to deploy because they require more knowledge from recommendations. In this paper, we explore a novel shilling attack called Graph cOnvolution-based generative shilling ATtack (GOAT) to balance the attacks’ feasibility and effectiveness. GOAT adopts the primitive attacks’ paradigm that assigns items for fake users by sampling and the upgraded attacks’ paradigm that generates fake ratings by a deep learning-based model. It deploys a generative adversarial network (GAN) that learns the real rating distribution to generate fake ratings. Additionally, the generator combines a tailored graph convolution structure that leverages the correlations between co-rated items to smoothen the fake ratings and enhance their authenticity. The extensive experiments on two public datasets evaluate GOAT’s performance from multiple perspectives. Our study of the GOAT demonstrates technical feasibility for building a more powerful and intelligent attack model with a much-reduced cost, enables analysis the threat of such an attack and guides for investigating necessary prevention measures. 为了探索推荐系统的鲁棒性，研究人员提出了各种 “shilling ”攻击模型，并分析了它们的不利影响。原始攻击的可行性很高，但由于手工创建的规则比较简单，因此效果较差；升级攻击的威力较大，但由于需要从推荐中获取更多知识，因此成本较高且难以部署。在本文中，我们探索了一种名为基于图形演化的生成式起信攻击（GOAT）的新型起信攻击，以平衡攻击的可行性和有效性。GOAT 采用了原始攻击范式和升级攻击范式，前者通过采样为虚假用户分配项目，后者通过基于深度学习的模型生成虚假评分。它部署了一个生成式对抗网络（GAN），通过学习真实评分分布来生成虚假评分。此外，生成器还结合了量身定制的图卷积结构，利用共同评分项目之间的相关性来平滑虚假评分并增强其真实性。在两个公共数据集上进行的大量实验从多个角度评估了 GOAT 的性能。我们对 GOAT 的研究表明，以更低的成本建立更强大、更智能的攻击模型在技术上是可行的，可以分析此类攻击的威胁，并指导研究必要的预防措施。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | 2021                                                                                                                                                                                    | 2024-10-09 12:49:35                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:33                                                                                                                                  | 2024-10-09 12:49:35                                                                                                                                                                                                                                                                                                                                 | 683–701                                                                                                                                                                                          | ccfInfo: CCF-B; citationNumber: 14                  | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Black-Box Attacks on Sequential Recommenders via Data-Free Model Extraction                                                                                     | Yue, Zhenrui; He, Zhankui; Zeng, Huimin; McAuley, Julian                                                                                                                | 2021             | conferencePaper | Fifteenth ACM Conference on Recommender Systems                                                                                                                                                     | https://dl.acm.org/doi/10.1145/3460231.3474275                                                                                                                                                                                                                                | 978-1-4503-8458-2                   |                                 | 10.1145/3460231.3474275               | We investigate whether model extraction can be used to ‘steal’ the weights of sequential recommender systems, and the potential threats posed to victims of such attacks. This type of risk has attracted attention in image and text classification, but to our knowledge not in recommender systems. We argue that sequential recommender systems are subject to unique vulnerabilities due to the specific autoregressive regimes used to train them. Unlike many existing recommender attackers, which assume the dataset used to train the victim model is exposed to attackers, we consider a data-free setting, where training data are not accessible. Under this setting, we propose an API-based model extraction method via limited-budget synthetic data generation and knowledge distillation. We investigate state-of-the-art models for sequential recommendation and show their vulnerability under model extraction and downstream attacks. We perform attacks in two stages. (1) Model extraction: given different types of synthetic data and their labels retrieved from a black-box recommender, we extract the black-box model to a white-box model via distillation. (2) Downstream attacks: we attack the black-box model with adversarial samples generated by the white-box recommender. Experiments show the effectiveness of our data-free model extraction and downstream attacks on sequential recommenders in both profile pollution and data poisoning settings. 我们研究了模型提取是否可用于 “窃取 ”顺序推荐系统的权重，以及这种攻击对受害者造成的潜在威胁。这种类型的风险在图像和文本分类中引起了关注，但据我们所知，在推荐系统中还没有出现过。我们认为，顺序推荐系统因其训练所使用的特定自回归机制而存在独特的脆弱性。现有的许多推荐程序攻击者都假定用于训练受害者模型的数据集暴露在攻击者面前，与此不同，我们考虑的是无数据环境，即无法访问训练数据。在这种情况下，我们通过有限预算的合成数据生成和知识提炼，提出了一种基于 API 的模型提取方法。我们研究了最先进的顺序推荐模型，并展示了这些模型在模型提取和下游攻击下的脆弱性。我们分两个阶段进行攻击。(1) 模型提取：给定不同类型的合成数据以及从黑盒推荐器中获取的标签，我们通过蒸馏将黑盒模型提取为白盒模型。(2) 下游攻击：我们利用白盒推荐器生成的对抗样本攻击黑盒模型。实验表明，我们的无数据模型提取和下游攻击在配置文件污染和数据中毒两种情况下对顺序推荐器都很有效。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | 2021-09-13                                                                                                                                                                              | 2024-10-09 12:54:14                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:22                                                                                                                                  | 2024-10-09 12:54:14                                                                                                                                                                                                                                                                                                                                 | 44-54                                                                                                                                                                                            | ccfInfo: CCF-B RecSys; citationNumber: 41           | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Data Poisoning Attack against Recommender System Using Incomplete and Perturbed Data                                                                            | Zhang, Hengtong; Tian, Changxin; Li, Yaliang; Su, Lu; Yang, Nan; Zhao, Wayne Xin; Gao, Jing                                                                             | 2021             | conferencePaper | Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining                                                                                                                  | https://dl.acm.org/doi/10.1145/3447548.3467233                                                                                                                                                                                                                                | 978-1-4503-8332-5                   |                                 | 10.1145/3447548.3467233               | Recent studies reveal that recommender systems are vulnerable to data poisoning attack due to their openness nature. In data poisoning attack, the attacker typically recruits a group of controlled users to inject well-crafted user-item interaction data into the recommendation model’s training set to modify the model parameters as desired. Thus, existing attack approaches usually require full access to the training data to infer items’ characteristics and craft the fake interactions for controlled users. However, such attack approaches may not be feasible in practice due to the attacker’s limited data collection capability and the restricted access to the training data, which sometimes are even perturbed by the privacy preserving mechanism of the service providers. Such design-reality gap may cause failure of attacks. In this paper, we fill the gap by proposing two novel adversarial attack approaches to handle the incompleteness and perturbations in user-item interaction data. First, we propose a bi-level optimization framework that incorporates a probabilistic generative model to find the users and items whose interaction data is sufficient and has not been significantly perturbed, and leverage these users and items’ data to craft fake user-item interactions. Moreover, we reverse the learning process of recommendation models and develop a simple yet effective approach that can incorporate context-specific heuristic rules to handle data incompleteness and perturbations. Extensive experiments on two datasets against three representative recommendation models show that the proposed approaches can achieve better attack performance than existing approaches. 最近的研究表明，由于推荐系统的开放性，它很容易受到数据中毒攻击。在数据中毒攻击中，攻击者通常会招募一组受控用户，向推荐模型的训练集中注入精心制作的用户-物品交互数据，以达到修改模型参数的目的。因此，现有的攻击方法通常需要完全访问训练数据，以推断项目特征，并为受控用户制作虚假交互。然而，由于攻击者的数据收集能力有限，而且对训练数据的访问受限，有时甚至会受到服务提供商隐私保护机制的干扰，因此这类攻击方法在实践中可能并不可行。这种设计与现实的差距可能会导致攻击失败。在本文中，我们提出了两种新型对抗攻击方法来处理用户-物品交互数据的不完整性和扰动性，从而填补了这一空白。 首先，我们提出了一个双层优化框架，该框架结合了一个概率生成模型，以找到互动数据充足且未受到明显扰动的用户和项目，并利用这些用户和项目的数据来制作虚假的用户-项目互动。此外，我们还扭转了推荐模型的学习过程，并开发出一种简单而有效的方法，可以结合特定情境的启发式规则来处理数据不完整和扰动问题。针对三个具有代表性的推荐模型在两个数据集上进行的广泛实验表明，与现有方法相比，我们提出的方法可以获得更好的攻击性能。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | 2021-08-14                                                                                                                                                                              | 2024-10-09 12:53:00                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:23                                                                                                                                  | 2024-10-09 12:53:00                                                                                                                                                                                                                                                                                                                                 | 2154-2164                                                                                                                                                                                        | ccfInfo: CCF-A SIGKDD; citationNumber: 38           | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Strategic attacks on recommender systems: an obfuscation scenario                                                                                               | Al Jurdi, Wissam; Abdo, Jacques Bou; Demerjian, Jacques; Makhoul, Abdallah                                                                                              | 2022             | conferencePaper | 2022 IEEE/ACS 19th International Conference on Computer Systems and Applications (AICCSA)                                                                                                           | https://ieeexplore.ieee.org/abstract/document/10017953/                                                                                                                                                                                                                       |                                     |                                 |                                       | Understanding user behavior in the context of recommender systems remains challenging for researchers and practitioners. Inconsistent and misleading user information, which is often concealed in datasets, can inevitably shape the recommendation results in certain distorted ways despite utilizing recommender models with enhanced personalizing capabilities. Naturally, the quality of data that fuels those recommenders should be extremely reliable and free of any biases that might be invisible to a model, irrespective of its type. In this article, we introduce two modern forms of noise that are intrinsically hard to detect and eliminate; one is malicious in nature and will be termed Burst while the other is unique in that it forms its own category and will be referred to as Opt-out. Additionally, with the aim of segregating the nature of noise behind such threats, we present a distinct case study on Burst and Opt-out to illustrate how the detection of those threats can be challenging compared to that of traditional noise and with the current detection methods. Finally, we expound on the ability of such threats to bias the output of recommenders in their own unique way while primarily retaining data that is not fundamentally erroneous. 对于研究人员和从业人员来说，理解推荐系统中的用户行为仍然具有挑战性。不一致和误导性的用户信息往往隐藏在数据集中，尽管使用了具有增强个性化功能的推荐模型，但这些信息不可避免地会以某些扭曲的方式影响推荐结果。当然，不管是哪种类型的模型，为这些推荐器提供支持的数据质量都应该极其可靠，并且不存在任何可能被模型所忽略的偏差。在本文中，我们将介绍两种本质上难以检测和消除的现代噪音形式：一种是恶意噪音，将被称为 “突发噪音”（Burst）；另一种是自成一类的独特噪音，将被称为 “退出噪音”（Opt-out）。此外，为了区分此类威胁背后的噪声性质，我们对 Burst 和 Opt-out 进行了独特的案例研究，以说明与传统噪声和当前的检测方法相比，检测这些威胁如何具有挑战性。最后，我们阐述了此类威胁以其独特的方式对推荐器的输出产生偏差的能力，同时主要保留了没有根本性错误的数据。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | 2022                                                                                                                                                                                    | 2024-10-09 13:00:57                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:17                                                                                                                                  | 2024-10-09 13:00:57                                                                                                                                                                                                                                                                                                                                 | 1–8                                                                                                                                                                                              | citationNumber: 0; ccfInfo: CCF-None AICCSA         | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Knowledge-enhanced Black-box Attacks for Recommendations                                                                                                        | Chen, Jingfan; Fan, Wenqi; Zhu, Guanghui; Zhao, Xiangyu; Yuan, Chunfeng; Li, Qing; Huang, Yihua                                                                         | 2022             | conferencePaper | Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining                                                                                                                | https://dl.acm.org/doi/10.1145/3534678.3539359                                                                                                                                                                                                                                | 978-1-4503-9385-0                   |                                 | 10.1145/3534678.3539359               | Recent studies have shown that deep neural networks-based recommender systems are vulnerable to adversarial attacks, where attackers can inject carefully crafted fake user profiles (i.e., a set of items that fake users have interacted with) into a target recommender system to achieve malicious purposes, such as promote or demote a set of target items. Due to the security and privacy concerns, it is more practical to perform adversarial attacks under the black-box setting, where the architecture/parameters and training data of target systems cannot be easily accessed by attackers. However, generating high-quality fake user profiles under black-box setting is rather challenging with limited resources to target systems. To address this challenge, in this work, we introduce a novel strategy by leveraging items’ attribute information (i.e., items’ knowledge graph), which can be publicly accessible and provide rich auxiliary knowledge to enhance the generation of fake user profiles. More specifically, we propose a knowledge graph-enhanced blackbox attacking framework (KGAttack) to effectively learn attacking policies through deep reinforcement learning techniques, in which knowledge graph is seamlessly integrated into hierarchical policy networks to generate fake user profiles for performing adversarial black-box attacks. Comprehensive experiments on various realworld datasets demonstrate the effectiveness of the proposed attacking framework under the black-box setting. 最近的研究表明，基于深度神经网络的推荐系统很容易受到恶意攻击，攻击者可以向目标推荐系统注入精心制作的虚假用户配置文件（即一组虚假用户曾与之互动的项目），以达到恶意目的，如晋升或降级一组目标项目。出于安全和隐私方面的考虑，在黑盒环境下实施对抗性攻击更为实用，因为攻击者无法轻易获取目标系统的架构/参数和训练数据。然而，由于目标系统资源有限，在黑盒环境下生成高质量的虚假用户配置文件相当具有挑战性。为了应对这一挑战，我们在这项工作中引入了一种新策略，即利用项目的属性信息（即项目知识图谱），这些信息可以公开访问，并提供丰富的辅助知识，以增强虚假用户配置文件的生成。更具体地说，我们提出了一种知识图谱增强型黑盒攻击框架（KGAttack），通过深度强化学习技术有效地学习攻击策略，其中知识图谱被无缝集成到分层策略网络中，以生成用于执行对抗性黑盒攻击的虚假用户配置文件。在各种真实世界数据集上进行的综合实验证明了所提出的攻击框架在黑盒环境下的有效性。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 2022-08-14                                                                                                                                                                              | 2024-10-09 13:26:22                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:07                                                                                                                                  | 2024-10-09 13:26:22                                                                                                                                                                                                                                                                                                                                 | 108-117                                                                                                                                                                                          | ccfInfo: CCF-A SIGKDD; citationNumber: 46           | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Shilling black-box recommender systems by learning to generate fake user profiles                                                                               | Lin, Chen; Chen, Si; Zeng, Meifang; Zhang, Sheng; Gao, Min; Li, Hui                                                                                                     | 2022             | journalArticle  | IEEE Transactions on Neural Networks and Learning Systems                                                                                                                                           | https://ieeexplore.ieee.org/abstract/document/9806457/                                                                                                                                                                                                                        |                                     |                                 |                                       | Due to the pivotal role of Recommender Systems (RS) in guiding customers towards the purchase, there is a natural motivation for unscrupulous parties to spoof RS for profits. In this paper, we study Shilling Attack where an adversarial party injects a number of fake user profiles for improper purposes. Conventional Shilling Attack approaches lack attack transferability (i.e., attacks are not effective on some victim RS models) and/or attack invisibility (i.e., injected profiles can be easily detected). To overcome these issues, we present Leg-UP, a novel attack model based on the Generative Adversarial Network. Leg-UP learns user behavior patterns from real users in the sampled “templates” and constructs fake user profiles. To simulate real users, the generator in Leg-UP directly outputs discrete ratings. To enhance attack transferability, the parameters of the generator are optimized by maximizing the attack performance on a surrogate RS model. To improve attack invisibility, Leg-UP adopts a discriminator to guide the generator to generate undetectable fake user profiles. Experiments on benchmarks have shown that Leg-UP exceeds state-of-the-art Shilling Attack methods on a wide range of victim RS models. The source code of our work is available at: https://github.com/XMUDM/ShillingAttack. 由于推荐系统（RS）在引导客户购买方面发挥着举足轻重的作用，因此不法分子自然有动机伪造推荐系统以牟利。在本文中，我们研究的是 Shilling Attack，即敌方出于不正当目的注入大量虚假用户配置文件。传统的 Shilling Attack 方法缺乏攻击的可转移性（即攻击对某些受害者 RS 模型无效）和/或攻击的隐蔽性（即注入的配置文件很容易被检测到）。为了克服这些问题，我们提出了基于生成对抗网络的新型攻击模型 Leg-UP。Leg-UP 从采样 “模板 ”中的真实用户那里学习用户行为模式，并构建虚假用户配置文件。为了模拟真实用户，Leg-UP 中的生成器直接输出离散评级。为了提高攻击的可转移性，生成器的参数通过最大化对代理 RS 模型的攻击性能进行了优化。为了提高攻击的隐蔽性，Leg-UP 采用了鉴别器来引导生成器生成无法检测的虚假用户配置文件。基准实验表明，Leg-UP 在各种受害者 RS 模型上的性能都超过了最先进的 Shilling 攻击方法。我们工作的源代码可在以下网址获取：https://github.com/XMUDM/ShillingAttack。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | 2022                                                                                                                                                                                    | 2024-10-09 12:55:33                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:16                                                                                                                                  | 2024-10-09 12:55:33                                                                                                                                                                                                                                                                                                                                 | 1305–1319                                                                                                                                                                                        | citationNumber: 7; ccfInfo: CCF-B TNNLS             | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Fedrecattack: Model poisoning attack to federated recommendation                                                                                                | Rong, Dazhong; Ye, Shuai; Zhao, Ruoyan; Yuen, Hon Ning; Chen, Jianhai; He, Qinming                                                                                      | 2022             | conferencePaper | 2022 IEEE 38th International Conference on Data Engineering (ICDE)                                                                                                                                  | https://ieeexplore.ieee.org/abstract/document/9835228/                                                                                                                                                                                                                        |                                     |                                 |                                       | Federated Recommendation (FR) has received considerable popularity and attention in the past few years. In FR, for each user, its feature vector and interaction data are kept locally on its own client thus are private to others. Without the access to above information, most existing poisoning attacks against recommender systems or federated learning lose validity. Benifiting from this characteristic, FR is commonly considered fairly secured. However, we argue that there is still possible and necessary security improvement could be made in FR. To prove our opinion, in this paper we present FedRecAttack, a model poisoning attack to FR aiming to raise the exposure ratio of target items. In most recommendation scenarios, apart from private user-item interactions (e.g., clicks, watches and purchases), some interactions are public (e.g., likes, follows and comments). Motivated by this point, in FedRecAttack we make use of the public interactions to approximate users’ feature vectors, thereby attacker can generate poisoned gradients accordingly and control malicious users to upload the poisoned gradients in a well-designed way. To evaluate the effectiveness and side effects of FedRecAttack, we conduct extensive experiments on three real-world datasets of different sizes from two completely different scenarios. Experimental results demonstrate that our proposed FedRecAttack achieves the state-of-the-art effectiveness while its side effects are negligible. Moreover, even with small proportion (3%) of malicious users and small proportion (1%) of public interactions, FedRecAttack remains highly effective, which reveals that FR is more vulnerable to attack than people commonly considered. 联合推荐（Federated Recommendation，FR）在过去几年中受到了广泛的欢迎和关注。在联合推荐中，每个用户的特征向量和交互数据都保存在自己的客户端上，因此对其他用户是保密的。由于无法获取上述信息，现有的针对推荐系统或联合学习的大多数中毒攻击都失去了有效性。得益于这一特点，FR 通常被认为是相当安全的。然而，我们认为 FR 仍有可能进行必要的安全改进。为了证明我们的观点，我们在本文中介绍了 FedRecAttack，这是一种针对 FR 的模型中毒攻击，旨在提高目标项目的曝光率。在大多数推荐场景中，除了用户与项目之间的私人互动（如点击、观看和购买）外，有些互动是公开的（如喜欢、关注和评论）。基于这一点，在 FedRecAttack 中，我们利用公开互动来近似用户的特征向量，从而攻击者可以据此生成中毒梯度，并通过精心设计的方式控制恶意用户上传中毒梯度。为了评估 FedRecAttack 的有效性和副作用，我们在两个完全不同场景的三个不同规模的真实数据集上进行了大量实验。实验结果表明，我们提出的 FedRecAttack 达到了最先进的效果，其副作用可以忽略不计。此外，即使恶意用户比例很小（3%），公共互动比例很小（1%），FedRecAttack 仍然非常有效，这揭示了 FR 比人们通常认为的更容易受到攻击。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | 2022                                                                                                                                                                                    | 2024-10-09 13:00:45                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:31                                                                                                                                  | 2024-10-09 13:00:45                                                                                                                                                                                                                                                                                                                                 | 2643–2655                                                                                                                                                                                        | ccfInfo: CCF-A ICDE; citationNumber: 8              | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Gray-Box Shilling Attack: An Adversarial Learning Approach                                                                                                      | Wang, Zongwei; Gao, Min; Li, Jundong; Zhang, Junwei; Zhong, Jiang                                                                                                       | 2022             | journalArticle  | ACM Transactions on Intelligent Systems and Technology                                                                                                                                              | https://dl.acm.org/doi/10.1145/3512352                                                                                                                                                                                                                                        |                                     | 2157-6904, 2157-6912            | 10.1145/3512352                       | Recommender systems are essential components of many information services, which aim to find relevant items that match user preferences. Several studies have shown that shilling attacks can significantly weaken the robustness of recommender systems by injecting fake user profiles. Traditional shilling attacks focus on creating hand-engineered fake user profiles, but these profiles can be detected effortlessly by advanced detection methods. Adversarial learning, which has emerged in recent years, can be leveraged to generate powerful and intelligent attack models. To this end, in this article we explore potential risks of recommender systems and shed light on a gray-box shilling attack model based on generative adversarial networks, named GSA-GANs. Specifically, we aim to generate fake user profiles that can achieve two goals: unnoticeable and offensive. Toward these goals, there are several challenges that we need to address: (1) learning complex user behaviors from user-item rating data, and (2) adversely influencing the recommendation results without knowing the underlying recommendation algorithms. To tackle these challenges, two essential GAN modules are respectively designed to make generated fake profiles more similar to real ones and harmful to recommendation results. Experimental results on three public datasets demonstrate that the proposed GSA-GANs framework outperforms baseline models in attack effectiveness, transferability, and camouflage. In the end, we also provide several possible defensive strategies against GSA-GANs. The exploration and analysis in our work will contribute to the defense research of recommender systems. 推荐系统是许多信息服务的重要组成部分，其目的是找到符合用户偏好的相关项目。多项研究表明，伪装攻击会通过注入虚假用户配置文件大大削弱推荐系统的鲁棒性。传统的 “伪装 ”攻击主要是通过手工制作虚假用户配置文件，但这些配置文件可以被先进的检测方法轻松检测出来。近年来兴起的对抗学习可以用来生成强大的智能攻击模型。为此，我们在本文中探讨了推荐系统的潜在风险，并阐明了一种基于生成式对抗网络（名为 GSA-GANs）的灰盒推销攻击模型。具体来说，我们的目标是生成虚假用户配置文件，以实现两个目标：不易察觉和攻击性。为了实现这些目标，我们需要解决几个难题： (1) 从用户项目评级数据中学习复杂的用户行为；(2) 在不了解底层推荐算法的情况下对推荐结果产生负面影响。为了应对这些挑战，我们分别设计了两个基本的 GAN 模块，使生成的虚假资料与真实资料更加相似，并对推荐结果产生不利影响。在三个公共数据集上的实验结果表明，所提出的 GSA-GANs 框架在攻击效果、可转移性和伪装性方面都优于基线模型。最后，我们还针对 GSA-GAN 提出了几种可能的防御策略。我们工作中的探索和分析将有助于推荐系统的防御研究。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | 2022-10-31                                                                                                                                                                              | 2024-10-09 13:25:49                                                                                                                                                                                                                                                                                                                                       | 2024-11-30 08:20:24                                                                                                                                  | 2024-10-09 13:25:48                                                                                                                                                                                                                                                                                                                                 | 1-21                                                                                                                                                                                             | citationNumber: 6; ccfInfo: CCF-None TIST           | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Accelerating the Surrogate Retraining for Poisoning Attacks against Recommender Systems                                                                         | Wu, Yunfan; Cao, Qi; Tao, Shuchang; Zhang, Kaike; Sun, Fei; Shen, Huawei                                                                                                | 2024             | conferencePaper | 18th ACM Conference on Recommender Systems                                                                                                                                                          | https://dl.acm.org/doi/10.1145/3640457.3688148                                                                                                                                                                                                                                | 979-8-4007-0505-2                   |                                 | 10.1145/3640457.3688148               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | 2024-10-08                                                                                                                                                                              | 2024-12-06 13:27:25                                                                                                                                                                                                                                                                                                                                       | 2024-12-06 13:28:03                                                                                                                                  | 2024-12-06 13:27:25                                                                                                                                                                                                                                                                                                                                 | 701-711                                                                                                                                                                                          | ccfInfo: CCF-B RecSys; citationNumber: 0            | /unread                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Multi-agent Attacks for Black-box Social Recommendations                                                                                                        | Wang, Shijie; Fan, Wenqi; Wei, Xiao-yong; Mei, Xiaowei; Lin, Shanru; Li, Qing                                                                                           | 2024             | preprint        |                                                                                                                                                                                                     | http://arxiv.org/abs/2311.07127                                                                                                                                                                                                                                               |                                     |                                 | 10.48550/arXiv.2311.07127             | The rise of online social networks has facilitated the evolution of social recommender systems, which incorporate social relations to enhance users' decision-making process. With the great success of Graph Neural Networks (GNNs) in learning node representations, GNN-based social recommendations have been widely studied to model user-item interactions and user-user social relations simultaneously. Despite their great successes, recent studies have shown that these advanced recommender systems are highly vulnerable to adversarial attacks, in which attackers can inject well-designed fake user profiles to disrupt recommendation performances. While most existing studies mainly focus on argeted attacks to promote target items on vanilla recommender systems, untargeted attacks to degrade the overall prediction performance are less explored on social recommendations under a black-box scenario. To perform untargeted attacks on social recommender systems, attackers can construct malicious social relationships for fake users to enhance the attack performance. However, the coordination of social relations and item profiles is challenging for attacking black-box social recommendations. To address this limitation, we first conduct several preliminary studies to demonstrate the effectiveness of cross-community connections and cold-start items in degrading recommendations performance. Specifically, we propose a novel framework MultiAttack based on multi-agent reinforcement learning to coordinate the generation of cold-start item profiles and cross-community social relations for conducting untargeted attacks on black-box social recommendations. Comprehensive experiments on various real-world datasets demonstrate the effectiveness of our proposed attacking framework under the black-box setting.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | 2024-09-16                                                                                                                                                                              | 2024-12-06 13:36:42                                                                                                                                                                                                                                                                                                                                       | 2024-12-06 13:37:45                                                                                                                                  | 2024-12-06 13:36:42                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                  |                                                     | /unread; Computer Science - Artificial Intelligence; Computer Science - Social and Information Networks                                                                                                                                                                                                                                                                                                                                |
| Defense Against Model Extraction Attacks on Recommender Systems                                                                                                 | Zhang, Sixiao; Yin, Hongzhi; Chen, Hongxu; Long, Cheng                                                                                                                  | 2024             | conferencePaper | Proceedings of the 17th ACM International Conference on Web Search and Data Mining                                                                                                                  | https://dl.acm.org/doi/10.1145/3616855.3635751                                                                                                                                                                                                                                | 979-8-4007-0371-3                   |                                 | 10.1145/3616855.3635751               | The robustness of recommender systems has become a prominent topic within the research community. Numerous adversarial attacks have been proposed, but most of them rely on extensive prior knowledge, such as all the white-box attacks or most of the black-box attacks which assume that certain external knowledge is available. Among these attacks, the model extraction attack stands out as a promising and practical method, involving training a surrogate model by repeatedly querying the target model. However, there is a significant gap in the existing literature when it comes to defending against model extraction attacks on recommender systems. In this paper, we introduce Gradient-based Ranking Optimization (GRO), which is the first defense strategy designed to counter such attacks. We formalize the defense as an optimization problem, aiming to minimize the loss of the protected target model while maximizing the loss of the attacker's surrogate model. Since top-k ranking lists are non-differentiable, we transform them into swap matrices which are instead differentiable. These swap matrices serve as input to a student model that emulates the surrogate model's behavior. By back-propagating the loss of the student model, we obtain gradients for the swap matrices. These gradients are used to compute a swap loss, which maximizes the loss of the student model. We conducted experiments on three benchmark datasets to evaluate the performance of GRO, and the results demonstrate its superior effectiveness in defending against model extraction attacks. 推荐系统的鲁棒性已成为研究界的一个重要课题。人们提出了许多对抗性攻击，但其中大多数都依赖于大量的先验知识，例如所有的白盒攻击或大多数黑盒攻击都假定某些外部知识是可用的。在这些攻击中，模型提取攻击是一种很有前途的实用方法，它通过反复查询目标模型来训练一个代理模型。然而，在防御推荐系统的模型提取攻击方面，现有文献还存在很大差距。在本文中，我们介绍了基于梯度的排名优化（GRO），这是第一种旨在对抗此类攻击的防御策略。我们将这种防御形式化为一个优化问题，旨在使受保护目标模型的损失最小化，同时使攻击者代理模型的损失最大化。由于 top-k 排序列表是不可微的，因此我们将其转换为可微的交换矩阵。这些交换矩阵是学生模型的输入，学生模型会模仿代理模型的行为。通过反向传播学生模型的损失，我们得到了交换矩阵的梯度。这些梯度用于计算交换损失，使学生模型的损失最大化。我们在三个基准数据集上进行了实验，以评估 GRO 的性能，结果表明它在抵御模型提取攻击方面非常有效。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | 2024-03-04                                                                                                                                                                              | 2024-12-07 06:18:15                                                                                                                                                                                                                                                                                                                                       | 2024-12-07 07:10:56                                                                                                                                  | 2024-12-06                                                                                                                                                                                                                                                                                                                                          | 949–957                                                                                                                                                                                          | ccfInfo: CCF-B WSDM; citationNumber: 0              |                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| Shilling Attacks and Fake Reviews Injection: Principles, Models, and Datasets                                                                                   | Nawara, Dina; Aly, Ahmed; Kashef, Rasha                                                                                                                                 | 2024             | journalArticle  | IEEE Transactions on Computational Social Systems                                                                                                                                                   | https://ieeexplore.ieee.org/document/10716004/?arnumber=10716004                                                                                                                                                                                                              |                                     | 2329-924X                       | 10.1109/TCSS.2024.3465008             | Recommendation systems have proved to be a compelling performance in overcoming the data overload problem in many domains, such as e-commerce, e-health, and transportation. Recommender systems guide users/clients to personalized recommendations based on their preferences. However, some recommendation systems are vulnerable to shilling attacks, which create rating biases or fake reviews that will eventually affect the authenticity and integrity of the generated recommendations. This survey comprehensively covers various shilling attack methods, including high-knowledge, low-knowledge attacks, and obfuscated attacks. It explores malicious review generators that generate fake text. In addition to that, this survey covers shilling attack detection methods such as supervised, unsupervised, semisupervised, and hybrid techniques. Natural Language Processing techniques are also thoroughly explored for fake text review detection using large language models (LLMs). A wide range of detection mechanisms incorporated in the literature is examined, such as convolutional neural network (CNN), long short term memory (LSTM)-based detectors for rating-based shilling attacks, and bidirectional encoder representation (BERT) and RoBERTa-based detectors for fake reviews that are accompanied by shilling attacks, aiming to offer insights into the evolving methods of shilling attack strategies and the corresponding advancements in the detection methods. 事实证明，在电子商务、电子医疗和交通等许多领域，推荐系统在克服数据过载问题方面都有令人信服的表现。推荐系统根据用户/客户的偏好向其提供个性化推荐。然而，一些推荐系统容易受到 “托”（shilling）攻击，这种攻击会造成评分偏差或虚假评论，最终影响所生成推荐的真实性和完整性。本调查全面涵盖了各种托客攻击方法，包括高知识攻击、低知识攻击和混淆攻击。它探讨了生成虚假文本的恶意评论生成器。除此以外，本调查还涵盖了监督、无监督、半监督和混合技术等托词攻击检测方法。还深入探讨了使用大型语言模型（LLM）进行虚假评论检测的自然语言处理技术。此外，还研究了文献中包含的各种检测机制，如卷积神经网络（CNN）、基于长短期记忆（LSTM）的检测器，用于检测基于评级的托儿攻击，以及基于双向编码器表示（BERT）和 RoBERTa 的检测器，用于检测伴有托儿攻击的虚假评论，旨在深入探讨不断演变的托儿攻击策略方法以及检测方法的相应进步。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | 2024                                                                                                                                                                                    | 2024-12-07 06:35:20                                                                                                                                                                                                                                                                                                                                       | 2024-12-07 07:06:40                                                                                                                                  | 2024-12-07 06:35:20                                                                                                                                                                                                                                                                                                                                 | 1-14                                                                                                                                                                                             |                                                     | Collaboration; Fake news; Fake reviews; Knowledge based systems; Knowledge engineering; large language models (LLMs); Motion pictures; profile injection; recommendation systems; Recommender systems; Reviews; Robustness; shilling attacks; Standards; Surveys                                                                                                                                                                       |
| Trustworthy Recommender Systems                                                                                                                                 | Wang, Shoujin; Zhang, Xiuzhen; Wang, Yan; Ricci, Francesco                                                                                                              | 2024             | journalArticle  | ACM Transactions on Intelligent Systems and Technology                                                                                                                                              | https://dl.acm.org/doi/10.1145/3627826                                                                                                                                                                                                                                        |                                     | 2157-6904, 2157-6912            | 10.1145/3627826                       | Recommender systems (RSs) aim at helping users to effectively retrieve items of their interests from a large catalogue. For a quite long time, researchers and practitioners have been focusing on developing accurate RSs. Recent years have witnessed an increasing number of threats to RSs, coming from attacks, system and user generated noise, and various types of biases. As a result, it has become clear that the focus on RS accuracy is too narrow, and the research must consider other important factors, particularly trustworthiness. A trustworthy recommender system (TRS) should not only be accurate but also transparent, unbiased, fair, and robust to noise and attacks. These observations actually led to a paradigm shift of the research on RSs: from accuracy-oriented RSs to TRSs. However, there is a lack of a systematic overview and discussion of the literature in this novel and fast-developing field of TRSs. To this end, in this article, we provide an overview of TRSs, including a discussion of the motivation and basic concepts of TRSs, a presentation of the challenges in building TRSs, and a perspective on the future directions in this area. We also provide a novel conceptual framework to support the construction of TRSs. 推荐系统（RSs）旨在帮助用户从大量目录中有效检索其感兴趣的项目。长期以来，研究人员和从业人员一直致力于开发精确的推荐系统。近年来，来自攻击、系统和用户产生的噪音以及各种偏差的威胁日益增多。因此，对 RS 准确性的关注显然过于狭隘，研究必须考虑其他重要因素，尤其是可信度。值得信赖的推荐系统（TRS）不仅要准确，而且要透明、无偏见、公平，并能抵御噪音和攻击。这些观点实际上导致了推荐系统研究范式的转变：从以准确性为导向的推荐系统转变为 TRS。然而，对于 TRS 这一快速发展的新领域，目前还缺乏系统的文献综述和讨论。为此，我们在本文中概述了 TRS，包括讨论 TRS 的动机和基本概念，介绍构建 TRS 所面临的挑战，并展望这一领域的未来发展方向。我们还提供了一个支持构建 TRS 的新概念框架。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | 2024-08-31                                                                                                                                                                              | 2024-12-07 06:35:45                                                                                                                                                                                                                                                                                                                                       | 2024-12-07 07:10:23                                                                                                                                  | 2024-12-07 06:35:45                                                                                                                                                                                                                                                                                                                                 | 1-20                                                                                                                                                                                             | ccfInfo: CCF-None TIST; citationNumber: 0           |                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| On-Device Recommender Systems: A Comprehensive Survey                                                                                                           | Yin, Hongzhi; Qu, Liang; Chen, Tong; Yuan, Wei; Zheng, Ruiqi; Long, Jing; Xia, Xin; Shi, Yuhui; Zhang, Chengqi                                                          | 2024             | preprint        |                                                                                                                                                                                                     | http://arxiv.org/abs/2401.11441                                                                                                                                                                                                                                               |                                     |                                 | 10.48550/arXiv.2401.11441             | Recommender systems have been widely deployed in various real-world applications to help users identify content of interest from massive amounts of information. Traditional recommender systems work by collecting user-item interaction data in a cloud-based data center and training a centralized model to perform the recommendation service. However, such cloud-based recommender systems (CloudRSs) inevitably suffer from excessive resource consumption, response latency, as well as privacy and security risks concerning both data and models. Recently, driven by the advances in storage, communication, and computation capabilities of edge devices, there has been a shift of focus from CloudRSs to on-device recommender systems (DeviceRSs), which leverage the capabilities of edge devices to minimize centralized data storage requirements, reduce the response latency caused by communication overheads, and enhance user privacy and security by localizing data processing and model training. Despite the rapid rise of DeviceRSs, there is a clear absence of timely literature reviews that systematically introduce, categorize and contrast these methods. To bridge this gap, we aim to provide a comprehensive survey of DeviceRSs, covering three main aspects: (1) the deployment and inference of DeviceRSs (2) the training and update of DeviceRSs (3) the security and privacy of DeviceRSs. Furthermore, we provide a fine-grained and systematic taxonomy of the methods involved in each aspect, followed by a discussion regarding challenges and future research directions. This is the first comprehensive survey on DeviceRSs that covers a spectrum of tasks to fit various needs. We believe this survey will help readers effectively grasp the current research status in this field, equip them with relevant technical foundations, and stimulate new research ideas for developing DeviceRSs. 推荐系统已被广泛应用于现实世界的各种应用中，帮助用户从海量信息中识别感兴趣的内容。传统推荐系统的工作原理是在云数据中心收集用户与项目的交互数据，然后训练一个集中模型来执行推荐服务。然而，这种基于云的推荐系统（CloudRSs）不可避免地存在资源消耗过大、响应延迟以及数据和模型的隐私和安全风险等问题。最近，在边缘设备的存储、通信和计算能力不断进步的推动下，人们开始将重点从云推荐系统转向设备上推荐系统（DeviceRSs），后者充分利用边缘设备的能力，最大限度地减少集中数据存储需求，降低通信开销造成的响应延迟，并通过本地化数据处理和模型训练提高用户隐私和安全性。尽管 DeviceRS 迅速崛起，但显然缺乏系统介绍、分类和对比这些方法的及时文献综述。为了弥补这一空白，我们旨在提供一份关于设备检索系统的综合调查报告，主要涵盖三个方面：（1）设备检索系统的部署和推理（2）设备检索系统的训练和更新（3）设备检索系统的安全性和隐私性。此外，我们还对每个方面所涉及的方法进行了细粒度的系统分类，并对挑战和未来研究方向进行了讨论。这是第一份关于设备资源系统的全面调查报告，涵盖了各种任务，以满足各种需求。我们相信，这份调查报告将帮助读者有效掌握该领域的研究现状，为他们提供相关的技术基础，并激发开发设备可视化系统的新研究思路。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | 2024-02-15                                                                                                                                                                              | 2024-12-07 06:36:34                                                                                                                                                                                                                                                                                                                                       | 2024-12-07 07:10:40                                                                                                                                  | 2024-12-07 06:36:34                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                  | ccfInfo: CCF-None CORR; citationNumber: 0           | Computer Science - Information Retrieval                                                                                                                                                                                                                                                                                                                                                                                               |
| A Survey on Federated Recommendation Systems                                                                                                                    | Sun, Zehua; Xu, Yonghui; Liu, Yong; He, Wei; Kong, Lanju; Wu, Fangzhao; Jiang, Yali; Cui, Lizhen                                                                        | 2024             | journalArticle  | IEEE Transactions on Neural Networks and Learning Systems                                                                                                                                           | https://ieeexplore.ieee.org/document/10423793/?arnumber=10423793                                                                                                                                                                                                              |                                     | 2162-2388                       | 10.1109/TNNLS.2024.3354924            | Federated learning has recently been applied to recommendation systems to protect user privacy. In federated learning settings, recommendation systems can train recommendation models by collecting the intermediate parameters instead of the real user data, which greatly enhances user privacy. In addition, federated recommendation systems (FedRSs) can cooperate with other data platforms to improve recommendation performance while meeting the regulation and privacy constraints. However, FedRSs face many new challenges such as privacy, security, heterogeneity, and communication costs. While significant research has been conducted in these areas, gaps in the surveying literature still exist. In this article, we: 1) summarize some common privacy mechanisms used in FedRSs and discuss the advantages and limitations of each mechanism; 2) review several novel attacks and defenses against security; 3) summarize some approaches to address heterogeneity and communication costs problems; 4) introduce some realistic applications and public benchmark datasets for FedRSs; and 5) present some prospective research directions in the future. This article can guide researchers and practitioners understand the research progress in these areas. 联合学习最近被应用于推荐系统，以保护用户隐私。在联合学习环境中，推荐系统可以通过收集中间参数而不是真实用户数据来训练推荐模型，从而大大提高了用户隐私保护。此外，联合推荐系统（FedRS）可以与其他数据平台合作，在满足法规和隐私约束的同时提高推荐性能。然而，联合推荐系统面临着许多新的挑战，如隐私、安全、异构性和通信成本等。虽然在这些领域已经开展了大量研究，但调查文献中仍然存在空白。在本文中，我们将 1）总结了 FedRS 中常用的一些隐私机制，并讨论了每种机制的优势和局限性；2）回顾了针对安全性的几种新型攻击和防御方法；3）总结了解决异构性和通信成本问题的一些方法；4）介绍了 FedRS 的一些现实应用和公共基准数据集；5）提出了未来的一些前瞻性研究方向。本文可指导研究人员和从业人员了解这些领域的研究进展。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 2024                                                                                                                                                                                    | 2024-12-07 06:55:56                                                                                                                                                                                                                                                                                                                                       | 2024-12-07 07:06:59                                                                                                                                  | 2024-12-07 06:55:56                                                                                                                                                                                                                                                                                                                                 | 1-15                                                                                                                                                                                             |                                                     | Communication costs; Data models; Data privacy; federated learning; heterogeneity; privacy; Privacy; recommendation systems; Recommender systems; security; Security; Servers; Training                                                                                                                                                                                                                                                |
| A Review of Modern Recommender Systems Using Generative Models (Gen-RecSys)                                                                                     | Deldjoo, Yashar; He, Zhankui; McAuley, Julian; Korikov, Anton; Sanner, Scott; Ramisa, Arnau; Vidal, René; Sathiamoorthy, Maheswaran; Kasirzadeh, Atoosa; Milano, Silvia | 2024             | conferencePaper | Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining                                                                                                                | https://dl.acm.org/doi/10.1145/3637528.3671474                                                                                                                                                                                                                                | 979-8-4007-0490-1                   |                                 | 10.1145/3637528.3671474               | Traditional recommender systems typically use user-item rating histories as their main data source. However, deep generative models now have the capability to model and sample from complex data distributions, including user-item interactions, text, images, and videos, enabling novel recommendation tasks. This comprehensive, multidisciplinary survey connects key advancements in RS using Generative Models (Gen-RecSys), covering: interaction-driven generative models; the use of large language models (LLM) and textual data for natural language recommendation; and the integration of multimodal models for generating and processing images/videos in RS. Our work highlights necessary paradigms for evaluating the impact and harm of Gen-RecSys and identifies open challenges. This survey accompanies a tutorial presented at ACM KDD’24, with supporting materials provided at: https://encr.pw/vDhLq. 传统的推荐系统通常使用用户项目评级历史记录作为主要数据源。然而，深度生成模型现在有能力对复杂的数据分布进行建模和采样，包括用户与项目的交互、文本、图像和视频，从而实现新颖的推荐任务。这篇综合性多学科调查报告将使用生成模型（Gen-RecSys）的RS领域的主要进展联系起来，内容包括：交互驱动生成模型；使用大型语言模型（LLM）和文本数据进行自然语言推荐；在RS中整合多模态模型生成和处理图像/视频。我们的工作强调了评估 Gen-RecSys 的影响和危害所需的范例，并指出了有待解决的挑战。本调查报告附有在 ACM KDD'24 上发表的教程，辅助材料请访问：https://encr.pw/vDhLq。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | 2024-08-25                                                                                                                                                                              | 2024-12-07 07:03:31                                                                                                                                                                                                                                                                                                                                       | 2024-12-07 07:17:13                                                                                                                                  | 2024-12-07 07:03:31                                                                                                                                                                                                                                                                                                                                 | 6448-6458                                                                                                                                                                                        | ccfInfo: CCF-A SIGKDD; citationNumber: 8            |                                                                                                                                                                                                                                                                                                                                                                                                                                        |
